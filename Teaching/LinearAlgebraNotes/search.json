[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MAT-30045: Linear Algebra and Rings",
    "section": "",
    "text": "Welcome\nWelcome to MAT-30045: Linear Algebra and Rings. This module is an option for Single Honours BSc and MMath and Combined Honours BSc and expands on two part-modules from the second year. In the first half we shall be studying ideas in Linear Algebra that expand on the introduction to this subject given in Exploring Algebra and Analysis (MAT-20035). In the second half we delve more deeply into the study of the algebraic object called a ring which was first introduced formally in Abstract Algebra (MAT-20025)."
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "MAT-30045: Linear Algebra and Rings",
    "section": "Structure",
    "text": "Structure\nThe module is comprised of the following parts:\nPart I\n\nRevision and the exchange lemma.\nLinear mappings and the Rank-Nullity Theorem.\nEigenspaces.\nVector space isomorphisms.\n\nPart II\n\nRing theory fundamentals.\nIdeals.\nUnique factorisation domains.\n\n\nIntended Learning Outcomes (ILO’s)\nUpon successful completion of this module you will be able to:\n\ndefine a linear transformation, prove and apply associated results, including the use of linear transformations to change between bases in a vector space;\ndefine an eigenvalue and eigenvector of a linear transformation and apply these concepts to, inter alia, the diagonalisation of square matrices;\ndefine vector space isomorphisms;\ndefine different types of ring, and state and prove associated results;\ndefine ideals, maximal ideals and principal ideals and solve associated problems;\nrecognise and define ideals and unique factorisation domains, prove associated results and/or solve associated problems.\n\n\n\nPrerequisites\nMAT-20025\n\n\nLecture notes and recommended reading\nA full-set of gapped notes is available on KLE1. At the conclusion of a given chapter, the notes will be updated to include the missing parts of that chapter. For example at the conclusion of Chapter n, the notes will be updated to include the gaps in Chapter n.\nIn addition, the following non-essential texts are recommended as providing more in-depth discussion/ a different point of view on topics covered in lectures as well as additional practise examples.\n\nH. Anton & C. Rorres: Elementary Linear Algebra: applications version (7th edition).\nPeter J. Cameron: Introduction to Algebra (2nd edition).\nR. B. J. T. Allenby: Rings, Fields and Groups: an Introduction to Abstract Algebra (2nd edition).\n\nCopies of the above are available in the library. In addition, Cameron is available as an e-book."
  },
  {
    "objectID": "index.html#logistics",
    "href": "index.html#logistics",
    "title": "MAT-30045: Linear Algebra and Rings",
    "section": "Logistics",
    "text": "Logistics\n\nLectures\nThe lecture material will be delivered by way of 2.5 standard, face-to-face lectures each week (3 in even weeks and 2 in odd). If you have any questions while going through the content then do email me f.a.olukoya@keele.ac.uk. I am more than happy to arrange a meeting over teams or in person.\n\n\nExample classes\nIn weeks 2, 4, 6, 8, and 10 the Thursday class will be an examples class given over to the study of specified problems. The expectation is that students will prepare solutions to these problems for discussion in the session; this is an important part of your learning process. Problem Sheets can be found at the end of the relevant section in the notes. Please note that you should only attend the example class to which you have been allocated.\n\n\n\n\n\nTable 1:  Example Classes \nExample Class\nQuestions\n\n\n1\nSheet 1\n\n\n2\nSheet 2 Q2.1 – Q2.16\n\n\n3\nSheet 2 Q2.17 – Q2.21\n\n\n4\nSheet 3 Q3.1 – 3.7\n\n\n5\nSheet 3 Q 3.8 – 3.11 & Sheet 4\n\n\n\n\n\n\n\n\nTimetable\nDetails of all sessions (lectures and examples classes) will be available on your eVision timetable. Please make sure that you have the correct day, time and room for each session. You should check this regularly as there are occasionally changes, particularly in the first couple of weeks of the semester.\nTable 2 displays a detailed schedule for the semester.\n\n\n\n\n\nTable 2:  Timetable \nWeek Beginning\nDay\nChapter\nMaterial\n\n\n23 Jan\nM\n1\nIntroduction and Revision\n\n\nT\n1\nDimension and Sums of subspaces\n\n\nThu\n1\nSums of subspaces\n\n\n30 Jan\nM\n1\nDirect sums & row and column spaces\n\n\nT\n1\nRank-Nullity Theorem of matrices\n\n\nThu\nExample Class 1\n\n\n6 Feb\nM\n2\nLinear Mappings\n\n\nT\n2\nImage and Kernel\n\n\nThu\n2\nRank-Nullity Theorem\n\n\n13\nM\n2\nMatrices from linear mappings\n\n\nT\n2\nChange of basis\n\n\nThu\nExample Class 2\n\n\n20\nM\n2\nEigenvalues and Eigenvectors\n\n\nT\n2\nDiagonalisation\n\n\nThu\n2\nVector space isomorphisms\n\n\n27\nM\n3\nRing theory introduction\n\n\nT\n3\nPolynomial and quadratic integer rings\n\n\nThu\nExample Class 3   (Assignment)\n\n\n6 Mar\nM\n3\nDivision in a ring\n\n\nT\n3\nZero divisors, and integral domains\n\n\nThu\n3\nIntegral domains and subrings  (Assignment due)\n\n\n13 Mar\nM\n3\nSubrings examples\n\n\nT\n3\nRing homomorphisms\n\n\nThu\nExample Class 4\n\n\n20\nM\n3\nIdeals\n\n\nT\n3\nIdeals II\n\n\nThu\n3\nIdeals III\n\n\n17 Apr\nM\n3\nFactor rings: cosets\n\n\nT\n3\nFactor rings & the First Isomorphism Theorem\n\n\nThu\nExample Class 5  (Coursework)\n\n\n24 Apr\nM\n4\nPrincipal Ideal Domains (PIDs)\n\n\nT\n4\nMaximal Ideals and Prime Ideals\n\n\nThu\n4\nZ[i] is a PID and Unique Factorisation Domains  (Coursework due)\n\n\n1 May\nM\nRevision\n\n\nT\n\n\nThu\n\n\n8\nExams\n\n\n\n\n\n\n\n\nLecturers\nThis semester Dr. Feyisayo (Shayo) Olukoya will be the lecturer on the module. As mentioned above you can reach me by email; you should also feel free to arrange an in-person meeting my office is Mac2.30 in the Mackay Building; meeting virtually over teams is also an option.\n\n\nKLE\nAll resources for the module (lecture notes, problem sheets, solutions e.t.c) will be made available on KLE at the appropriate time."
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "MAT-30045: Linear Algebra and Rings",
    "section": "Assessment",
    "text": "Assessment\n\nContinuous Assessment\nThis will be made up of two take-home assessments (each contributing 15% of the overall module mark). An assessment schedule will be available on the Mathematics Noticeboard on the KLE. Note that the first assessment is called an assignment, whilst the second is called a coursework; this nomenclature is purely for administrative convenience.\n\n\nFormative assessment\nProblem sheets can be found at the end of the each chapter of the lecture notes. Although these sheets do not contribute to the continuous assessment component, you are strongly encouraged to attempt them as they are designed to consolidate your understanding and enhance your problem-solving skills. Full solutions are provided after the sheet has been covered in example classes.\n\n\nFinal Exam\nThis comprises 70% of the module mark. It is an unseen, closed-book examination, with all questions being compulsory. The use of calculators is governed by the University regulations. The examination will require you to state definitions, state (and possibly prove) results, and apply these to solving problems. You should be able to state every definition and result in the module unless they are marked in the lecture notes as non-examinable."
  },
  {
    "objectID": "index.html#student-support",
    "href": "index.html#student-support",
    "title": "MAT-30045: Linear Algebra and Rings",
    "section": "Student Support",
    "text": "Student Support\nFor advice on any non-academic issue (including financial, international, personal or health matters) or if you are unsure who to go to for help, please contact Student Services. You can book a virtual appointment or email student.services@keele.ac.uk.\nYou can also contact the school’s Student Experience and Support Officer by emailing student services student.services@keele.ac.uk."
  },
  {
    "objectID": "01-intro.html#sec-RevisionofLinearAlgebra",
    "href": "01-intro.html#sec-RevisionofLinearAlgebra",
    "title": "1  Introduction and Revision",
    "section": "1.1 Revision of Linear Algebra",
    "text": "1.1 Revision of Linear Algebra\nWe begin with a recap of some of the basic ideas and concepts from Linear Algebra which you have seen in the Exploring Algebra and Analysis module. The main difference to note here is that all of the results will be expressed in terms of vector spaces over a general field \\(\\mathcal{F}\\), rather than the specific field \\(\\R\\). There is a set of examples (questions R.1 – R.6 in Section 1.3 at the end of the chapter).\n\nDefinition 1.1 (Vector space) Let \\(\\F\\) be a field. We say that a set \\(V\\) is a vector space over \\(\\F\\) if addition of elements of \\(V\\) and multiplication of elements of \\(V\\) by scalars from \\(\\F\\) are both defined such that the vector space axioms hold. These axioms are as follows:\n\nA0.\n\n\\(\\vecx,\\vecy\\in V\\imp \\vecx+\\vecy\\in V\\).\n\nA1.\n\nFor all \\(\\vecx,\\vecy,\\vecz\\in V,\\:\\:\\: (\\vecx+\\vecy)+\\vecz=\\vecx+(\\vecy+\\vecz)\\).\n\nA2.\n\nFor all \\(\\vecx,\\vecy\\in V,\\:\\:\\: \\vecx+\\vecy=\\vecy+\\vecx\\).\n\nA3.\n\nThere exists an element \\(\\veczero\\in V\\) such that \\(\\vecx+\\veczero=\\vecx\\) for all \\(\\vecx\\in V\\).\n\nA4.\n\nFor all \\(\\vecx\\in V\\), there exists \\(-\\vecx\\in V\\) such that \\(\\vecx+(-\\vecx)=\\veczero\\).\n\nM0.\n\n\\(\\lambda\\in \\F, \\vecx\\in V\\imp \\lambda\\vecx\\in V\\)\n\nM1.\n\nFor all \\(\\vecx,\\vecy\\in V\\) and for all \\(\\lambda\\in \\F,\\:\\:\\: \\lambda(\\vecx+\\vecy)=\\lambda\\vecx+\\lambda\\vecy\\).\n\nM2.\n\nFor all \\(\\vecx\\in V\\) and for all \\(\\lambda,\\mu\\in \\F,\\:\\:\\: (\\lambda+\\mu)\\vecx=\\lambda\\vecx+\\mu\\vecx\\).\n\nM3.\n\nFor all \\(\\vecx\\in V\\) and for all \\(\\lambda,\\mu\\in \\F,\\:\\:\\: \\lambda(\\mu\\vecx)=(\\lambda\\mu)\\vecx\\).\n\nM4.\n\nFor all \\(\\vecx\\in V\\), \\(1\\vecx=\\vecx\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIn a vector space \\(V\\), we refer to the elements of \\(V\\) as vectors.\nA vector space cannot be empty by A3.\nFor \\(\\vecx \\in V\\), the vector \\(-\\vecx\\) is called the negative of \\(\\vecx\\).\nA vector space is sometimes called a linear space.\n\n\n\nRecall the following lemmas, which we state without proof:\n\nLemma 1.1 Let \\(V\\) be a vector space. There is only one zero vector in \\(V\\).\n\n\nLemma 1.2 The negative of a vector in a vector space \\(V\\) is unique.\n\n\nLemma 1.3 Let \\(V\\) be a vector space. For all \\(\\vecx\\in V\\), \\(0\\vecx=\\veczero\\).\n\n\nLemma 1.4 Let \\(V\\) be a vector space. For all \\(\\lambda\\in \\F\\), \\(\\lambda\\veczero=\\veczero\\).\n\n\nLemma 1.5 Let \\(V\\) be a vector space. For all \\(\\vecx\\in V\\), \\(\\lambda\\in \\F\\), \\[\\lambda\\vecx=\\veczero\\imp \\lambda=0\\:\\:\\:\\:{\\rm{or}}\\:\\:\\:\\: \\vecx=\\veczero.\\]\n\nWe are often interested in whether or not a subset of vectors from a vector space satisfies the conditions to be a vector space in its own right. If so, it forms a subspace.\n\nDefinition 1.2 (Subspace) Let \\(V\\) be a vector space over the field \\(\\F\\). We say that a subset \\(U \\subseteq V\\) which is a vector space in its own right is called a subspace of \\(V\\).\n\nIt is not necessary to check all of the axioms to determine whether a subset forms a subspace; the following lemma says we need only check three things.\n\nLemma 1.6 (Checking Lemma) Let \\(V\\) be a vector space over the field \\(\\F\\). A subset \\(U\\subseteq V\\) is a subspace of \\(V\\) if the following three conditions hold:\n\n\\(U\\neq\\emptyset\\);\n\\(\\vecx,\\vecy\\in U\\imp \\vecx+\\vecy\\in U\\);\n\\(\\lambda\\in \\F,\\vecx\\in U\\imp \\lambda\\vecx\\in U\\).\n\n\nThe following three lemmas concern important properties of subspaces:\n\nLemma 1.7 Let \\(V\\) be a vector space over the field \\(\\F\\). The zero vector \\(\\veczero\\) belongs to every subspace of \\(V\\).\n\n\nLemma 1.8 Let \\(A\\in \\F_{m\\times n}\\). The set \\[N(A)=\\{\\vecX\\in \\F_{n\\times 1} \\where A\\vecX=\\veczero\\}\\] is a subspace of \\(\\F_{n\\times 1}\\).\n\n\nLemma 1.9 Let \\(V\\) be a vector space over the field \\(\\F\\) and let \\(U\\) and \\(W\\) be subspaces of \\(V\\). Then \\(U\\cap W\\) is also a subspace of \\(V\\).\n\nOne of the most fundamental concepts in algebra is that of linear independence. This, in itself, is derived from the concept of a linear combination and we remind ourselves of the important definitions and results.\n\nDefinition 1.3 (Linear combination) Let \\(\\vecx_{1},\\vecx_{2},...,\\vecx_{k}\\) be vectors in a vector space \\(V\\). A linear combination of the sequence \\((\\vecx_{1},\\vecx_{2},...,\\vecx_{k})\\) is an expression of the form \\[\\lambda_{1}\\vecx_{1}+\\lambda_{2}\\vecx_{2}+...+\\lambda_{k}\\vecx_{k},\\] where \\(\\lambda_{1},\\lambda_{2},...,\\lambda_{k}\\in \\F\\).\n\n\nDefinition 1.4 (Span) Let \\(V\\) be a vector space and let \\((\\vecx_{1},...,\\vecx_{k})\\) be a sequence of vectors in \\(V\\). The span of the sequence \\((\\vecx_{1},...,\\vecx_{k})\\) is the set of all linear combinations of the vectors in the sequence. Hence \\[{\\rm{span}}(\\vecx_{1},...,\\vecx_{k})=\\{\\lambda_{1}\\vecx_{1}+\\lambda_{2}\\vecx_{2}+...+\\lambda_{k}\\vecx_{k} \\where \\lambda_{1},\\lambda_{2},...,\\lambda_{k}\\in \\F\\}.\\] We define the span of the empty sequence to be the set consisting of the zero vector, \\(\\{\\veczero\\}\\).\n\n\nLemma 1.10 Let \\((\\vecx_{1},...,\\vecx_{k})\\) be a sequence of vectors in a vector space \\(V\\). Then \\(\\spn (\\vecx_{1},...,\\vecx_{k})\\) is a subspace of \\(V\\).\n\n\nDefinition 1.5 (Linear independence) Let \\((\\vecx_{1},...,\\vecx_{k})\\) be a sequence of vectors in a vector space \\(V\\). We say that the sequence is linearly independent (or L.I.) if \\[\\lambda_{1}\\vecx_{1}+\\lambda_{2}\\vecx_{2}+...+\\lambda_{k}\\vecx_{k}=\\veczero\\imp \\lambda_{1}=\\lambda_{2}=...=\\lambda_{k}=0.\\:\\:\\:\\:\\:\\:\\:\\:\\: (\\lambda_{i}\\in \\F)\\]\n\n\nDefinition 1.6 (Linear dependence) Let \\((\\vecx_{1},\\vecx_{2},...,\\vecx_{k})\\) be a sequence of vectors in a vector space \\(V\\). The sequence is linearly dependent (or L.D.) if there exist scalars \\(\\lambda_{1},\\lambda_{2},...,\\lambda_{k}\\in \\F\\), not all zero, such that \\[\\lambda_{1}\\vecx_{1}+\\lambda_{2}\\vecx_{2}+...+\\lambda_{k}\\vecx_{k}=\\veczero.\\]\n\nThe following are important results on linear independence/dependence.\n\nLemma 1.11 Let \\(L=(\\vecx_{1},\\vecx_{2},...,\\vecx_{k})\\) be a linearly independent sequence of vectors in a vector space \\(V\\). If \\(\\vecx\\in V\\) can be written as a linear combination of the vectors in \\(L\\), then this representation is unique.\n\n\nLemma 1.12 If a sequence of vectors in a vector space \\(V\\) contains a repetition, then the sequence is linearly dependent.\n\n\nLemma 1.13 If a sequence of vectors in a vector space \\(V\\) contains the zero vector then the sequence is linearly dependent.\n\n\nLemma 1.14 Let \\(V\\) be a vector space and let \\(\\vecx\\in V\\) be non-zero. Then the sequence \\((\\vecx)\\) is linearly independent.\n\nTwo important theorems from the second year module are the Minus Theorem and the Plus Theorem.\n\nTheorem 1.1 (Minus Theorem) Let \\(V\\) be a vector space. Suppose that the vectors \\(\\vecx_{1},...,\\vecx_{k}\\in V\\) are linearly dependent. Then there exists \\(j\\in\\{1,2,...,k\\}\\) such that \\[\\spn(\\vecx_{1},...,\\vecx_{j-1},\\vecx_{j+1},...,\\vecx_{k})=\\spn(\\vecx_{1},...,\\vecx_{k}).\\]\n\nThe way to think about this result is that we can remove a vector from a linearly dependent sequence without changing the span. However, we need to be careful with the wording ‘there exists’; we cannot remove any vector, we need to choose carefully.\n\nTheorem 1.2 (Plus Theorem) Let \\(V\\) be a vector space and suppose that the sequence \\((\\vecx_{1},...,\\vecx_{k})\\) in \\(V\\) is linearly independent. Then, for any \\(\\veca \\in V\\), \\[\\veca\\notin\\spn(\\vecx_{1},...,\\vecx_{k})\\imp (\\vecx_{1},...,\\vecx_{k},\\veca)\\:\\:\\:\\text{is linearly independent.}\\]\n\nThe way to think of this theorem is that we can add a certain vector to a linearly independent sequence and still have a linearly independent sequence. But, we cannot just add any vector; it has to be a vector that is not a linear combination of the vectors in the set. These two theorems can be used to prove the following theorem, known as the ‘Exchange Lemma’, which we state without proof.\n\nTheorem 1.3 (Exchange Lemma) Let \\(V\\) be a vector space and suppose that the sequence \\((\\vecx_{1},...,\\vecx_{m})\\) in \\(V\\) is linearly independent and that the sequence \\((\\vecy_{1},...,\\vecy_{k})\\) spans \\(V\\). Then,\n\n\\(m\\leq k\\),\nthere is a spanning sequence for \\(V\\) consisting of \\(\\vecx_{1},\\vecx_{2},...,\\vecx_{m}\\) and \\(k-m\\) of the vectors \\(\\vecy_{1},\\vecy_{2},...,\\vecy_{k}\\).\n\n\nThe sequence \\((\\vecy_1, \\ldots, \\vecy_k)\\) spans \\(V\\), so every vector in \\(V\\) can be written as a linear combination of the \\(\\vecy_i\\). Part (i) says that a linearly independent sequence is not longer than a sequence that spans \\(V\\). From part (ii) we see why it is called the ‘exchange’ lemma; we can exchange \\(m\\) of the \\(\\vecy_i\\) for the \\(\\vecx_i\\).\nAnother central concept in linear algebra is that of a basis:\n\nDefinition 1.7 (Finite dimension) Let \\(V\\) be a vector space. We say that \\(V\\) is finite dimensional if there exists a finite sequence of vectors that spans \\(V\\). Otherwise we say that \\(V\\) is infinite dimensional.\n\n\nDefinition 1.8 (Basis) Let \\(V\\) be a vector space. We say that a finite sequence \\((\\vecx_{1},...,\\vecx_{k})\\) of vectors in \\(V\\) is a basis of \\(V\\) if the sequence is (i) linearly independent and (ii) spans \\(V\\).\n\n\nLemma 1.15 Every finite dimensional vector space has a finite basis.\n\n\nLemma 1.16 Let \\(V\\) be a vector space and suppose that \\((\\vecx_{1},\\vecx_{2},...,\\vecx_{n})\\) is a basis of \\(V\\) (of length \\(n\\)). Then every sequence of more than \\(n\\) vectors in \\(V\\) is linearly dependent.\n\n\nCorollary 1.1 In a finite dimensional vector space, all bases have the same length.\n\nSo, what is meant by the ‘dimension’ of a vector space?\n\nDefinition 1.9 (Dimension) The number of vectors in each and every basis of a vector space \\(V\\) is called the dimension of \\(V\\) and is denoted \\(\\dimn(V)\\).\n\nThe following lemma says that a finite sequence of vectors in a vector space forms a basis if it satisfies two of three properties.\n\nLemma 1.17 Let \\(L\\) be a finite sequence of vectors in a vector space \\(V\\). We can conclude that \\(L\\) is a basis of \\(V\\) if we know that it possesses any two of the following properties:\n\nit spans \\(V\\);\nit is linearly independent;\nit has length equal to \\(\\dimn(V)\\).\n\n\nNote that, thanks to the Exchange Lemma, we can extend a linearly independent sequence in a vector space to a basis.\n\nDefinition 1.10 (Coordinate vector) Let \\((\\vecx_{1},...,\\vecx_{n})\\) be a basis of the finite dimensional vector space \\(V\\). Then for each \\(\\vecy\\in V\\), we can express \\(\\vecy\\) uniquely as \\[\\vecy=\\gamma_{1}\\vecx_{1}+...+\\gamma_{n}\\vecx_{n}\\] for some \\(\\gamma_{1},...,\\gamma_{n}\\in \\F\\). The \\(n\\)-tuple \\((\\gamma_{1},...,\\gamma_{n})\\in \\F^{n}\\) is called the coordinate vector of \\(\\vecy\\) with respect to the basis \\((\\vecx_{1},...,\\vecx_{n})\\).\n\n\nExample 1.1 Show that the sequence \\(L_{1}=(1+x,\\:1-x)\\) in \\(P_{2}\\) is linearly independent and use the Exchange Lemma to extend \\(L_{1}\\) to a basis of \\(P_{2}\\). Find the coordinate vector of \\(p(x)=2-x+3x^{2}\\) with respect to your basis.\n\nLet \\(a,b \\in \\R\\) and suppose that \\(a(1+x) + b(1-x) = 0\\) (where \\(0\\) is the zero polynomial). Equating coefficients, it follows that \\(a+b =0\\) and and \\(a-b = 0\\). Therefore \\(a = b = 0\\) since adding the two equations gives \\(2a = 0\\).\nThe set \\(B=(1,x,x^2)\\) is the standard basis for \\(P_2\\). By the Exchange Lemma, there is an element \\(v \\in B\\) such that \\((1+x, 1-x, v)\\) is a spanning set for \\(P_2\\). Since all bases for \\(P_2\\) have the same length then \\((1+x, 1-x, v)\\) must be a basis for \\(P_2\\) as well (since otherwise we may find a basis for \\(P_2\\) of length strictly less than 3!). Thus the vector \\(v \\in B\\) is any element of \\(B\\) which is not in the span of \\(L_1\\). This follows since \\((1+x,1-x,v)\\) must be linearly independent. Since \\(x^2\\) does not occur in any of the vectors in \\(P_2\\), then \\(x^2\\) is the obvious choice for \\(v\\). Indeed we have, \\[\\begin{align*}\n1 &= \\frac{1}{2}((1 + x) + (1-x)),\\\\\nx &= \\frac{1}{2}((1+x) - (1-x))\n\\end{align*}\\] whereas if \\(a (1+x) + b(1-x) = x^2\\) for \\(a,b \\in \\R\\) then (equating coefficients) \\(a = b = 0\\) which is a contradiction since \\(x^2 \\ne 0\\).\nTherefore \\(B'=(1+x, 1-x, x^2)\\) is a basis for \\(P_2\\).\nUsing the formula above expressing \\(1\\) and \\(x\\) in terms of the basis \\(B'\\), we have \\[p(x)=2-x+3x^{2} =((1+x) + (1-x)) -\\frac{1}{2}((1+x) - (1-x)) + 3x^2 = \\frac{1}{2}(1+x) + \\frac{3}{2}(1-x) + 3x^3.\\]"
  },
  {
    "objectID": "01-intro.html#sec-SomeFurtherResults",
    "href": "01-intro.html#sec-SomeFurtherResults",
    "title": "1  Introduction and Revision",
    "section": "1.2 Some Further Results",
    "text": "1.2 Some Further Results\nBefore we can progress with our study of linear mappings we require some further theory. The first few results you will have seen previously, the rest is new material.\n\n1.2.1 Dimension of Subspaces\n\nLemma 1.18 Let \\(W\\) be a subspace of a finite dimensional vector space \\(V\\). Then\n\n\\(W\\) is also finite dimensional,\n\\(\\dimn(W)\\leq \\dimn(V)\\),\nif \\(W\\neq V\\) then \\(\\dimn(W)<\\dimn(V)\\),\nany basis of \\(W\\) can be extended to produce a basis of \\(V\\).\n\n\n\n\nProof. \nLet \\(n = \\dim(V)\\). Since \\(W\\) is a subspace of \\(V\\), the length of a linearly independent sequence in \\(W\\) is at most \\(n\\). Let \\(m\\) be maximum length of a linearly independent sequence in \\(W\\) and let \\(B = (w_1, w_2, \\ldots, w_m)\\) be a linearly independent sequence in \\(W\\) of length \\(m\\). We prove that \\(b\\) is a basis for \\(w\\).\nLet \\(w \\in W\\) and suppose that \\(w \\ne \\spn(B)\\). Then \\(B' = (w_1, w_2, \\ldots, w_m, w)\\) is also linearly independent. By the Plus Theorem (Theorem 1.2). Therefore \\(B'\\) is a linearly independent subset of \\(W\\) of length \\(m+1\\) which is a contradiction. We see that \\(w \\in \\spn(B)\\). Since \\(w \\in W\\) was arbitrarily chosen, \\(\\spn(B) = W\\) and \\(W\\) is finite dimensional and has dimension less than or equal to \\(n\\).\nIf the dimension of \\(W\\) is \\(n\\), then any basis for \\(W\\) is a basis for \\(V\\) (any linearly independent subset of \\(V\\) of length \\(n\\) is a basis for \\(V\\)) and \\(W=V\\). Therefore if \\(W \\ne V\\), then \\(\\dim(W)< \\dim(V)\\).\nThe final part of the lemma is a consequence of the Exchange Lemma (Theorem 1.3).\n\n\n\nLemma 1.19 Let \\(S\\) and \\(T\\) be finite dimensional subspaces of a vector space \\(V\\) such that\n\n\\(S\\subseteq T\\) and\n\\(\\dimn(S)=\\dimn(T)\\).\n\nThen \\(S=T\\).\n\n\n\nProof. \nThis is a direct consequence of part iv. of Lemma 1.18 since as \\(S \\subseteq T\\) then \\(S\\) is a subspace of \\(T\\) as well.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNote that in the statement of Lemma 1.19 although \\(S\\) and \\(T\\) are assumed to be finite dimensional, \\(V\\) is does not have to be finite dimensional.\nFor example we may take \\(V = \\R[x]\\), \\(S= P_4\\) and \\(T = P_6\\); \\(S\\) and \\(T\\) are finite dimensional subspaces of \\(V\\), but \\(V\\) is not finite dimensional.\n\n\n\n\nDefinition 1.11 (Sum of subspaces) Let \\(S\\) and \\(T\\) be subspaces of a vector space \\(V\\). Then the sum of \\(S\\) and \\(T\\) is defined as \\[S+T=\\{\\textbf{s}+\\textbf{t} \\where \\textbf{s}\\in S,\\:\\:\\textbf{t}\\in T\\}.\\]\n\n\nLemma 1.20 Let \\(V\\) be a vector space and let \\(S\\) and \\(T\\) be subspaces of \\(V\\). Then,\n\n\\(S+T\\) is also a subspace of \\(V\\),\n\\(S+T\\) contains \\(S\\) and \\(T\\) as subsets,\n\\(S+T\\) is the smallest subspace of \\(V\\) that contains both \\(S\\) and \\(T\\).\n\n\n\n\nProof. \n\nWe use the Checking Lemma (Lemma 1.6).\nClearly \\(S+T\\) contains the zero vector since \\(\\vec{0}+ \\vec{0} = \\vec{0}\\).\nLet \\(\\vec{s}_1, \\vec{s}_2 \\in S\\) and \\(\\vec{t}_1, \\vec{t}_2 \\in T\\), then \\((\\vec{s}_1 + \\vec{t}_1) + (\\vec{s}_2 + \\vec{t}_2) = (\\vec{s}_1 + \\vec{s}_2) + (\\vec{t}_1 + \\vec{t}_2) \\in S+T\\).\nLet \\(\\vec{s} \\in S\\), \\(\\vec{t} \\in T\\) and \\(a \\in \\F\\). Then \\(a (\\vec{s} + \\vec{t}) = a \\vec{s} + a \\vec{t} \\in S + T\\).\nTherefore \\(S+T\\) is non-empty, closed under addition and closed under scalar multiplication — it is a subspace of \\(V\\).\nLet \\(\\vec{s} \\in S\\) and \\(\\vec{t} \\in T\\), then \\(\\vec{s} = \\vec{s} + \\vec{0} \\in S+T\\) and \\(\\vec{t} = \\vec{0} + \\vec{t} \\in S+T\\). Therefore \\((S \\cup T) \\subseteq S+T\\).\nLet \\(W\\) be any other subspace containing both \\(S\\) and \\(T\\). Then \\(S+T \\subseteq W\\) since \\(W\\) contains both \\(S\\) and \\(T\\) and is closed under addition.\n\n\n\n\n\n\n\nTheorem 1.4 Let \\(V\\) be a finite dimensional vector space and let \\(S\\) and \\(T\\) be subspaces of \\(V\\). Then \\[\\dimn(S+T)=\\dimn(S)+\\dimn(T)-\\dimn(S\\cap T).\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe final term, \\(S \\cap T\\), accounts for the overlap between \\(S\\) and \\(T\\).\n\n\n\n\nProof. \nLet \\(B_{S\\cap T}=(\\vec{c}_{1}, \\vec{c}_2, \\ldots, \\vec{c}_{j})\\) be a basis for \\(S \\cap T\\). Since \\(S \\cap T\\) is a subspace of both \\(S\\) and \\(T\\), by the Exchange Lemma (Theorem 1.3) there are vectors \\(\\vec{s}_1, \\ldots, \\vec{s}_{k} \\in S\\) and \\(\\vec{t}_1, \\ldots \\vec{t}_{l} \\in T\\) such that \\[B_{S}=(\\vec{c}_{1}, \\ldots, \\vec{c}_{j}, \\vec{s}_1,\\ldots, \\vec{s}_{k})\\] is a basis for \\(S\\) and \\[B_{T}=(\\vec{c}_{1}, \\ldots, \\vec{c}_{j}, \\vec{t}_1,\\ldots, \\vec{t}_{l})\\] is a basis for \\(T\\).\nConsider the sequence \\[B:=(\\vec{c}_{1}, \\ldots, \\vec{c}_{j}, \\vec{s}_1,\\ldots, \\vec{s}_{k},\\vec{t}_1,\\ldots, \\vec{t}_{l}).\\] Clearly \\(S\\) and \\(T\\) are contained in the span of \\(B\\) and so \\(B\\) spans \\(S+T\\) (by Lemma 1.20 part c.). We show that \\(B\\) is linearly independent.\nFirst note that \\[\\spn(\\vec{t}_1,\\vec{t}_{2},\\ldots, \\vec{t}_{l}) \\cap \\spn(\\vec{c}_{1}, \\ldots, \\vec{c}_{j}, \\vec{s}_1,\\ldots, \\vec{s}_{k}) = \\{\\vec{0}\\}.\\] Since otherwise, there is an element \\(\\vec{t} \\ne \\{0\\} \\in \\spn(\\vec{t}_1,\\vec{t}_{2},\\ldots, \\vec{t}_{l})\\) which is an element of \\(S\\) and so of \\(S \\cap T\\). Thus, \\(\\vec{t}\\) can be written as a linear combination of of the sequence \\(B_{S \\cap T}\\). This means that there is an element of \\(T\\) which can be written in two different ways as a linear combination of the sequence \\(B_{T}\\) contradicting the linear independence of \\(B_{T}\\).\nNow let \\(c_1, \\ldots, c_j, s_1, \\ldots, s_k, t_1, \\ldots, t_l \\in \\F\\) and consider the equation:\n\\[\\begin{equation*}\n    \\sum_{i=1}^{j}c_i \\vec{c}_{i} + \\sum_{i=1}^{k} s_i \\vec{s}_{i} + \\sum_{i=1}^{l}t_i \\vec{t}_{i} = \\vec{0}.\n\\end{equation*}\\]\nIt must be the case that \\(t_i = 0\\) for all \\(1 \\le i \\le k\\) since otherwise \\(t=\\sum_{i=1}^{l}t_i \\vec{t}_{i} \\ne 0\\) and \\(t \\in \\spn(\\vec{t}_1,\\vec{t}_{2},\\ldots, \\vec{t}_{l}) \\cap \\spn(\\vec{c}_{1}, \\ldots, \\vec{c}_{j}, \\vec{s}_1,\\ldots, \\vec{s}_{k})\\) which is a contradiction.\nHowever, if \\(t_i = 0\\) for all \\(1 \\le i \\le k\\), then the \\(c_i\\)’s and \\(s_i\\)’s must also be \\(0\\) since \\(B_{S} = (\\vec{c}_{1}, \\ldots, \\vec{c}_{j}, \\vec{s}_1,\\ldots, \\vec{s}_{k})\\) is a basis. Therefore, \\(B\\) is linearly independent.\nLastly observe that \\[|B| = |B_{S}| + |B_{T}| - |B_{S \\cap T}| = \\dim{S} + \\dim{T} - \\dim{S \\cap T}\\] as required.\n\n\n\nExample 1.2 Let \\(S,T\\) and \\(U\\) be subspaces of \\(\\R^{2n-1}\\) such that \\(\\mathbb{R}^{2n-1}=S+T=S+U\\) with \\(\\dimn(S)=n-1\\) and \\(S\\cap T=S\\cap U=\\{\\veczero\\}\\). By considering dimensions, show that \\(T\\cap U\\neq\\{\\veczero\\}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough \\(S+T = T+U\\) it is not necessarily the case that \\(T = U\\).\n\n\nApplying Theorem Theorem 1.4, consider the following:\n\\[\\begin{equation*}\n2n-1 = \\dim(s) + \\dim(T) - \\dim(S \\cap T)\n\\end{equation*}\\] rearranging, we have\n\\[\\begin{equation*}\nn = 2n-1 -n+1 = 2n-1 - \\dim(s) = \\dim(T) - \\dim(S \\cap T) = \\dim(T) - 0\n\\end{equation*}\\]\nTherefore \\(\\dim(T) = n\\). In a similar way, we have that \\(\\dim(T) = n\\). Now observe that as \\(T + U\\) is a subspace of \\(\\R^{2n-1}\\), \\(\\dim(T + U) \\le 2n-1\\). We have: \\[2n-1 \\ge  \\dim(T+U) = \\dim(T) + \\dim(U) - \\dim(T \\cap U) = 2n - \\dim(T \\cap U).\\] Rearranging, we have that \\(\\dim(T \\cap U) \\ge 1\\), thus \\(T \\cap U\\) must contain at least one non-zero vector — \\(T \\cap U \\ne \\{\\vec{0}\\}\\).\n\n\n\nDefinition 1.12 (Direct sum) Let \\(V\\) be a vector space over the field \\(\\F\\) and let \\(S\\) and \\(T\\) be subspaces of \\(V\\). We say that a subspace \\(W\\) of \\(V\\) is a direct sum of the subspaces \\(S\\) and \\(T\\) if \\(W=S+T\\) and \\(S\\cap T=\\{\\veczero\\}\\). When the sum of \\(S\\) and \\(T\\) is direct we use the notation \\(W=S\\oplus T\\).\n\n\nLemma 1.21 Let \\(V\\) be a vector space over the field \\(\\F\\) and suppose that \\(S\\) and \\(T\\) are subspaces of \\(V\\) such that \\(W=S\\oplus T\\). Then every element \\(\\vecw\\in W\\) can be written uniquely in the form \\(\\vecw=\\textbf{s}+\\textbf{t}\\) where \\(\\textbf{s}\\in S\\) and \\(\\textbf{t}\\in T\\).\n\n\n\nProof. \nSuppose there are elements \\(\\vec{s},\\vec{s}' \\in S\\), \\(\\vec{t},\\vec{t}' \\in T\\) such that \\(\\vec{s} = \\vec{t} = \\vec{s}' + \\vec{t}'\\). It then follows that \\(\\vec{s} - \\vec{s}' = \\vec{t}' -\\vec{t}\\). Therefore \\(\\vec{s}-\\vec{s}' = \\vec{t}'-\\vec{t} \\in S \\cap T\\). Since \\(S \\cap T = \\{\\vec{0}\\}\\), then \\(\\vec{s}-\\vec{s}' = \\vec{0}= \\vec{t}-\\vec{t}'\\) and so \\(\\vec{s} = \\vec{s}'\\) and \\(\\vec{t} = \\vec{t}'\\) as required.\n\n\n\nExample 1.3  \n\nLet \\(\\vece_x= (1,0,0)\\), \\(\\vece_{y} = (0,1,0)\\) and \\(\\vec{e}_{z} = (0,0,1)\\). Then, as these are the standard basis for \\(\\R^3\\) every vector in \\(\\R^3\\) can be written as a linear combination of \\(\\vece_x, \\vece_y\\) and \\(\\vece_z\\). That is \\(\\R^3 = \\spn(\\vece_x) + \\spn(\\vece_y) + \\spn(\\vece_z)\\). Notice that for any pair \\((t,u) \\in \\{(x,y),(x,z),(y,z)\\}\\) \\(\\spn(\\vece_{t}) \\cap \\spn(\\vece_{u}) = \\{\\vec{0}\\}\\). Moreover, for \\(v \\in \\{x,y,z\\}\\backslash\\{t,u\\}\\) \\((\\spn(\\vece_{t}) + \\spn(\\vece_{u})) \\cap (\\spn(\\vece_{v})) = \\{\\vec{0}\\}\\).\nIt follows that \\(\\R^3= \\spn(\\vece_x) \\oplus \\spn(\\vece_y) \\oplus \\spn(\\vece_{z})\\).\n\n\n\nLemma 1.22 Let \\(V\\) be a finite dimensional vector space over the field \\(\\F\\) and let \\(S\\) and \\(T\\) be subspaces of \\(V\\) such that the sum of \\(S\\) and \\(T\\) is a direct sum. Suppose that \\((\\textbf{e}_{1},\\textbf{e}_{2},...,\\textbf{e}_{m})\\) is a basis of \\(S\\) and \\((\\textbf{f}_{1},\\textbf{f}_{2},...,\\textbf{f}_{n})\\) is a basis of \\(T\\). Then,\n\n\\((\\textbf{e}_{1},...,\\textbf{e}_{m},\\textbf{f}_{1},...,\\textbf{f}_{n})\\) is a basis of \\(S\\oplus T\\),\n\\(\\dimn(S\\oplus T)=\\dimn(S)+\\dimn(T)\\).\n\n\n\n\nProof. \nWe have by Theorem 1.4 \\[ \\dim(S + T) = \\dim(S) + \\dim(T) - \\dim(S \\cap T) =  \\dim(S) + \\dim(T)\\] since \\(S \\cap T= \\{\\vec{0}\\}\\).\nNow since \\(S \\oplus T\\) contains both \\(S\\) and \\(T\\), then \\(S \\oplus T\\) contains \\(S+T\\). However as \\(S \\oplus T\\) is spanned by \\((\\textbf{e}_{1},...,\\textbf{e}_{m},\\textbf{f}_{1},...,\\textbf{f}_{n})\\), we have:\n\\[\\dim(S) + \\dim(T) \\le \\dim(S \\oplus T) \\le \\dim(S) + \\dim(T)\\] Therefore \\(\\dim(S \\oplus T) = \\dim(S) + \\dim(T)\\) and \\((\\textbf{e}_{1},...,\\textbf{e}_{m},\\textbf{f}_{1},...,\\textbf{f}_{n})\\) is in fact a basis for \\(S \\oplus T\\).\n\n\n\n\n1.2.2 Back To Matrices\nLet \\(A=(a_{ij})\\in \\R_{m\\times n}\\) and recall that the matrix product of \\(A\\) with a column \\(\\vecX\\in \\R_{n\\times 1}\\) produces a column \\(A\\vecX\\in \\R_{m\\times 1}\\) where the entry in the \\(i\\)th row is the element \\[a_{i1}x_{1}+a_{i2}x_{2}+...+a_{in}x_{n}.\\] Of course we can view columns and rows in the matrix \\(A\\) as elements of \\(\\R^{m}\\) or \\(\\R^{n}\\) respectively.\n\nDefinition 1.13 (Row space and column space) With the matrix \\(A\\) as above, we define the \\(i\\)th row vector of \\(A\\) to be the vector \\(\\textbf{r}_{i}=(a_{i1},a_{i2},...,a_{in})\\in \\mathbb{R}^{n}\\) and the \\(j\\)th column vector of \\(A\\) to be the vector \\(\\textbf{c}_{j}=(a_{1j},a_{2j},...,a_{mj})\\in \\mathbb{R}^{m}\\). The row space of \\(A\\) is the subspace span\\((\\textbf{r}_{1},...,\\textbf{r}_{m})\\subset \\R^{n}\\). This subspace is denoted \\(\\row(A)\\) The column space of \\(A\\) is the subspace span\\((\\textbf{c}_{1},...,\\textbf{c}_{n})\\subset \\R^{m}\\). This subspace is denoted \\(\\col(A)\\).\n\nThere are some important points to note about row-echelon matrices.\n\n\n\n\n\n\n\nImportant\n\n\n\nA matrix is said to be in row-echelon form if the following conditions hold:\n\nthe first non-zero entry in a row is \\(1\\) (called the pivot);\nany zero rows are grouped together at the bottom of the matrix;\nif row \\(i\\) is above row \\(j\\) (and both are non-zero), then the pivot in row \\(i\\) is to the left of the pivot in row \\(j\\).\n\nA matrix is said to be in reduced row-echelon form if it is in row-echelon form and each column containing a pivot has zeros in every entry excluding the pivot.\n\n\n\n\nLemma 1.23 Let \\(A,B\\in \\R_{m\\times n}\\). If \\(A\\) and \\(B\\) are row-equivalent then \\(\\row(A)=\\row(B)\\).\n\n\n\nProof. \nSuppose \\(B\\) is obtained from \\(A\\) by an elementary row operation. This means that one row of \\(B\\) has been replaced by a linear combination of the rows of \\(A\\). Therefore \\(\\row(B) \\subseteq \\row(A)\\).\nTo see that \\(\\row(A) \\subseteq \\row(B)\\) observe that the row of \\(A\\) that is missing from \\(B\\) can be obtained as a linear combination of the rows of \\(B\\) simply by re-arranging to obtain an expression for the missing row vector.\nTherefore \\(\\row(A) = \\row(B)\\). The result is now an easy application of induction.\n\n\nIf we want to understand \\(\\row(A)\\), then we need a basis, that is a sequence of vectors that spans and is linearly independent. The following helps:\n\nLemma 1.24 The non-zero rows in a row-echelon matrix form an linearly independent sequence.\n\n\n\nProof. \nThe empty sequence is trivially linearly independent. Therefore if the sequence of non-zero rows of \\(E\\) is empty, the result holds.\nWe may assume that \\(E\\) has some non-zero rows. Let \\(\\vec{r}_1, \\vec{r_2}, \\ldots, \\vec{r}_{m}\\) be the non-zero rows (from top to bottom). Let \\(r_1, r_2, \\ldots, r_m \\in \\F\\) and consider the expression: \\[r_1 \\vec{r}_{1} + r_2 \\vec{r}_{2}  + \\ldots + r_{m}\\vec{r}_{m} = \\vec{0}.\\] By definition of the row-echelon form, the first non-zero position of \\(\\vec{r}_1\\) is to the left of all non-zero positions in the remaining row vectors \\(\\vec{r}_2 \\ldots \\vec{r}_{m}\\). Therefore \\(r_1 = 0\\). We may then repeat the argument to get \\(r_2 = 0, r_3 = 0,\\ldots, r_m= 0\\).\nTherefore the sequence \\((\\vec{r}_1, \\vec{r_2}, \\ldots, \\vec{r}_{m})\\) is linearly independent.\n\n\n\nConsider the following example:\n\n\n\n\n\n\n\n\n\nThe first leading entry, implies that \\(r_1 = 0\\), the second implies that \\(r_2\\) is equal to \\(0\\), and then \\(r_3\\) must be zero as well.\n\n\nLemma 1.25 The sequence of non-zero rows in a row-echelon matrix \\(E\\) is a basis for the row space, \\(\\row(E)\\), and for the row space of every matrix row-equivalent to \\(E\\).\n\n\n\nProof. \nAny matrix row-equivalent to \\(E\\) has the same row space as \\(E\\) so it suffices to show only the first half of the statement.\nBy Lemma 1.24, the non-zero rows of \\(E\\) are linearly independent. However, the row space of \\(E\\) is also spanned by the sequence of its non-zero rows (if they are all zero then \\(\\row(E) = \\{\\vec{0}\\}\\) which is spanned by the empty sequence). Therefore the non-zero rows of \\(E\\) are a basis for \\(\\row(E)\\).\n\n\n\nExample 1.4 Find a basis of the subspace, \\(S\\), of \\(\\R^{4}\\) spanned by the sequence of vectors \\[((1,0,1,2),\\:(2,3,0,1),\\:(-1,1,1,-2),\\:(1,5,3,-1)).\\]\n\nThe first step is to interpret the vectors as the rows of some matrix \\(M\\), then we apply elementary row operations to until we get a matrix \\(E\\) in row-echelon form row equivalent to \\(M\\). The non-zero rows of \\(E\\) then give a basis for \\(S\\).\nWe have: \\[\\begin{align*}\n\\begin{pmatrix}\n1 & 0 & 1 & 2 \\\\\n2 & 3 & 0 & 1 \\\\\n-1 & 1 & 1 & -2 \\\\\n1 & 5 & 3 & -1\n\\end{pmatrix}  \\begin{array}{c} \\\\ r_2 \\to r_2 - 2r_1 \\\\ r_3 \\to r_3 + r_1 \\\\ r_4 \\to r_4 - r_1  \\end{array}& \\begin{pmatrix}\n1 & 0 & 1 & 2 \\\\\n0 & 3 & -2 & -3 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 5 & 2 & -3\n\\end{pmatrix} \\\\\n\\begin{array}{c} \\\\ r_2 \\leftrightarrow r_3 \\\\ \\\\  \\end{array}& \\begin{pmatrix}\n1 & 0 & 1 & 2 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 3 & -2 & -3 \\\\\n0 & 5 & 2 & -3\n\\end{pmatrix} \\\\\n\\begin{array}{c} \\\\  \\\\ r_3 \\to r_3 - 3r_2 \\\\ r_4 \\to r_4 - 5r_2  \\end{array} &\\begin{pmatrix}\n1 & 0 & 1 & 2 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 0 & -8 & -3 \\\\\n0 & 0 & -8 & -3\n\\end{pmatrix}\\\\\n\\begin{array}{c} \\\\  \\\\ r_3 \\to r_3/8  \\\\ r_4 \\to r_4 - r_3  \\end{array} & \\begin{pmatrix}\n1 & 0 & 1 & 2 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 0 & 1 & 3/8 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}\n\\end{align*}\\]\nTherefore the sequence \\(\\left((1,0,1,2),(0,1,2,0),(0,0,1,3/8)\\right)\\) is a basis for \\(S\\).\n\n\nSince we have identified the row and column subspaces, it makes sense to think about their dimensions.\n\nDefinition 1.14 (Rank) With the matrix \\(A\\) as above, we define the row rank of \\(A\\) to be the quantity \\(\\dimn(\\row(A))\\) and the column rank of \\(A\\) to be the quantity \\(\\dimn(\\col(A))\\). The nullity of the matrix \\(A\\) is the quantity \\(\\dimn(N(A))\\) with \\(N(A)\\) being the subspace identified in Lemma 1.8. This will be denoted \\(\\nul(A)\\).\n\n\n\n\n\n\n\nNote\n\n\n\nWe remind the reader that \\[N(A) = \\{\\vecx \\in \\R_{n \\times 1} \\where A \\vecx = \\veczero \\}.\\]\n\n\n\nTheorem 1.5 The row and column rank of a matrix \\(A\\in \\R_{m\\times n}\\) are equal.\n\n\n\nProof. \nLet \\(R\\) be the reduced row-echelon form of \\(A\\). Then \\(\\row(A) = \\row (R)\\). It is not necessarily the case that \\(\\row(R) = \\col(A)\\). However what is true is that if a column of \\(A\\) can be written as a linear combination of the other columns, the corresponding column of \\(R\\) can be written as a linear combination of the other columns of \\(R\\) — that is the columns of \\(R\\) and the columns of \\(A\\) satisfy the same linear dependence relations. (The easiest way to see this is to show that if \\(B\\) is row equivalent to \\(A\\) the the columns of \\(A\\) and the columns of \\(B\\) satisfy the same linear dependence relations and apply induction.) Thus the dimension of the column space of \\(R\\) is equal to the dimension of the column space of \\(A\\).\nNow the columns of \\(R\\) containing the pivots of \\(R\\) are linearly independent and span the column space of \\(R\\). Therefore, \\(\\dim(\\row(R)) = \\dim(\\col(R)) = \\dim(\\col(A))\\) is precisely the number of non-zero rows of \\(R\\).\n\n\nFrom now on we will use the notation \\(\\rank(A)\\) to denote the column/row rank of a matrix \\(A\\in\\R_{m\\times n}\\).\n\nExample 1.5  \n\nConsider the matrix \\[A = \\begin{pmatrix} 1 & -1 & 2 & -1 \\\\ 1 & -1 & 3 & 2 \\\\ -2 & 2 & -3 & 5 \\end{pmatrix}.\\]\nWe can compute the reduced echelon form of \\(A\\), this is the matrix \\(R\\) below: \\[R = \\begin{pmatrix}\n        1 & -1 & 0 & -7 \\\\\n        0 & 0 & 1 & 3 \\\\\n        0 & 0 & 0 & 0\n      \\end{pmatrix}\\]\nNotice that the sequence of vectors consisting of columns one and three is linearly independent and the other two columns are in the span of this sequence. Thus we see that the rank of \\(A\\) is \\(2\\)\nSet \\[c_1 := \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}, \\quad \\text{and} \\quad c_2 := \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}\\] and notice that \\(c_1\\) and \\(c_2\\) are some of the standard basis vectors of \\(\\R^3\\).\nLet us also compute the null space of \\(A\\). For this we need to solve the equation: \\(A \\vec{x} = \\vec{0}\\) (note that \\(\\vec{x} \\in \\R^4\\) for this multiplication to be defined). Writing \\[\\vec{x}= \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix},\\] we have \\(A\\vec{x} = 0\\) if and only if the following system of linear equations hold: \\[\\begin{align*}\n                       x_1 -x_2 + 2x_3 -x_4 &= 0 \\\\\n                      x_1 - x_2 + 3x_3 + 2x_4 &= 0 \\\\\n                      -2x_1 + 2x_2 -3x_3 + 5x_4 &= 0\n\\end{align*}\\] The matrix of coefficients is \\(A\\), and we can apply row operations to reduce \\(A\\) to its echelon form \\(R\\), therefore, we can replace the system of equations above with the following equivalent one: \\[\\begin{align*}\nx_1 -x_2 + 0x_3 - 7x_4 &= 0 \\\\\n    0x_1 + 0 x_2 + x_3 + 3x_4 &= 0\n\\end{align*}\\]. We have two equations in \\(4\\) unknowns, so we need 2 parameters, we can choose \\(x_2\\) and \\(x_4\\). From the second equation \\(x_3 = -3x_4\\); from the first: \\(x_1 = x_2 + 7x_4\\). We have \\[N(A) := \\left\\{ \\begin{pmatrix} x_2 + 7x_4 \\\\ x_2 \\\\ -3x_4 \\\\ x_4 \\end{pmatrix} : x_2, x_4 \\in \\R \\right\\} = \\spn\\left( \\begin{pmatrix} 1  \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix}   7 \\\\ 0 \\\\ -3 \\\\ 1 \\end{pmatrix} \\right).\\] Therefore, the nullity of \\(A\\) is two since \\[\\left( \\begin{pmatrix} 1  \\\\ 1  \\\\ 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix}   7 \\\\ 0 \\\\ -3 \\\\ 1 \\end{pmatrix} \\right)\\] is a basis for the null space.\nObserve that \\(\\rank(A) + \\nul(A) = 4\\) which is the dimension of \\(\\R^4\\)."
  },
  {
    "objectID": "01-intro.html#sec-sheet1",
    "href": "01-intro.html#sec-sheet1",
    "title": "1  Introduction and Revision",
    "section": "1.3 Problem Sheet 1",
    "text": "1.3 Problem Sheet 1\nFor the example class in Week 2.\n\n\n\n\nQuestion R.1\n\n\nVerify that the set of all positive real numbers, \\(\\R_{>0}\\), is a vector space over \\(\\R\\) when given the addition and scalar multiplication defined below: \\[\\begin{equation*}\nx\\oplus y=xy,\\:\\:\\:\\:\\:\\: \\lambda x=x^{\\lambda}\n\\end{equation*}\\] for all positive real numbers \\(x,y\\in\\R_{>0}\\) and for all \\(\\lambda\\in\\R\\).\n\n\n\n\n\nShow Solution R.1\n\n\n\n\nSolution R.1\n\n\nObserve that here we need to check ALL the vector space axioms.\nA0. This Holds since for all \\(x,y\\in\\R_{>0}\\), \\(x\\oplus y=xy\\in\\R_{>0}\\).\nA1. Holds since for all \\(x,y,z\\in\\R_{>0}\\), \\[\\begin{align*}\n    x\\oplus(y\\oplus z) & =x\\oplus yz \\\\\n    & =x(yz) \\\\\n    & = (xy)z \\\\\n    & = xy\\oplus z \\\\\n    & = (x\\oplus y)\\oplus z.\n    \\end{align*}\\]\nA2. Holds since for all \\(x,y\\in\\R_{>0}\\), \\(x\\oplus y=xy=yx=y\\oplus x\\).\nA3. Holds since \\(1\\in\\R_{>0}\\) and, for all \\(x\\in\\R_{>0}\\), \\(x\\oplus 1=x1=x\\) and \\(1\\oplus x = 1x=x\\). (Note, in particular, that the role of the zero vector here is played by the real number \\(1\\)).\nA4. Holds since, for all \\(x\\in\\R_{>0}\\), \\(1/x\\in\\R_{>0}\\) and \\(x\\oplus 1/x=x(1/x)=1\\) and \\(1/x\\oplus x=(1/x)x=1\\). Hence the negative of \\(x\\in\\R_{>0}\\) is the element \\(1/x\\).\nM0. Holds since for all \\(x\\in\\R_{>0}\\), \\(\\lambda\\in\\R\\), \\(\\lambda x= x^{\\lambda}\\in\\R_{>0}\\).\nM1. Holds since for all \\(x,y\\in\\R_{>0}\\) and for all \\(\\lambda\\in\\R\\), \\[\\begin{align*}\n  \\lambda(x\\oplus y) & = \\lambda(xy) \\\\\n  & = (xy)^{\\lambda} \\\\\n  & = x^{\\lambda}y^{\\lambda} \\\\\n  & = \\lambda x\\oplus \\lambda y.\n  \\end{align*}\\] M2. Holds since for all \\(x\\in\\R_{>0}\\) and for all \\(\\lambda,\\mu\\in\\R\\), \\[\\begin{align*}\n  (\\lambda+\\mu) x & = x^{\\lambda+\\mu} \\\\\n  & = x^{\\lambda}x^{\\mu} \\\\\n  & = \\lambda x\\oplus \\mu x.\n  \\end{align*}\\] M3. Holds since for all \\(x\\in\\R_{>0}\\) and for all \\(\\lambda,\\mu\\in\\R\\), \\[\\begin{align*}\n  \\lambda(\\mu x) & = \\lambda x^{\\mu} \\\\\n  & = (x^{\\mu})^{\\lambda} \\\\\n  & = x^{\\mu\\lambda} \\\\\n  & = x^{\\lambda\\mu} \\\\\n  & = (\\lambda\\mu) x.\n  \\end{align*}\\] M4. Holds since for all \\(x\\in\\R_{>0}\\) we have \\(1 x=x^{1}=x\\).\nThis shows that all the vector space axioms hold and hence \\(\\R_{>0}\\) equipped with this scalar multiplication and addition is a vector space.\n\n\n\n\n\n\n\nQuestion R.2\n\n\nConsider \\(\\R^{2}\\) with the usual addition but with scalar multiplication defined as \\[\\begin{equation*}\n\\lambda(x,y)=(\\lambda x,y)\n\\end{equation*}\\] for all \\((x,y)\\in\\R^{2}\\), \\(\\lambda\\in\\R\\). Show that \\(\\R\\) equipped with the usual addition and this scalar multiplication is not a vector space.\n\n\n\n\n\nShow Solution R.2\n\n\n\n\nSolution R.2\n\n\nHere we just need to find one axiom where it all goes a bit pear-shaped and obviously it will be something to do with this scalar multiplication.\nAxiom (M2) is the one which causes us trouble here. Consider the vector \\(\\vecx=(1,1)\\in\\R^{2}\\) and \\(\\lambda=\\mu=1\\). Then \\[\\begin{equation*}\n\\lambda\\vecx+\\mu\\vecx=(1,1)+(1,1)=(2,2).\n\\end{equation*}\\] However \\[\\begin{equation*}\n(\\lambda+\\mu)\\vecx=2(1,1)=(2,1).\n\\end{equation*}\\] Hence \\((\\lambda+\\mu)\\vecx\\neq \\lambda\\vecx+\\mu\\vecx\\) and so (M2) fails and \\(\\R^{2}\\) with this addition and scalar multiplication is not a vector space.\n\n\n\n\n\n\n\nQuestion R.3\n\n\nShow that the subset \\[A=\\{(x,y,z) \\where x+2y+3z=0\\}\\] is a subspace of \\(\\R^{3}\\).\n\n\n\n\n\nShow Solution R.3\n\n\n\n\nSolution R.3\n\n\nObserve that \\(A\\subset \\mathbb{R}^{3}\\). Now \\((0,0,0)\\in A\\) since \\(0+2\\times 0+3\\times 0=0\\) and so \\(A\\neq\\phi\\).\nLet \\((a,b,c),(x,y,z)\\in A\\). Then \\(a+2b+3c=x+2y+3z=0\\). Now \\((a,b,c)+(x,y,z)=(a+x,b+y,c+z)\\) and \\[\\begin{align*}\na+x+2(b+y)+3(c+z) & =a+x+2b+2y+3c+3z \\\\\n& =(a+2b+3c)+(x+2y+3z) \\\\\n& =0+0 \\\\\n& =0.\n\\end{align*}\\] Hence \\((a,b,c)+(x,y,z)\\in A\\).\nFinally, let \\((x,y,z)\\in A\\) and \\(\\lambda\\in\\R\\). Then \\(\\lambda(x,y,z)=(\\lambda{x},\\lambda{y},\\lambda{z})\\) and \\[\n\\lambda{x}+2\\lambda{y}+3\\lambda{z}=\\lambda(x+2y+3z)=\\lambda(0)=0.\n\\] Hence \\(\\lambda(x,y,z)\\in A\\) and \\(A\\) is a subspace of \\(\\R^{3}\\).\n\n\n\n\n\n\n\nQuestion R.4\n\n\nDetermine whether each of the following sets are subspaces of \\(\\R^{2}\\). You should either prove that the set is a subspace or provide an appropriate counterexample as to why the set does not form a subspace.\n\n\\(A=\\{(x,y)\\in\\R^{2} \\where y=2x\\}\\).\n\\(B=\\{(x,y)\\in\\R^{2} \\where x\\geq 0,\\:\\:y\\geq 0\\}\\).\n\\(C=\\{(x,y)\\in\\R^{2} \\where x=0\\}\\).\n\\(D=\\{(x,y)\\in\\R^{2} \\where xy\\geq 0\\}\\).\n\n\n\n\n\n\nShow Solution R.4\n\n\n\n\nSolution R.4\n\n\n\n\\(A\\) is a subspace of \\(\\R^{2}\\). Note that \\(A\\subset\\R^{2}\\). The zero vector \\((0,0)\\in A\\) so \\(A\\neq\\phi\\).\nNow let \\((x,2x),(y,2y)\\in A\\). Then \\[\n(x,2x)+(y,2y)=(x+y,2x+2y)=(x+y,2(x+y))\\in A.\n\\] Finally, let \\((x,2x)\\in A\\) and \\(\\lambda\\in\\mathbb{R}\\). Then \\[\n\\lambda(x,2x)=(\\lambda{x},2(\\lambda{x}))\\in A.\n\\] Hence \\(A\\) is a subspace of \\(\\R^{2}\\).\n\\(B\\) is not a subspace of \\(\\R^{2}\\). Consider the vector \\(\\vecx=(1,1)\\in B\\). Then we have \\[-1\\vecx=(-1,-1)\\notin B.\\] Hence \\(B\\) is not closed under scalar multiplication and so is not a subspace of \\(\\R^{2}\\).\n\\(C\\) is a subspace of \\(\\R^{2}\\). Note that \\(C\\subset\\R^{2}\\). The zero vector \\((0,0)\\in C\\) so \\(C\\neq\\phi\\).\nNow let \\(\\vecx=(0,a),\\vecy=(0,b)\\in C\\). Then \\[\\vecx+\\vecy=(0,a+b)\\in C.\\] Finally, let \\(\\vecx=(0,a)\\in C\\) and \\(\\lambda\\in\\R\\). Then \\[\\lambda\\vecx=(0,\\lambda{a})\\in C.\\] Hence \\(C\\) is a subspace of \\(\\R^{2}\\).\n\\(D\\) is not a subspace. The vectors \\((1,1)\\) and \\((-2,0)\\) are both in the set \\(D\\) but \\[(1,1)+(-2,0)=(-1,1)\\notin D.\\] Hence \\(D\\) is not closed under addition and is not a subspace of \\(\\R^{2}\\).\n\n\n\n\n\n\n\n\nQuestion R.5\n\n\nIn \\(\\R_{2\\times 2}\\), let \\[A=\\left(\\begin{array}{cc} 2 & -1 \\\\ 0 & 1\\end{array}\\right),\\:\\:\\: B=\\left(\\begin{array}{cc} 1 & 0 \\\\ -2 & -3\\end{array}\\right),\\:\\:\\: C=\\left(\\begin{array}{cc} 0 & 3 \\\\ -1 & 1\\end{array}\\right),\\:\\:\\: D=\\left(\\begin{array}{cc} 3 & 7 \\\\ -1 & 8\\end{array}\\right).\\]\n\nShow that \\(A\\in\\spn(B,C,D)\\).\nFind necessary and sufficient conditions on \\(a,b,c,d\\in\\R\\) for \\(\\left(\\begin{array}{cc} a & b \\\\ c & d\\end{array}\\right)\\in\\spn(A,B)\\).\n\n\n\n\n\n\nShow Solution R.5\n\n\n\n\nSolution R.5\n\n\n\nWe require \\[\\begin{align*}\n  \\left(\\begin{array}{cc} 2 & -1 \\\\ 0 & 1\\end{array}\\right)& = \\alpha\\left(\\begin{array}{cc} 1 & 0 \\\\ -2 & -3\\end{array}\\right)+\\beta\\left(\\begin{array}{cc} 0 & 3 \\\\ -1 & 1\\end{array}\\right)+\\gamma\\left(\\begin{array}{cc} 3 & 7 \\\\ -1 & 8\\end{array}\\right) \\\\\n  & = \\left(\\begin{array}{cc} \\alpha & 0 \\\\ -2\\alpha & -3\\alpha\\end{array}\\right)+\\left(\\begin{array}{cc} 0 & 3\\beta \\\\ -\\beta & \\beta\\end{array}\\right)+\\left(\\begin{array}{cc} 3\\gamma & 7\\gamma \\\\ -\\gamma & 8\\gamma\\end{array}\\right).\n  \\end{align*}\\] This gives rise to the system of equations \\[\\begin{align*}\n  \\alpha+3\\gamma & = 2 \\\\\n  3\\beta+7\\gamma & = -1 \\\\\n  -2\\alpha-\\beta-\\gamma & = 0 \\\\\n  -3\\alpha+\\beta+8\\gamma & = 1.\n  \\end{align*}\\] As you should verify via standard echelon reduction, this system has the unique solution \\(\\alpha=\\frac{1}{2}\\), \\(\\beta=-\\frac{3}{2}\\) and \\(\\gamma=\\frac{1}{2}\\). Hence \\(A\\in{\\rm{span}}(B,C,D)\\) since \\[\\begin{equation*}\n  A=\\frac{1}{2}B-\\frac{3}{2}C+\\frac{1}{2}D.\n  \\end{equation*}\\]\nWe require \\[\\begin{align*}\n  \\left(\\begin{array}{cc} a & b \\\\ c & d\\end{array}\\right) & = \\alpha\\left(\\begin{array}{cc} 2 & -1 \\\\ 0 & 1\\end{array}\\right)+\\beta\\left(\\begin{array}{cc} 1 & 0 \\\\ -2 & -3\\end{array}\\right) \\\\\n  & = \\left(\\begin{array}{cc} 2\\alpha & -\\alpha \\\\ 0 & \\alpha\\end{array}\\right)+\\left(\\begin{array}{cc} \\beta & 0 \\\\ -2\\beta & -3\\beta\\end{array}\\right).\n  \\end{align*}\\] This leads to the system of equations \\[\\begin{align*}\n  2\\alpha+\\beta & = a \\\\\n  -\\alpha & = b \\\\\n  -2\\beta & = c \\\\\n  \\alpha-3\\beta & = d\n  \\end{align*}\\] with augmented matrix \\[\\begin{equation*}\n  \\left(\\begin{array}{cc|c} 2 & 1 & a \\\\ -1 & 0 & b \\\\ 0 & -2 & c \\\\ 1 & -3 & d\\end{array}\\right)\\sim\\left(\\begin{array}{cc|c} 1 & 0 & -b \\\\ 0 & 1 & 2b+a \\\\ 0 & 0 & 2a+4b+c \\\\ 0 & 0 & 3a+7b+d\\end{array}\\right).\n  \\end{equation*}\\] Hence necessary and sufficient conditions for \\(\\left(\\begin{array}{cc} a & b \\\\ c & d\\end{array}\\right)\\in{\\rm{span}}(A,B)\\) are \\(3a+7b+d=0\\) and \\(2a+4b+c=0\\).\n\n\n\n\n\n\n\n\nQuestion R.6\n\n\nFor each of the following vector spaces, find a basis and hence state the dimension of the given space.\n\n\\(C(A)=\\{B\\in\\R_{2\\times 2} \\where AB=BA\\}\\) where \\(A=\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right)\\).\n\\(\\{p\\in P_{5} \\where \\int_{0}^{1} p(x)dx=0\\}\\).\n\n\n\n\n\n\nShow Solution R.6\n\n\n\n\nSolution R.6\n\n\n\nLet \\(B=\\left(\\begin{array}{cc} a & b \\\\ c & d\\end{array}\\right)\\in\\mathbb{R}_{2\\times 2}\\). Then \\[\\begin{align*}\n  B\\in C(A) & \\Leftrightarrow AB=BA \\\\\n  & \\Leftrightarrow \\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right)\\left(\\begin{array}{cc} a & b \\\\ c & d\\end{array}\\right)=\\left(\\begin{array}{cc} a & b \\\\ c & d\\end{array}\\right)\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right) \\\\\n  & \\Leftrightarrow \\left(\\begin{array}{cc} a+c & b+d \\\\ c & d\\end{array}\\right)=\\left(\\begin{array}{cc} a & a+b \\\\ c & c+d\\end{array}\\right) \\\\\n  & \\Leftrightarrow c=0,\\:\\:a=d.\n  \\end{align*}\\] Hence \\[\\begin{equation*}\n  C(A)=\\Bigg\\{\\left(\\begin{array}{cc} a & b \\\\ 0 & a\\end{array}\\right) : a,b\\in\\mathbb{R}\\Bigg\\}.\n  \\end{equation*}\\] Hence if \\(B=\\left(\\begin{array}{cc} a & b \\\\ 0 & a\\end{array}\\right)\\in C(A)\\), then \\[\\begin{equation*}\n  B=a\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 1\\end{array}\\right)+b\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 0\\end{array}\\right).\n  \\end{equation*}\\] Thus the sequence \\[\\begin{equation*}\n  \\Bigg(\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 1\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 0\\end{array}\\right)\\Bigg)\n  \\end{equation*}\\] spans \\(C(A)\\). Now suppose that \\[\\begin{equation*}\n  \\alpha\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 1\\end{array}\\right)+\\beta\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 0\\end{array}\\right)=\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 0\\end{array}\\right).\\:\\:\\:\\:\\: (\\alpha,\\beta\\in\\mathbb{R})\n  \\end{equation*}\\] Then \\[\\begin{equation*}\n  \\left(\\begin{array}{cc} \\alpha & \\beta \\\\ 0 & \\alpha\\end{array}\\right)=\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 0\\end{array}\\right)\n  \\end{equation*}\\] and so equating entries, we see that we must have \\(\\alpha=\\beta=0\\). This demonstrates that the sequence \\[\\begin{equation*}\n  \\Bigg(\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 1\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 0\\end{array}\\right)\\Bigg)\n  \\end{equation*}\\] is L.I. It follows that the above sequence is a basis of \\(C(A)\\) and so dim\\((C(A))=2\\).\nLet \\[\\begin{equation*}\n  S=\\{p\\in P_{5} \\where \\int_{0}^{1} p(x)dx=0\\}.\n  \\end{equation*}\\] \\[\\begin{align*}\n  & p(x)=a+bx+cx^{2}+dx^{3}+ex^{4}+fx^{5}\\in S \\\\\n  & \\Leftrightarrow \\int_{0}^{1} a+bx+cx^{2}+dx^{3}+ex^{4}+fx^{5}\\:dx=0 \\\\\n  & \\Leftrightarrow \\Bigg[ax+\\frac{bx^{2}}{2}+\\frac{cx^{3}}{3}+\\frac{dx^{4}}{4}+\\frac{ex^{5}}{5}+\\frac{fx^{6}}{6}\\Bigg]_{0}^{1}=0 \\\\\n  & \\Leftrightarrow a+\\frac{b}{2}+\\frac{c}{3}+\\frac{d}{4}+\\frac{e}{5}+\\frac{f}{6}=0 \\\\\n  & \\Leftrightarrow a=-\\frac{f}{6}-\\frac{e}{5}-\\frac{d}{4}-\\frac{c}{3}-\\frac{b}{2} \\\\\n  & \\Leftrightarrow p(x)=(-\\frac{f}{6}-\\frac{e}{5}-\\frac{d}{4}-\\frac{c}{3}-\\frac{b}{2})+bx+cx^{2}+dx^{3}+ex^{4}+fx^{5} \\\\\n  & = b(-\\frac{1}{2}+x)+c(-\\frac{1}{3}+x^{2})+d(-\\frac{1}{4}+x^{3})+e(-\\frac{1}{5}+x^{4})+f(-\\frac{1}{6}+x^{5}).\n  \\end{align*}\\] Hence the sequence \\((-\\frac{1}{2}+x,\\:-\\frac{1}{3}+x^{2},\\:-\\frac{1}{4}+x^{3},\\:-\\frac{1}{5}+x^{4},\\:-\\frac{1}{6}+x^{5})\\) spans \\(S\\). Now suppose that \\[\\begin{equation*}\n  \\lambda_{1}(-\\frac{1}{2}+x)+\\lambda_{2}(-\\frac{1}{3}+x^{2})+\\lambda_{3}(-\\frac{1}{4}+x^{3})+\\lambda_{4}(-\\frac{1}{5}+x^{4})+\\lambda_{5}(-\\frac{1}{6}+x^{5})=0\\:\\:\\:\\:\\: (\\lambda_{i}\\in\\mathbb{R})\n  \\end{equation*}\\] Then \\[\\begin{equation*}\n  (-\\frac{\\lambda_{1}}{2}-\\frac{\\lambda_{2}}{3}-\\frac{\\lambda_{3}}{4}-\\frac{\\lambda_{4}}{5}-\\frac{\\lambda_{5}}{6})+\\lambda_{1}x+\\lambda_{2}x^{2}+\\lambda_{3}x^{3}+\\lambda_{4}x^{4}+\\lambda_{5}x^{5}=0\n  \\end{equation*}\\] and so equating up coefficients of powers of \\(x\\) we see that we must have \\(\\lambda_{1}=\\ldots=\\lambda_{5}=0\\). This shows that the sequence \\[(-\\frac{1}{2}+x,\\:-\\frac{1}{3}+x^{2},\\:-\\frac{1}{4}+x^{3},\\:-\\frac{1}{5}+x^{4},\\:-\\frac{1}{6}+x^{5})\\] is a basis of \\(S\\) and hence dim\\(S=5\\).\n\n\n\n\n\n\n\n\nQuestion 1.1\n\n\n\nShow that the sequence \\(B_{1}=((1,1,1),(0,1,1),(0,0,1))\\) is a basis of \\(\\R^{3}\\) and find the coordinate vector of \\((2,3,-1)\\) with respect to the basis \\(B_{1}\\).\nConsider the sequence \\[\nB_{2}=\\Bigg(\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 1\\end{array}\\right)\\Bigg)\n\\] in \\(\\R_{2\\times 2}\\). Show that \\(B_{2}\\) is a basis of \\(\\R_{2\\times 2}\\). Find the coordinate vector of \\(A=\\left(\\begin{array}{cc} 1 & 2 \\\\ 3 & 4\\end{array}\\right)\\) with respect to \\(B_{2}\\).\nFind the coordinate vector of \\(p(x)=1+2x-x^{2}\\) with respect to the basis \\[(1+x,x+x^{2},1+x^{2})\\] of \\(P_{2}\\).\n\n\n\n\n\n\nShow Solution 1.1\n\n\n\n\nSolution 1.1\n\n\n\nSuppose that \\[\\begin{equation*}\n  \\lambda_{1}(1,1,1)+\\lambda_{2}(0,1,1)+\\lambda_{3}(0,0,1)=(0,0,0).\\:\\:\\:\\:\\: (\\lambda_{i}\\in\\mathbb{R})\n  \\end{equation*}\\] Then \\[\\begin{equation*}\n  (\\lambda_{1},\\:\\lambda_{1}+\\lambda_{2},\\:\\lambda_{1}+\\lambda_{2}+\\lambda_{3})=(0,0,0).\n  \\end{equation*}\\] Hence from the first components, \\(\\lambda_{1}=0\\), then from the second components we see that \\(\\lambda_{2}=0\\) and finally from the third components, \\(\\lambda_{3}=0\\). This demonstrates that the sequence \\(B_{1}=((1,1,1),(0,1,1),(0,0,1))\\) is L.I. Hence the sequence \\(B_{1}\\) is L.I. and has length equal to dim\\((\\mathbb{R}^{3})=3\\) and so it follows that the sequence \\(B_{1}\\) is a basis of \\(\\mathbb{R}^{3}\\).\nTo find the coordinate vector of \\((2,3,-1)\\) w.r.t. \\(B_{1}\\), we must consider the equation \\[\\begin{equation*}\n\\alpha(1,1,1)+\\beta(0,1,1)+\\gamma(0,0,1)=(2,3,-1).\\:\\:\\:\\:\\: (\\alpha,\\beta,\\gamma\\in\\mathbb{R})\n\\end{equation*}\\] For \\(\\alpha,\\beta\\) and \\(\\gamma\\) to satisfy this equation, we require \\[\\begin{align*}\n\\alpha & = 2 \\\\\n\\alpha+\\beta & = 3 \\\\\n\\alpha+\\beta+\\gamma & = -1.\n\\end{align*}\\] Hence we require \\(\\alpha=2,\\beta=1\\) and \\(\\gamma=-4\\). Thus, the coordinate vector we require is \\((2,1,-4)\\).\nSuppose that \\[\\begin{equation*}\n  \\alpha\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right)+\\beta\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 0\\end{array}\\right)+\\gamma\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 0\\end{array}\\right)+\\delta\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 1\\end{array}\\right)=\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 0\\end{array}\\right).\n  \\end{equation*}\\] for \\(\\alpha,\\beta,\\gamma,\\delta\\in\\mathbb{R}\\). Then \\[\\begin{equation*}\n  \\left(\\begin{array}{cc} \\alpha+\\beta+\\gamma+\\delta & \\beta+\\gamma+\\delta \\\\ \\gamma+\\delta & \\delta\\end{array}\\right)=\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 0\\end{array}\\right).\n  \\end{equation*}\\] Equating up entries shows that we must have \\(\\alpha=\\beta=\\gamma=\\delta=0\\). This demonstrates that the sequence \\(B_{2}\\) is L.I. It follows that \\(B_{2}\\) is an L.I. sequence in \\(\\mathbb{R}_{2\\times 2}\\) with length equal to dim\\((\\mathbb{R}_{2\\times 2})=4\\) and so \\(B_{2}\\) is a basis of \\(\\mathbb{R}_{2\\times 2}\\).\nTo find the coordinate vector of \\(A=\\left(\\begin{array}{cc} 1 & 2 \\\\ 3 & 4\\end{array}\\right)\\) w.r.t. \\(B_{2}\\), we need to consider the equation \\[\\begin{equation*}\n\\lambda_{1}\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right)+\\lambda_{2}\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 0\\end{array}\\right)+\\lambda_{3}\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 0\\end{array}\\right)+\\lambda_{4}\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 1\\end{array}\\right)=\\left(\\begin{array}{cc} 1 & 2 \\\\ 3 & 4\\end{array}\\right).\n\\end{equation*}\\] for \\(\\lambda_{i}\\in\\mathbb{R}\\). i.e. \\[\\begin{equation*}\n\\left(\\begin{array}{cc} \\lambda_{1}+\\lambda_{2}+\\lambda_{3}+\\lambda_{4} & \\lambda_{2}+\\lambda_{3}+\\lambda_{4} \\\\ \\lambda_{3}+\\lambda_{4} & \\lambda_{4}\\end{array}\\right)=\\left(\\begin{array}{cc} 1 & 2 \\\\ 3 & 4\\end{array}\\right).\n\\end{equation*}\\] leading to the system of equations \\[\\begin{align*}\n\\lambda_{1}+\\lambda_{2}+\\lambda_{3}+\\lambda_{4} & = 1 \\\\\n\\lambda_{2}+\\lambda_{3}+\\lambda_{4} & = 2 \\\\\n\\lambda_{3}+\\lambda_{4} & = 3 \\\\\n\\lambda_{4} & = 4.\n\\end{align*}\\] Back substitution yields the solution \\(\\lambda_{1}=-1,\\lambda_{2}=-1,\\lambda_{3}=-1,\\lambda_{4}=4\\). Hence the required coordinate vector is \\((-1,-1,-1,4)\\).\nWe have \\[p(x) = 1 + 2x - x^2 = \\alpha(1+x) + \\beta(x + x^2) + \\gamma (1 + x^2)\\] for some \\(\\alpha, \\beta, \\gamma \\in \\R\\).\\\\ That is, \\[1+2x-x^2 = (\\alpha+\\gamma) + (\\alpha+\\beta)x + (\\beta+\\gamma)x^2.\\] Equating coefficients gives \\[\\begin{eqnarray*}\n  \\alpha + \\gamma &=& 1\\\\\n  \\alpha + \\beta &=& 2\\\\\n  \\beta + \\gamma &=& -1\n  \\end{eqnarray*}\\] This leads to the augmented matrix \\[\\left(\\begin{array}{ccc|c} 1 & 0 & 1 & 1 \\\\ 1 & 1 & 0 & 2 \\\\0 & 1 & 1 & -1\\end{array}\\right)  \\sim \\left(\\begin{array}{ccc|c} 1 & 0 & 0 & 2 \\\\0 & 1 & 0 & 0\\\\ 0 & 0 &1 & -1\\end{array}\\right).\\] So, we have the unique solution \\[\\alpha = 2, \\quad \\beta = 0, \\quad \\gamma = -1.\\] Hence \\[p(x) = 2(1+x) + 0(x+x^2) -1(1 + x^2),\\] and so the coordinate vector of \\(p(x)\\) with respect to the given basis is \\((2, 0, -1)\\).\n\n\n\n\n\n\n\n\nQuestion 1.2\n\n\nVerify that the sequence \\(L_{2}=\\Bigg(\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 1\\end{array}\\right),\\:\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right)\\Bigg)\\) is L.I. in \\(\\R_{2\\times 2}\\) and hence use the Exchange Lemma to extend \\(L_{2}\\) to a basis of \\(\\R_{2\\times 2}\\).\n\n\n\n\n\nShow Solution 1.2\n\n\n\n\nSolution 1.2\n\n\nSuppose that \\[\\begin{equation*}\n\\lambda_{1}\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 1\\end{array}\\right)+\\lambda_{2}\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right)=\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 0\\end{array}\\right).\\:\\:\\:\\:\\:\\: (\\lambda_{i}\\in\\mathbb{R})\n\\end{equation*}\\] i.e. \\[\\begin{equation*}\n\\left(\\begin{array}{cc} \\lambda_{2} & \\lambda_{1}+\\lambda_{2} \\\\ 0 & \\lambda_{1}+\\lambda_{2}\\end{array}\\right)=\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Equating up entries gives \\(\\lambda_{1}=\\lambda_{2}=0\\). This demonstrates that the sequence \\(L_{2}\\) is L.I.\nTo apply the Exchange Lemma, we require a L.I. sequence and a spanning sequence. In this example, we take the L.I. sequence \\(L_{2}\\) and the we use the standard basis as our spanning sequence. This is the sequence \\[\\begin{equation*}\n\\Bigg(\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 0 \\\\ 1 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 1\\end{array}\\right)\\Bigg).\n\\end{equation*}\\] Remember the Exchange Lemma says we want to chuck away some of our spanning matrices and let the L.I. ones take their places. We start with the matrix \\(\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 1\\end{array}\\right)\\) from \\(L_{2}\\). We write this as a linear combination of the matrices in the standard basis. Hence \\[\\begin{equation*}\n\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 1\\end{array}\\right)=\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 0\\end{array}\\right)+\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 1\\end{array}\\right).\n\\end{equation*}\\] The coefficient of the matrix \\(\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 0\\end{array}\\right)\\) is nonzero and hence the sequence \\[\\begin{equation*}\n\\Bigg(\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 1\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 0 \\\\ 1 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 1\\end{array}\\right)\\Bigg)\n\\end{equation*}\\] spans \\(\\mathbb{R}_{2\\times 2}\\).\nNow \\[\\begin{equation*}\n\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right)=\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right)+\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 1\\end{array}\\right).\n\\end{equation*}\\] Hence the sequence \\[\\begin{equation*}\n\\Bigg(\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 1\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 0 \\\\ 1 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 1\\end{array}\\right)\\Bigg)\n\\end{equation*}\\] spans \\(\\mathbb{R}_{2\\times 2}\\). Note that the length of this sequence is \\(4=\\:\\)dim\\((\\mathbb{R}_{2\\times 2})\\) and so we have a spanning sequence with length equal to the dimension of the space. It follows that \\[\\begin{equation*}\n\\Bigg(\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 1\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 0 \\\\ 1 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 1\\end{array}\\right)\\Bigg)\n\\end{equation*}\\] is a basis of \\(\\mathbb{R}_{2\\times 2}\\).\n\n\n\n\n\n\n\nQuestion 1.3\n\n\nConsider the subspaces \\[\\begin{align*}\nU & = \\{(x,y,z)\\in\\R^{3} \\where x+y+z=0\\}, \\\\\nW & = \\{(x,y,z)\\in\\R^{3} \\where x+2y-3z=0\\}.\n\\end{align*}\\] of \\(\\R^{3}\\).\n\nFind a basis of \\(U\\cap W\\).\nFind \\(\\dimn U\\) and \\(\\dimn W\\) and hence state the value of \\(\\dimn (U+W)\\). Deduce that \\(U+W=\\R^{3}\\).\n\n\n\n\n\n\nShow Solution 1.3\n\n\n\n\nSolution 1.3\n\n\n\n\\[\\begin{align*}\n  \\textbf{x}=(x,y,z)\\in U\\cap W & \\Leftrightarrow \\textbf{x}\\in U\\:\\:{\\rm and}\\:\\: \\textbf{x}\\in W \\\\\n  & \\Leftrightarrow x+y+z=0\\:\\:{\\rm and}\\:\\: x+2y-3z=0.\n  \\end{align*}\\] This system has augmented matrix \\[\\left(\\begin{array}{ccc|c} 1 & 1 & 1 & 0 \\\\ 1 & 2 & -3 & 0\\end{array}\\right)\\sim\\left(\\begin{array}{ccc|c} 1 & 0 & 5 & 0 \\\\ 0 & 1 & -4 & 0\\end{array}\\right).\\] So \\(x=-5t\\), \\(y=4t\\) for some \\(t\\in\\mathbb{R}\\). Hence \\(\\textbf{x}=(-5t,4t,t)=t(-5,4,1)\\) for some \\(t\\in\\mathbb{R}\\).\nIt follows that \\(U\\cap W\\) is 1-dimensional and a basis is \\(((-5,4,1))\\).\nLet \\(\\textbf{x}=(x,y,z)\\in U\\). Then \\(x+y+z=0\\Rightarrow z=-x-y\\). Hence \\[\\textbf{x}=(x,y,-x-y)=x(1,0,-1)+y(0,1,-1).\\] So the sequence \\(L_{1}=((1,0,-1),(0,1,-1))\\) spans \\(U\\). Additionally, \\[\\alpha(1,0,-1)+\\beta(0,1,-1)=(0,0,0)\\:\\:\\:\\:\\:\\:\\: (\\alpha,\\beta\\in\\mathbb{R})\\] implies that \\((\\alpha,\\beta,-\\alpha-\\beta)=(0,0,0)\\) and hence \\(\\alpha=\\beta=0\\). It follows that \\(L_{1}\\) is L.I. and so is a basis of \\(U\\) and dim\\(U=2\\).\nSimilarly, if \\(\\textbf{y}=(a,b,c)\\in W\\) then \\(a+2b-3c=0\\Rightarrow a=3c-2b\\). Hence \\[\\textbf{y}=(3c-2b,b,c)=b(-2,1,0)+c(3,0,1).\\] Hence the sequence \\(L_{2}=((-2,1,0),(3,0,1))\\) spans \\(W\\).\nNow \\[\\alpha(-2,1,0)+\\beta(3,0,1)=(0,0,0)\\:\\:\\:\\:\\:\\:\\: (\\alpha,\\beta\\in\\mathbb{R})\\] implies that \\((3\\beta-2\\alpha,\\alpha,\\beta)=(0,0,0)\\) and hence \\(\\alpha=\\beta=0\\). This shows that \\(L_{2}\\) is L.I. and it follows that \\(L_{2}\\) is a basis of \\(W\\). Hence dim\\(W=2\\).\nNow \\[{\\rm dim}(U+W)={\\rm dim}U+{\\rm dim}W-{\\rm dim}(U\\cap W)=2+2-1=3.\\] \\(U+W\\subseteq \\mathbb{R}^{3}\\) and hence we must have \\(U+W=\\mathbb{R}^{3}\\) by Lemma 1.19.\n\n\n\n\n\n\n\n\nQuestion 1.4\n\n\nLet \\(S\\), \\(T_{1}\\) and \\(T_{2}\\) be subspaces of \\(\\R^{7}\\) such that \\[\n\\R^{7}=S+T_{1}=S+T_{2}.\n\\] with \\(\\dimn S=3\\).\n\nUse the theorem on dimension of sums of subspaces to show that \\(\\dimn T_{i}\\geq 4\\) (\\(i=1,2\\))\nDeduce that \\(T_{1}\\cap T_{2}\\neq\\{\\veczero\\}\\).\n\n\n\n\n\n\nShow Solution 1.4\n\n\n\n\nSolution 1.4\n\n\n\nWe have \\[\\begin{equation*}\n  {\\rm{dim}}(S+T_{1})={\\rm{dim}}(S+T_{2})={\\rm{dim}}(\\mathbb{R}^{7})\n  \\end{equation*}\\] i.e. \\[\\begin{equation*}\n  {\\rm{dim}}S+{\\rm{dim}}T_{1}-{\\rm{dim}}(S\\cap T_{1})=7\n  \\end{equation*}\\] and \\[\\begin{equation*}\n  {\\rm{dim}}S+{\\rm{dim}}T_{2}-{\\rm{dim}}(S\\cap T_{2})=7.\n  \\end{equation*}\\] Hence \\[\\begin{equation*}\n  {\\rm{dim}}T_{1}=7-{\\rm{dim}}S+{\\rm{dim}}(S\\cap T_{1})=4+{\\rm{dim}}(S\\cap T_{1})\\geq 4\n  \\end{equation*}\\] and similarly \\[\\begin{equation*}\n  {\\rm{dim}}T_{2}=7-{\\rm{dim}}S+{\\rm{dim}}(S\\cap T_{2})=4+{\\rm{dim}}(S\\cap T_{2})\\geq 4.\n  \\end{equation*}\\]\nNow \\(T_{1}+ T_{2}\\subseteq \\mathbb{R}^{7}\\) and so dim\\((T_{1}+ T_{2})\\leq 7\\). Hence \\[\\begin{equation*}\n  {\\rm{dim}}(T_{1}\\cap T_{2})={\\rm{dim}}T_{1}+{\\rm{dim}}T_{2}-{\\rm{dim}}(T_{1}+T_{2})\\geq 4+4-7=1\n  \\end{equation*}\\] and so \\(T_{1}\\cap T_{2}\\neq\\{\\textbf{0}\\}\\).\n\n\n\n\n\n\n\n\nQuestion 1.5\n\n\nFind a basis of the subspace \\(N(A)=\\{X\\in\\R_{4\\times 1} \\where AX=\\veczero \\}\\) where \\[\nA=\\left(\\begin{array}{cccc} 1 & 1 & -1 & 1 \\\\ 2 & -1 & 0 & 2\\end{array}\\right).\n\\]\n\n\n\n\n\nShow Solution 1.5\n\n\n\n\nSolution 1.5\n\n\nLet \\(X={\\rm{col}}(x_{1},x_{2},x_{3},x_{4})\\). Then \\[\\begin{align*}\nX\\in N(A) & \\Leftrightarrow AX=0 \\\\\n& \\Leftrightarrow \\left(\\begin{array}{cccc} 1 & 1 & -1 & 1 \\\\ 2 & -1 & 0 & 2\\end{array}\\right)\\left(\\begin{array}{c} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4}\\end{array}\\right)=\\left(\\begin{array}{c} 0 \\\\ 0\\end{array}\\right).\n\\end{align*}\\] The augmented matrix here is \\[\\begin{equation*}\n\\left(\\begin{array}{cccc|c} 1 & 1 & -1 & 1 & 0 \\\\ 2 & -1 & 0 & 2 & 0\\end{array}\\right)\\sim\\left(\\begin{array}{cccc|c} 1 & 0 & -\\frac{1}{3} & 1 & 0 \\\\ 0 & 1 & -\\frac{2}{3} & 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Hence the solution set is \\[N(A)=\\left\\{\\left(\\begin{array}{c} \\frac{1}{3}s-t \\\\ \\frac{2}{3}s \\\\ s \\\\ t\\end{array}\\right) : s,t\\in\\mathbb{R}\\right\\}.\\] Take an arbitrary column \\[\\left(\\begin{array}{c} \\frac{1}{3}s-t \\\\ \\frac{2}{3}s \\\\ s \\\\ t\\end{array}\\right)\\in N(A).\\] Then \\[\\begin{equation*}\n\\left(\\begin{array}{c} \\frac{1}{3}s-t \\\\ \\frac{2}{3}s \\\\ s \\\\ t\\end{array}\\right)=\\frac{s}{3}\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 3 \\\\ 0\\end{array}\\right)+t\\left(\\begin{array}{c} -1 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right).\n\\end{equation*}\\] Hence the sequence \\[L=\\left(\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 3 \\\\ 0\\end{array}\\right),\\:\\left(\\begin{array}{c} -1 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right)\\right)\\]\nspans \\(N(A)\\).\nNow suppose that \\[\\lambda_{1}\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 3 \\\\ 0\\end{array}\\right)+\\lambda_{2}\\left(\\begin{array}{c} -1 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right)=\\left(\\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right).\\:\\:\\:\\:\\: (\\lambda_{1},\\lambda_{2}\\in\\mathbb{R}).\\]\nThen \\[\\left(\\begin{array}{c} \\lambda_{1}-\\lambda_{2} \\\\ 2\\lambda_{1} \\\\ 3\\lambda_{1} \\\\ \\lambda_{2}\\end{array}\\right)=\\left(\\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right)\\] and hence \\(\\lambda_{1}=\\lambda_{2}=0\\) on equating up the entries in rows \\(3\\) and \\(4\\) (say).\nHence the sequence \\(L\\) is L.I. so is a basis of \\(N(A)\\).\n\n\n\n\n\n\n\nQuestion 1.6\n\n\nFind the rank of the matrix \\[A=\\left(\\begin{array}{ccc} 1 & 2 & -4 \\\\ 2 & -1 & 2 \\\\ 1 & 1 & -2\\end{array}\\right).\\] Find also the dimension of the subspace \\(N(A)=\\{X\\in\\R_{3\\times 1} \\where AX=\\veczero \\}\\).\n\n\n\n\n\nShow Solution 1.6\n\n\n\n\nSolution 1.6\n\n\nWe have \\[\\begin{equation*}\n\\left(\\begin{array}{ccc} 1 & 2 & -4 \\\\ 2 & -1 & 2 \\\\ 1 & 1 & -2\\end{array}\\right)\\sim\\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & -2 \\\\ 0 & 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Hence the rank of \\(A\\) is \\(2\\) (the number of nonzero rows in RRE form).\nNow let \\(X=\\:\\)col\\((x,y,z)\\). Then \\[\\begin{align*}\nX\\in N(A) & \\Leftrightarrow AX=0 \\\\\n& \\Leftrightarrow \\left(\\begin{array}{ccc} 1 & 2 & -4 \\\\ 2 & -1 & 2 \\\\ 1 & 1 & -2\\end{array}\\right)\\left(\\begin{array}{c} x \\\\ y \\\\ z\\end{array}\\right)=\\left(\\begin{array}{c} 0 \\\\ 0 \\\\ 0\\end{array}\\right).\n\\end{align*}\\] The augmented matrix here is \\[\\begin{equation*}\n\\left(\\begin{array}{ccc|c} 1 & 2 & -4 & 0 \\\\ 2 & -1 & 2 & 0 \\\\ 1 & 1 & -2 & 0\\end{array}\\right)\\sim\\left(\\begin{array}{ccc|c} 1 & 0 & 0 & 0 \\\\ 0 & 1 & -2 & 0 \\\\ 0 & 0 & 0 & 0\\end{array}\\right)\n\\end{equation*}\\] and it follows that \\[N(A)=\\left\\{\\left(\\begin{array}{c} 0 \\\\ 2t \\\\ t\\end{array}\\right) \\where t\\in\\mathbb{R}\\right\\}.\\] So, if \\[\\left(\\begin{array}{c} 0 \\\\ 2t \\\\ t\\end{array}\\right)\\in N(A)\\] then \\[\\left(\\begin{array}{c} 0 \\\\ 2t \\\\ t\\end{array}\\right)=t\\left(\\begin{array}{c} 0 \\\\ 2 \\\\ 1\\end{array}\\right)\\] and so a basis of \\(N(A)\\) is \\[\\left(\\left(\\begin{array}{c} 0 \\\\ 2 \\\\ 1\\end{array}\\right)\\right)\\] (The spanning property is clearly seen and recall that a sequence with just one vector is always linearly independent.) Hence dim\\((N(A))=1\\)."
  },
  {
    "objectID": "02-Week2.html#sec-Definitions",
    "href": "02-Week2.html#sec-Definitions",
    "title": "2  Linear Mappings",
    "section": "2.1 Definitions",
    "text": "2.1 Definitions\n\nDefinition 2.1 (Linear mapping) Let \\(V\\) and \\(W\\) be vector spaces. A mapping \\(T : V\\maps W\\) is called a linear mapping if\n\n\\(T(\\vecx+\\vecy)=T(\\vecx)+T(\\vecy)\\) for all \\(\\vecx,\\vecy\\in V\\), and\n\\(T(\\lambda\\vecx)=\\lambda{T}(\\vecx)\\) for all \\(\\vecx\\in V\\), \\(\\lambda\\in \\F\\).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBoth conditions above are essential for a map to be linear: satisfying one does not imply the other! That is a. does not imply b. and b. does not imply a..\n\nFor example, consider \\(T: \\C \\to \\C\\) (where \\(\\C\\) is viewed a space over itself) defined by \\(T(a + ib) = a\\).\nThen \\[T((a + ib) + (c+ id)) = T((a+c) + i(b+d)) = a+c = T(a+ib) + T(c+id).\\] So \\(T\\) satisfies a.. However, \\(T\\) does not satisfy b. for instance\n\\[T(i(1+i)) = T(i-1) = -1 \\ne i(1+i) = i.\\] Therefore \\(T\\) is not a linear mapping on \\(\\C\\) as a vector space over itself.\nNotice that if we view \\(\\C\\) as a vector space over \\(\\R\\), then \\(T\\) is a linear map!\n\n\n\n\nExample 2.1  \n\n\nLet \\(V\\) and \\(W\\) be any vector spaces and \\(T: V \\to W\\) be defined by \\(\\vecx \\mapsto \\vec{0}_{w}\\). Then \\(T\\) is a linear map.\nFix a matrix \\(A \\in \\F_{m \\times n}\\) and define \\(T: \\F_{n\\times 1} \\to \\F_{m \\times 1}\\) by \\(\\vecx \\mapsto A\\vecx\\). Then \\(T\\) is a linear map.\nLet \\(V\\) be the real vector space of all real-valued differentiable functions and define \\(T: V \\to \\F(\\R)\\) by \\(f \\maps to f'\\) where \\(\\F(\\R)\\) is the vector space of all real valued functions in \\(\\R\\). Then \\(T\\) is a linear map. Note that elements of \\(V\\) are functions for instance \\(\\cos, \\sin, \\exp\\); indeed \\(T(\\cos) = -\\sin\\), \\(T(\\sin) = \\cos\\) and \\(T(\\exp) = \\exp\\).\nDefine \\(T: \\R_{n \\times n} \\to \\R\\) by \\(T(A) = \\Tr(A)\\). Then \\(T\\) is a linear map.\n\n\n\n\nLemma 2.1 Let \\(T : V\\maps W\\) be a linear mapping. Let \\(\\vecx,\\vecx_1,\\vecx_2, \\ldots,\\vecx_k\\in V\\) and \\(\\lambda_1, \\ldots,\\lambda_k\\in F\\). Then\n\n\\(T(\\veczero_V)=\\veczero_W\\),\n\\(T(-\\vecx) = -T(\\vecx)\\;\\; \\forall\\, \\vecx\\in V\\),\n\\(T(\\lambda_1\\vecx_1 + \\ldots + \\lambda_k\\vecx_k) = \\lambda_1T(\\vecx_1)+ \\ldots +\\lambda_k T(\\vecx_k)\\).\n\n\n\n\nProof. \n\nWe have \\(T(\\vec{0}_{V}) = T(\\vec{0}_{V} + \\vec{0}_{V}) = T(\\vec{0}_{V}) + T(\\vec{0}_{V})\\). Subtracting \\(T(\\vec{0}_{v})\\) from both sides, we have \\(T(\\vec{0}_{V}) = \\vec{0}_{V}\\).\nIt was shown in Exploring Algebra and Analysis that \\(-\\vecx = (-1)\\vecx\\) for any \\(\\vecx \\in V\\). Using this we have, \\(T(- \\vecx) = T((-1)\\vecx) = -1 T(\\vecx) = -T(\\vecx)\\).\nThis is most easily shown by induction. For the base case, \\(T(\\lambda_1 \\vecx_1) = \\lambda_{1}T(\\vec{x}_1)\\) follows from Definition 2.1. Assume that \\(T(\\lambda_1\\vecx_1 + \\ldots + \\lambda_k\\vecx_k) = \\lambda_1T(\\vecx_1)+ \\ldots +\\lambda_k T(\\vecx_k)\\) for \\(\\vecx,\\vecx_1,\\vecx_2, \\ldots,\\vecx_k\\in V\\) and \\(\\lambda_1, \\ldots,\\lambda_k\\in F\\). For the inductive step let \\(\\vecx,\\vecx_1,\\vecx_2, \\ldots,\\vecx_{k+1}\\in V\\) and \\(\\lambda_1, \\ldots,\\lambda_{k+1}\\in F\\). Then \\[\\begin{eqnarray*}\n   T(\\lambda_1\\vecx_1 + \\ldots + \\lambda_{k}\\vecx_{k} + \\lambda_{k+1}\\vecx_{k+1}) &=& T((\\lambda_1\\vecx_1 + \\ldots + \\lambda_{k}\\vecx_{k}) +\\lambda_{k+1}\\vecx_{k+1}) \\\\\n   &=& T(\\lambda_1\\vecx_1 + \\ldots + \\lambda_{k}\\vecx_{k}) + T(\\lambda_{k+1}\\vecx_{k+1}) \\\\\n   &=& T(\\lambda_1\\vecx_1 + \\ldots + \\lambda_{k}\\vecx_{k}) + \\lambda_{k+1}T(\\vecx_{k+1}) \\\\\n   &=& \\lambda_1T(\\vecx_1)+ \\ldots +\\lambda_k T(\\vecx_k) + \\lambda_{k+1}T(\\vecx_{k+1}).\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "02-Week2.html#sec-Image-and-Kernel",
    "href": "02-Week2.html#sec-Image-and-Kernel",
    "title": "2  Linear Mappings",
    "section": "2.2 Image and Kernel",
    "text": "2.2 Image and Kernel\n\nDefinition 2.2 (Image) Let \\(T : V\\maps W\\) be a linear mapping. The image of \\(T\\) is the set \\[\\im(T)=\\{T(\\vecx) \\where \\vecx\\in V\\} \\subseteq W.\\]\n\n\nDefinition 2.3 (Kernel) Let \\(T : V\\maps W\\) be a linear mapping. The kernel of \\(T\\) is the set \\[\\krn(T)=\\{\\vecx\\in V \\where T(\\vecx)=\\veczero_W\\} \\subseteq V.\\]\n\n\nLemma 2.2 Let \\(T : V\\maps W\\) be a linear mapping. Then\n\n\\(\\im(T)\\) is a subspace of \\(W\\).\n\\(\\krn(T)\\) is a subspace of \\(V\\).\n\n\n\n\nProof. \nWe use Lemma 1.6 in both cases:\n\nSince \\(T(\\vec{0}_{V}) = \\vec{0}_{W}\\), then \\(\\im(T)\\) is non-empty. Let \\(\\vecx, \\vecy \\in V\\). Then \\(T(\\vecx) + T(\\vecy) = T(\\vecx + \\vecy) \\in \\im(T)\\). Let \\(\\lambda \\in \\F\\). Then \\(\\lambda T(\\vecx) = T(\\lambda \\vecx) \\in \\im(T)\\). Therefore, \\(\\im(T)\\) is a subspace of \\(V\\)\nAs above, \\(T(\\vec{0}_{V}) = \\vec{0}_{W}\\), and so \\(\\ker(T)\\) is non-empty. Let \\(\\vecx, \\vecy \\in \\ker(T)\\) and \\(\\lambda \\in \\F\\). Then \\[T(\\vecx + \\vecy) = T(\\vecx) + T(\\vecy) = \\vec{0}_{W} + \\vec{0}_{w} = \\vec{0}_{W},\\] and, \\[T(\\lambda\\vecx) = \\lambda T(\\vecx) = \\lambda \\vec{0}_{W} =\\vec{0}_{W}.\\] Therefore \\(\\ker(T)\\) is a subspace of \\(V\\).\n\n\n\n\nExample 2.2 Let \\(T : \\R^3\\maps\\R^3\\) be the linear mapping given by \\[T((x_{1},x_{2},x_{3}))=(x_{1}+x_{2}+2x_{3},\\:2x_{1}+x_{2}+x_{3},\\:3x_{1}-x_{2}-6x_{3}).\\] Find bases of \\(\\im(T)\\) and \\(\\krn(T)\\).\n\nWe have \\[\\begin{eqnarray*}\n\\im(T) &=& \\left\\{(x_{1}+x_{2}+2x_{3},\\:2x_{1}+x_{2}+x_{3},\\:3x_{1}-x_{2}-6x_{3}): x_1, x_2, x_3 \\in \\R \\right\\} \\\\\n&=&\\left\\{ x_1(1,2,3) + x_2(1,1,-1) + x_3(2,1,-6): x_1,x_2, x_3 \\in \\R \\right\\} \\\\\n&=& \\spn((1,2,3),(1,1,-1),(2,1,-6)).\n\\end{eqnarray*}\\] A basis for \\(\\spn((1,2,3),(1,1,-1),(2,1,-6))\\) are the non-zero rows of a row-echelon form of the matrix below \\[\\begin{pmatrix} 1 & 2 & 3 \\\\ 1 & 1 & -1 \\\\ 2 & 1 & -6 \\end{pmatrix}\\] Applying row operations, we obtain the row echelon form:\n\\[\\begin{pmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 4 \\\\ 0 & 0 & 0\\end{pmatrix}.\\] Therefore a basis for \\(\\im(T)\\) is given by the sequence \\(((1,2,3),(0,1,4))\\). Thus, \\(\\dimn(im(T)) = 2\\).\nFor the kernel we solve \\(T(x_1, x_2,x_3) = (0,0,0)\\) for \\(x_1,x_2, x_3\\). That is, we want to find \\((x_1,x_2, x_3) \\in \\R^{3}\\) which satisfy the system of linear equations \\[\\begin{eqnarray*}\nx_1 + x_2 + 2x_3 &=& 0 \\\\\n2x_1 + x_2 + x_3 &=& 0 \\\\\n3x_1 -x_2 - 6x_3 &=& 0.\n\\end{eqnarray*}\\] We form the augmented matrix \\[\\left(\\begin{array}{ccc|c} 1 & 1 & 2  & 0 \\\\ 2 & 1 & 1  & 0 \\\\ 3 & -1 & -6  & 0 \\end{array}\\right).\\] Applying row operations: \\[\\left(\\begin{array}{ccc|c} 1 & 1 & 2 &  0 \\\\ 0 & 1 & 3 &  0 \\\\ 0 & 0 & 0 &  0 \\end{array}\\right).\\]\nReading off the solutions we see that \\(x_2 = -3x_3\\) and \\(x_1 = x_3\\). Therefore \\[\\ker(T) = \\{(x_3, -3x_3, x_3): x_3 \\in \\R\\} = \\spn((1,-3,1)).\\] Thus, \\(\\dimn(\\ker(T)) = 1\\).\nNotice that \\(\\dimn(\\im(T)) + \\dimn(\\ker(T)) = 3 = \\dim(\\R^3)\\).\n\n\n\nLemma 2.3 Let \\(T : V\\maps W\\) be a linear mapping. Then \\(T\\) is injective if and only if \\(\\krn(T)=\\{\\veczero_{V}\\}\\).\n\n\n\nProof. \nSuppose that \\(T\\) is injective. Then for any \\(\\vec{v} \\in V\\), \\(T(\\vec{v}) = \\vec{0}_{W}\\) if and only if \\(\\vec{v} = \\vec{0}_{V}\\) since \\(T(\\veczero_{V}) = \\veczero_{W}\\). Hence \\(\\krn(T) = \\{\\veczero_{V}\\}\\).\nSuppose \\(\\krn(T) = \\{\\veczero_{V}\\}\\). Let \\(\\vecx, \\vecy \\in V\\) be such that \\(T(\\vecx) = T(\\vecy)\\). Then \\(T(\\vecx) - T(\\vecy) = \\veczero_{W}\\). Using the fact that \\(T\\) is a linear map this means that \\[T(\\vecx - \\vecy) = \\veczero_{W}.\\] Therefore \\(\\vecx -\\vecy \\in \\krn(T) = \\{\\veczero_{V}\\}\\). We conclude that \\(\\vecx-\\vecy = \\veczero_V\\) and so \\(\\vecx = \\vecy\\).\n\n\n\nLemma 2.4 Let \\(T : V\\maps W\\) be a linear mapping and suppose that \\(V\\) is finite dimensional. Then \\(\\im(T)\\) is also finite dimensional.\n\n\n\nProof. \nSince \\(V\\) is finite dimensions, it has a finite basis \\((\\vecx_1, \\vecx_2,\\ldots,\\vecx_n)\\).\nIt suffices to show that \\(\\im(T)\\) is spanned by \\((T(\\vecx_1), T(\\vecx_2),\\ldots, T(\\vecx_n))\\) since this will imply that the size of any linearly independent subset of \\(\\im(T)\\) is at most \\(n\\).\nLet \\(\\vec{w} \\in \\im(T)\\), there is some \\(\\vec{v} \\in V\\) such that \\(T(\\vec{v}) = \\vec{w}\\). Since \\(\\vec{v} \\in V\\), we can find \\(a_1,a_2, \\ldots, a_n \\in \\F\\) such that \\[\\vec{v} = a_1 \\vecx_1+ a_2 \\vecx_2 + \\ldots + a_n \\vecx_n.\\]\nWe therefore have: \\[\\begin{eqnarray*}\n\\vec{w} = T(\\vec{v}) &=& T(a_1 \\vecx_1+ a_2 \\vecx_2 + \\ldots + a_n \\vecx_n) \\\\\n&=& a_1 T(\\vecx_1)+ a_2 T(\\vecx_2) + \\ldots + a_n T(\\vecx_n)\n\\end{eqnarray*}\\] where the second equality follows from the linearity of \\(T\\). Therefore \\(\\vec{w}\\) is n element of \\(\\spn((T(\\vecx_1), T(\\vecx_2),\\ldots, T(\\vecx_n)))\\).\n\n\n\nDefinition 2.4 (Rank, Nullity) Let \\(T : V\\maps W\\) be a linear mapping with \\(V\\) finite dimensional. The quantity \\(\\dimn(\\im(T))\\) is called the rank of \\(T\\) and will be denoted \\(\\rank(T)\\). The quantity \\(\\dimn(\\krn(T))\\) is called the nullity of \\(T\\) and will be denoted \\(\\nul(T)\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe kernel of \\(T\\) is finite dimensional since it is a subspace of \\(V\\) which is finite dimensional by assumption.\n\n\n\n\nTheorem 2.1 (Rank-Nullity Theorem) Let \\(T : V\\maps W\\) be a linear mapping with \\(V\\) finite dimensional. Then \\[\\rank(T) + \\nul(T) = \\dimn(V).\\]\n\n\n\nProof. \nSince \\(\\krn(T)\\) is a finite dimensional subspace of \\(V\\), there is a basis \\((\\vecx_1, \\vecx_2, \\ldots,\\vecx_{m})\\) for\\(\\krn(T)\\). We can extend this basis to a basis \\((\\vecx_1, \\vecx_2,\\ldots, \\vecx_{m}, \\vecx_{m+1}, \\ldots, \\vec{x}_{m+n})\\) for \\(V\\) by Theorem 1.3.\nWe claim that \\((T(\\vecx_{m+1}), \\ldots, T(\\vec{x}_{m+n}))\\) is a basis for \\(\\im(T)\\).\nWe note that \\(\\spn(T(\\vecx_{m+1}), \\ldots, T(\\vec{x}_{m+n}))\\) is equal \\(\\im(T)\\).\nClearly \\(\\spn(T(\\vecx_{m+1}), \\ldots, T(\\vec{x}_{m+n})) \\subseteq \\im(T)\\).\nNow let \\(\\vec{w} \\in \\im(T)\\), we can find \\(\\vec{v} \\in V\\) such that \\(T(\\vec{v}) = \\vec{w}\\). There are \\(a_1, a_2, \\ldots, a_{m+n} \\in \\F\\) such that \\[\\vec{v} = a_1 \\vecx_{1} + a_2 \\vecx_2 + \\ldots + a_m \\vecx_{m} + a_{m+1}\\vecx_{m+1} + \\ldots + a_{m+n}\\vecx_{m+n}.\\] We have: \\[\\begin{eqnarray*}\n  \\vec{w} = T(\\vec{v}) &=& T(a_1 \\vecx_{1} + a_2 \\vecx_2 + \\ldots + a_m \\vecx_{m} + a_{m+1}\\vecx_{m+1} + \\ldots + a_{m+n}\\vecx_{m+n}) \\\\\n&=&a_1 T \\vecx_{1}) + a_2 T(\\vecx_2) + \\ldots + a_m T(\\vecx_{m}) + a_{m+1}T(\\vecx_{m+1}) + \\ldots + a_{m+n}T(\\vecx_{m+n}) \\\\\n&=& a_1 \\vec{0}_{W} + a_2 \\vec{0}_{W} + \\ldots + a_m \\vec{0}_{W} + a_{m+1}T(\\vecx_{m+1}) + \\ldots + a_{m+n}T(\\vecx_{m+n}) \\\\\n&=& a_{m+1}T(\\vecx_{m+1}) + \\ldots + a_{m+n}T(\\vecx_{m+n}).\n\\end{eqnarray*}\\]\nHence \\(\\vec{w} \\in \\spn(T(\\vecx_{m+1}), \\ldots, T(\\vecx_{m+n}))\\). Since \\(\\vec{w} \\in \\im(T)\\) was arbitrarily chosen, we conclude that \\(\\im(T) \\subseteq \\spn(T(\\vecx_{m+1}), \\ldots, T(\\vecx_{m+n}))\\). Thus, \\(\\spn(T(\\vecx_{m+1}), \\ldots, T(\\vec{x}_{m+n})) = \\im(T)\\).\nNow suppose that there are elements \\(b_1, b_2, \\ldots, b_n \\in \\F\\) such that \\[b_{1}T(\\vecx_{m+1}) + \\ldots + b_{n}T(\\vecx_{m+n}) = \\vec{0}_{W}.\\] Then, by linearity, we have\n\\[T(b_{1}\\vecx_{m+1} + \\ldots + b_{n}\\vecx_{m+n}) = \\vec{0}_{W}.\\]\nTherefore, \\(b_{1}\\vecx_{m+1} + \\ldots + b_{n}\\vecx_{m+n} \\in \\krn(T)\\). This means that \\(b_{1}\\vecx_{m+1} + \\ldots + b_{n}\\vecx_{m+n}\\) can also be expressed as a linear combination of the sequence \\((\\vecx_1, \\vecx_2, \\ldots,\\vecx_{m})\\). However, since \\((\\vecx_1, \\vecx_2,\\ldots, \\vecx_{m}, \\vecx_{m+1}, \\ldots, \\vec{x}_{m+n})\\) this only happens precisely when \\(b_1 = b_2 \\ldots = b_n = \\vec{0}\\). This shows that the sequence \\((T(\\vecx_{m+1}), \\ldots, T(\\vec{x}_{m+n}))\\) is linearly independent.\nTherefore \\((T(\\vecx_{m+1}), \\ldots, T(\\vec{x}_{m+n}))\\) is a basis for \\(\\im(T)\\) and \\(\\dimn(\\im(T)) = n\\).\nPutting the above together, we have\n\\[\\dimn(V) = m+n = \\dim(\\ker(T)) + \\dimn(\\im(T))\\] as required.\n\n\n\nLemma 2.5 Let \\(T : V\\maps W\\) be a linear mapping. Then \\(T\\) is injective if and only if \\(\\nul(T)=0\\).\n\n\n\nProof. \nThis is a consequence of Lemma 2.3 since \\(\\nul(T)= \\dimn(\\ker(T))\\) and the dimension of subspace \\(\\{\\vec{0}_{V}\\}\\) is zero.\n\n\n\nLemma 2.6 Let \\(T : V\\maps W\\) be a linear mapping with \\(V\\) and \\(W\\) finite dimensional. Then \\(T\\) is surjective if and only if \\(\\rank(T)=\\dimn(W)\\).\n\n\n\nProof. \nIf \\(\\rank(T) = \\dimn(W)\\), then \\(\\dimn(\\im(T)) = \\dimn(W)\\). Therefore \\(\\im(T) = W\\) since \\(\\im(T)\\) is a subspace of \\(W\\) of the same dimension as \\(W\\).\n\n\n\nExample 2.3 Find the kernel of the linear mapping \\(T : \\R^2\\maps\\R^2\\) defined by \\[ T((a,b))=(2a-b,a+2b)\\] for all \\((a,b)\\in\\R^2\\). Deduce from the Rank-Nullity Theorem that \\(T\\) is a bijection.\n\nIf \\(T\\) is bijective then it is injective and consequently must satisfy \\(\\krn(T)= \\{(0,0)\\}\\). A good place to start is to verify this.\nSuppose \\((a,b) \\in \\krn(T)\\). Then \\(T((a,b)) = (2a -b, a+2b) = (0,0)\\). We deduce that \\(a = b/2\\), using this in the second coordinate, \\(5/2b =0\\) and so \\(b=a=0\\). It follows that \\(\\krn(T) = \\{(0,0)\\}\\) and \\(T\\) is injective.\nNow we use the Rank-Nullity Theorem (Theorem 2.1): \\[2 = \\dim(\\R^2) = \\rank(T)+ \\nul(T) = \\rank(T) + 0.\\] We conclude that \\(\\dim(\\im(T)) = 2\\). This means that \\(\\im(T) = \\R^2\\) and so \\(T\\) is surjective.\nWe conclude that \\(T\\) is a bijection.\n\n\n\nExample 2.4 Find the kernel of the linear mapping \\(T : P_2\\maps \\R^2\\) defined by \\[ T(p(x))=(p(0),p(1)) \\] for all \\(p\\in P_2\\). Hence state the nullity and rank of \\(T\\).\n\nLet \\(g:=ax^2 +bx +c \\in \\krn(T)\\). Then \\(T(g) = (c,b+c) = (0,0)\\). It follows that \\(c=b=0\\). Therefore \\(\\krn(T) = \\{ax^2: a \\in \\R\\}\\).\nThus \\(\\nul(T) = \\dimn(\\krn(T)) = 1\\) since \\(\\krn(T)\\) is spanned by the \\(x^2\\).\nUsing the fact that \\(\\dimn(P_2) = 3\\), by the Rank-Nullity Theorem, we have: \\[\\rank(T) = \\dimn(P_2) - \\dimn(\\krn(T)) = 3 -1 = 2.\\]\nNotice that \\(\\im(T) =\\R^2\\) and \\(T\\) is surjective."
  },
  {
    "objectID": "02-Week2.html#sec-Matrices-from-Linear-Mappings",
    "href": "02-Week2.html#sec-Matrices-from-Linear-Mappings",
    "title": "2  Linear Mappings",
    "section": "2.3 Matrices from Linear Mappings",
    "text": "2.3 Matrices from Linear Mappings\nThroughout this section, \\(V\\) and \\(W\\) will denote non-zero, finite dimensional vector spaces over the field \\(\\F\\).\nIn this section we are concerned with representing a given linear mapping \\(T : V\\longrightarrow W\\) by a matrix \\(M_{T}\\) which in some sense corresponds to \\(T\\).\n\nDefinition 2.5 Let \\(T : V\\maps W\\) be a linear mapping and let \\(n=\\dimn(V)\\) and \\(m=\\dimn(W)\\). Let \\(L_{V}=(\\vece_1, \\ldots, \\vece_n)\\) be a basis of \\(V\\) and let \\(L_{W}=(\\vecf_{1}, \\ldots, \\vecf_m)\\) be a basis of \\(W\\). We define the matrix of \\(T\\) with respect to \\(L_{V}\\) and \\(L_{W}\\) to be the \\(m\\times n\\) matrix whose \\(k^{th}\\) column is the coordinate vector of \\(T(\\vece_{k})\\) with respect to \\(L_{W}\\). We will denote this matrix by \\(M(T;L_{V},L_{W})\\) or \\(M_{T}\\) when there is no doubt about which bases of \\(V\\) and \\(W\\) we are using.\n\n\nLemma 2.7 Let \\(T : V\\maps W\\), \\(L_{V}=(\\vece_1, \\ldots, \\vece_n)\\) and \\(L_{W}=(\\vecf_1, \\ldots, \\vecf_m)\\) be as in the above definition. Let \\(M_{T}=(a_{ik})\\) in our usual matrix notation (so that the \\((i,k)^{th}\\) entry of the matrix \\(M_{T}\\) is the scalar \\(a_{ik}\\)). Then,\n\n\\(M_{T}\\) is a \\(m\\times n\\) matrix i.e. a (\\(\\dimn(W))\\times(\\dimn(V)\\)) matrix.\nFor each \\(\\vece_k\\) in \\(L_{V}\\), \\(T(\\vece_k)=\\sum_{i=1}^{m} a_{ik}\\vecf_i\\).\nFor every \\(\\vecx \\in V\\),\nIf an \\(m\\times n\\) matrix \\(N\\) is such that\nthen \\(N=M_{T}\\).\n\n\n\n\nProof. \n\nThis is a consequence of the definition. Since \\(L_{V}\\) has \\(n\\) elements, then \\(M_{T}\\) has \\(n\\)-columns, since \\(L_{W}\\) has \\(m\\) elements \\(M_{T}\\) has \\(m\\)-rows.\nBy definition the \\(k\\)th column of \\(M_{T}\\) is the coordinate vector of \\(T(e_k)\\) with respect to \\(L_{W}\\), by definition then, \\[T(e_k) =  \\sum_{i=1}^{m}a_{ik}\\vec{f}_{i}.\\]\nLet \\(\\vecx \\in V\\), and suppose \\((x_1, x_2,\\ldots, x_n)\\) is the coordinate vector of \\(\\vecx\\) with respect to \\(L_{V}\\). Thus,\\[\\vecx = \\sum_{i=1}^{n}x_i \\vece_{i}.\\] Applying the map \\(T\\), we have: \\[T(\\vecx) = T\\left(\\sum_{i=1}^{n}x_i \\vece_{i}\\right) = \\sum_{i=1}^{n}x_iT(\\vece_{i}).\\] Using the fact that \\(T(\\vece_{i}) = \\sum_{k=1}^{m}a_{ki} \\vecf_{k}\\), we have \\[\\begin{eqnarray*}\nT(\\vecx) = \\sum_{i=1}^{n}x_iT(\\vece_{i}) &=& \\sum_{i=1}^{n}x_i\\sum_{k=1}^{m}a_{ki}\\vecf_{k} \\\\\n&=& \\sum_{i=1}^{n}\\sum_{k=1}^{m}a_{ki}x_i\\vecf_{k} \\\\\n&=& \\sum_{k=1}^{m}\\left(\\sum_{i=1}^{n}a_{ki}x_i\\right)\\vecf_{k}.\n\\end{eqnarray*}\\]\n\nFor \\(1 \\le k \\le m\\) set \\(y_{k} = \\sum_{i=1}^{n}a_{ki}x_i\\) and observe that \\((y_1, y_2, \\ldots, y_m)\\) is the coordinate vector with respect to \\(L_{W}\\) of \\(T(\\vecx)\\).\nNow notice that \\[M_{T}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} = \\begin{pmatrix}\\sum_{i=1}^{n} a_{1i}x_{i}\\\\ \\sum_{i=1}^{n} a_{2i}x_{i} \\\\ \\vdots \\\\ \\sum_{i=1}^{n} a_{mi}x_{i} \\end{pmatrix} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{pmatrix}\\] d. Let \\(\\vec{E}_i \\in \\F_{n \\times 1}\\) be the coordinate vector of \\(\\vece_i\\) with respect to \\(L_{V}\\) written as a column vector. Notice that \\(\\vec{E}_i\\) has entries \\(0\\) everywhere and \\(1\\) in row \\(i\\). It follows, writing \\(N_{ij}\\) for the \\(ij\\)th entry of the matrix \\(N\\), that \\[N \\vec{E}_{i}= \\begin{pmatrix}N_{1i} \\\\ N_{2i} \\\\ \\vdots \\\\ N_{mi} \\end{pmatrix} = \\begin{pmatrix} a_{1i} \\\\ a_{2i} \\\\ \\vdots \\\\ a_{mi} \\end{pmatrix}  = M_T \\vec{E}_i.\\] It follows that \\(N_{ij} =a_{ij}\\) for all \\(1 \\le i \\le m\\) and \\(1 \\le j \\le n\\).\n\n\n\nExample 2.5 Consider the linear mapping \\(T : \\R^{3}\\maps\\R^{2}\\) defined by \\[ T((x_{1},x_{2},x_{3}))=(x_{1}+2x_{2}+3x_{3},\\:4x_{1}+5x_{2}+6x_{3}). \\] Find the matrix \\(M_{T}\\) of the linear mapping \\(T\\) with respect to the standard bases of \\(\\R^{3}\\) and \\(\\R^{2}\\).\n\nThe standard basis for \\(\\R^3\\) is \\[L_{\\R^3} = ((1,0,0),(0,1,0),(0,0,1)) = (\\vece_1,\\vece_2,\\vece_2)\\] and for \\(\\R^2\\) the standard basis is \\[L_{\\R^2}((1,0),(0,1)) = (\\vecf_1,\\vecf_2).\\]\nFor each \\(i\\) between \\(1\\) and \\(3\\) we need to find the coordinate vector of \\(T(\\vece_i)\\) with respect to \\(L_{\\R^2}\\). We have\n\\[\\begin{eqnarray*}\nT(\\vece_1) &=& (1,4) = 1\\vecf_1 + 4 \\vecf_2 \\\\\nT(\\vece_2) &=& (2,5) = 2\\vecf_1 + 5\\vecf_2 \\\\\nT(\\vece_3) &=& (3,6) = 3 \\vecf_1 + 6 \\vecf_2\n\\end{eqnarray*}\\]\nSo the matrix of \\(T\\) with respect to \\(L_{\\R^3}\\) and \\(L_{\\R^2}\\) is:\n\\[M_{T} = \\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{pmatrix}.\\]\n\n\n\nExample 2.6 Consider the linear mapping \\(T : P_{2}\\maps P_{2}\\) defined by \\[ T(p(x))=p(2x-1). \\] for all \\(p\\in P_{2}\\).\n\nFind \\(M_{T}\\) with respect to the basis \\((1,x,x^{2})\\) of \\(P_{2}\\).\nUse part a. to compute \\(T(3+2x-x^{2})\\). Verify your answer directly.\n\n\n\nWe proceed as in Example 2.5, we have: \\[\\begin{eqnarray*}\n   T(1) &=& 1 = 1 + 0x + 0x^2 \\\\\n   T(x) &=& 2x -1 = (-1)1 + 2x + 0 x^2 \\\\\n   T(x^2) &=& 1 - 4x  + 4x^2.\n\\end{eqnarray*}\\] Therefore the matrix of \\(T\\) with respect to the standard basis of \\(P_2\\) is: \\[M_{T} = \\begin{pmatrix} 1 & -1 & 1  \\\\ 0 & 2 & -4 \\\\ 0 &  0 & 4 \\end{pmatrix}.\\]\nWe use \\(M_T\\) to carry out this computation. We first find the coordinate vector of \\(3+ 2x -x^2\\) with respect to the standard basis of \\(P_2\\). This is \\((3, 2, -1)\\). Next we carry out the computation \\[ M_{T} \\begin{pmatrix} 3 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 8 \\\\ -4 \\end{pmatrix}.\\] The element \\((0, 8,-4)\\) now gives the coordinate vector of \\(T(3+2x - x^2)\\). It follows that \\(T(3 + 2x -x^2) = 8x -4x^2\\).\nWe can verify this directly: \\[\\begin{eqnarray*}\nT(3 + 2x - x^2) &=& 3 + 2(2x -1) - (2x-1)^2 \\\\\n                 &=& (3 -2 -1) + (4 +4)x -4x^2 \\\\\n                 &=& 0 + 8x -4x^2.\n\\end{eqnarray*}\\]\n\n\n\n\nLemma 2.8 Let \\(\\dimn(V)=n\\) and \\(\\dimn(W)=m\\). Let \\(L_{V}\\) and \\(L_{W}\\) be given bases of \\(V\\) and \\(W\\) and let \\(A\\) be an arbitrary matrix in \\(\\F_{m\\times n}\\). Then there is precisely one linear mapping \\(T : V\\maps W\\) such that \\(M(T;L_{V},L_{W})=A\\).\n\n\n\nProof. \nLet \\(L_{V} = (\\vece_1, \\vece_2, \\ldots, \\vece_n)\\) and \\(L_{W} = (\\vecf_1, \\vecf_2, \\ldots, \\vecf_m)\\). We define a linear map \\(T: V \\to W\\) by specifying its action on the basis elements of \\(V\\). For \\(1 \\le i \\le n\\) set: \\[T(e_i) = \\sum_{k=1}^{m}a_{ki}\\vec{f}_{k}.\\] Clearly \\(A = M_{T}\\).\nIf \\(S: V \\to W\\) is a linear map such that \\(M_S = M_{T} = A\\), then for \\(1 \\le i \\le n\\) \\[ S(e_i) = \\sum_{k=1}^{m}a_{ki}\\vec{f}_{k} = T(e_i).\\]\nIt follows that \\(S = T\\)."
  },
  {
    "objectID": "02-Week2.html#sec-Change-of-Basis",
    "href": "02-Week2.html#sec-Change-of-Basis",
    "title": "2  Linear Mappings",
    "section": "2.4 Change of Basis",
    "text": "2.4 Change of Basis\nLet \\(V\\) be a finite dimensional vector space with \\(\\dimn(V)=n\\). Let \\(L_{1}=(\\vece_1,\\vece_2, \\ldots,\\vece_n)\\) be a basis of \\(V\\). Let \\(L_{2}=(\\vecf_1,\\vecf_2, \\ldots,\\vecf_n)\\) be a sequence of vectors in \\(V\\). We might ask the important question “When is \\(L_{2}\\) a basis of \\(V\\)?” We know that \\(L_1\\) is a basis of \\(V\\), so we can express each \\(\\vecf_j\\) as a linear combination of the \\(\\vece_i\\). For example, \\[\\vecf_1 = \\lambda_{1,1} \\vece_1 + \\lambda_{1, 2}\\vece_2 + \\ldots + \\lambda_{1, n}\\vece_n.\\] In this way we get a linear mapping \\(T: V \\maps V\\). The sequence \\(L_2\\) forms a basis of \\(V\\) if and only if we can express each \\(\\vece_i\\) as a linear combination of the \\(\\vecf_j\\) ( by the “two out of three” rule) and this happens if and only if there is a linear mapping \\(S: V \\maps V\\) going ‘the other way’. In particular, in this case \\(ST = id\\) (this makes sense as \\(S\\) and \\(T\\) are functions, so can be composed) and using the matrices for these maps we have \\(M_SM_T = I_n\\), that is, \\(M_T\\) is invertible. So, \\(L_2\\) forms a basis of \\(V\\) if and only if \\(M_T\\) is invertible.\n\nDefinition 2.6 Let \\(V\\) be a vector space with \\(\\dim(V)=n\\) and let \\(L_{1}=(\\vece_1, \\ldots,\\vece_n)\\) and \\(L_{2}=(\\vecf_1, \\ldots,\\vecf_n)\\) be two bases of \\(V\\). The change of basis matrix from \\(L_{1}\\) to \\(L_{2}\\) is the \\(n\\times n\\) matrix whose \\(k^{th}\\) column is the coordinate vector of \\(\\vecf_k\\) with respect to \\(L_{1}\\). We denote this matrix by \\(M(L_{1}\\maps L_{2})\\).\n\n\nLemma 2.9 Let \\(V\\) have bases \\(L_{1}\\) and \\(L_{2}\\) and let \\(P=M(L_{1}\\maps L_{2})\\). Let an arbitrary vector \\(\\vecx\\in V\\) have coordinate vectors \\(X_{1}\\) and \\(X_{2}\\) with respect to \\(L_{1}\\) and \\(L_{2}\\) respectively. Then \\(X_{1}=PX_{2}\\).\n\n\n\nProof. \nConsider the map \\(\\id: V \\to V\\) by \\(\\id(\\vecx) = \\vecx\\). This is a linear map.\nAlso observe that \\(P\\) is precisely the matrix \\(M(\\id; L_{W}, L_{V})\\).\nLet \\(\\vecx \\in V\\) and let \\(X_1\\) and \\(X_2\\) be the coordinate vectors of \\(\\vecx\\) with respect to \\(L_1\\) and \\(L_2\\).\nBy Lemma 2.7 part c. the coordinate vector \\(X_1\\) is precisely \\(M(\\id; L_{W},L_{V})X_2 = P X_2\\) since \\(\\id(\\vecx) = \\vecx\\).\n\n\n\nExample 2.7 Let \\(B_{1}=(1,x,x^{2},x^{3})\\) be the standard basis of the space \\(P_{3}\\) and let \\(B_{2}\\) be the basis \\((p_{1},p_{2},p_{3},p_{4})\\) where \\[ p_{1}=1,\\:\\:\\:p_{2}=x-1,\\:\\:\\:p_{3}=x^{2}-x+1,\\:\\:\\:p_{4}=x^{3}-x^{2}+x-1. \\] Find the change of basis matrix \\(M(B_{2}\\maps B_{1})\\). Use this matrix to express the polynomial \\(1+x+x^{2}+x^{3}\\) as a linear combination of the vectors in the basis \\(B_{2}\\).\n\nWe express the vectors \\(B_1\\) as a linear combination of those in \\(B_2\\):\n\\[\\begin{eqnarray*}\n1 &=& p_1 + 0p_2 + 0 p_3 + 0p_4 \\\\\nx &=& p_1 + p_2 + 0p_3 + 0p_4 \\\\\nx^2 &=& 0p_1 + p_2 + p_3 + 0p_4 \\\\\nx^3 &=& 0p_1 + 0p_2 + p_3 + p_4.\n\\end{eqnarray*}\\]\nThe matrix \\(M(B_2 \\to B_1)\\) is therefore equal to \\[\\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}.\\]\nTo express \\(1 + x + x^2 + x^3\\) as a linear combination of the vectors in basis \\(B_2\\) we find its coordinate vectors in terms of \\(B_1\\) and pre-multiply it by \\(M(B_2 \\to B_1)\\) \\[M(B_2 \\to B_1) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\\\ 1 \\end{pmatrix}.\\]\nTherefore, \\(2p_1 + 2p_2 + 2p_3 + p_4\\) is the expression for \\(1 + x + x^2 + x^3\\) in the basis \\(B_2\\).\nWe can verify this: \\[\\begin{eqnarray*}\n2p_1 + 2p_2 + 2p_3 + p_4 &=& 2 + 2(x-1) + 2(x^2 -x +1) + (x^3 -x^2 +x -1) \\\\\n&=&(2 -2 +2 -1) + (2 -2 +1 )x + (2 -1)x^2 + x^3 \\\\\n&=& 1 + x + x^2 + x^3.\n\\end{eqnarray*}\\]\n\n\n\nLemma 2.10 Let \\(T : V\\maps W\\) be a linear mapping. Let \\(L_{1}\\) and \\(L_{2}\\) be two bases of \\(V\\) and let \\(K_{1}\\) and \\(K_{2}\\) be two bases of \\(W\\). Set \\(A=M(T;L_{1},K_{1})\\) and \\(B=M(T;L_{2},K_{2})\\). Then \\[ B=R^{-1}AP \\] where \\(P=M(L_{1}\\maps L_{2})\\) and \\(R=M(K_{1}\\maps K_{2})\\).\n\n\n\nProof. \nLet \\(\\vecx \\in V\\). Let \\(X_1\\) and \\(X_2\\) be the coordinate vectors of \\(\\vecx\\) with respect to the basis \\(L_1\\) and \\(L_2\\). Similarly let \\(Y_1\\) and \\(Y_2\\) be the coordinate vectors of \\(\\vecy = T(\\vecx)\\) with respect to the coordinate vectors \\(K_1\\) and \\(K_2\\).\nBy Lemma 2.7, \\(Y_1^{t} = AX_1^{t}\\) and \\(Y_2^{t} = BX_2^{t}\\). By Lemma 2.9 \\(Y_1^{t} = R Y_2^{t}\\) and \\(X_1 = PX_2^{t}\\). Putting all these together we have:\n\\[Y_2^{t} = R Y_1^{t} = APX_2^{t}\\] Meaning that \\(BX_2^{t} = Y_2^{t} = (R^{-1}AP) X_2^t\\).\nBy Lemma 2.7 part d. \\(B= R^{-1}AP\\).\n\n\n\nCorollary 2.1 Let \\(T : V\\maps V\\) be a linear mapping and let \\(L_{1}\\) and \\(L_{2}\\) be two bases of \\(V\\). Set \\(A=M(T;L_{1})\\) and \\(B=M(T;L_{2})\\). Then \\(B=P^{-1}AP\\) where \\(P=M(L_{1}\\maps L_{2})\\).\n\n\n\nProof. \nThis is a consequence of Lemma 2.10 with \\(V=W\\).\n\n\n\nCorollary 2.2 Let \\(T : V\\maps V\\) be a linear mapping and let \\(A\\) be the matrix of \\(T\\) with respect to some particular basis of \\(V\\). Then the set of all matrices arising as the matrices of \\(T\\) with respect to the different possible bases of \\(V\\) is the set of matrices similar to \\(A\\).\n\n\n\n\n\n\n\nNote\n\n\n\nLet \\(A\\) and \\(B\\) be \\(n\\times n\\) matrices. Then \\(B\\) is similar to \\(A\\) if and only if there is a nonsingular matrix \\(P\\) such that \\(B = P^{-1}AP\\).\n\n\n\nExample 2.8 Let \\(T : \\R^2\\maps\\R^2\\) be the linear mapping defined by \\[ T((x,y))=(5x-4y,3x-2y). \\] Find the matrix of \\(T\\) with respect to the standard basis of \\(\\R^2\\) and use this to find the matrix of \\(T\\) with respect to the basis \\(((1,1),(4,3))\\) of \\(\\R^2\\).\n\nLet \\(L_1 = ((1,0),(0,1))\\) be the standard basis for \\(\\R^2\\) and write \\(L_2\\) for the basis \\(((1,1),(4,3))\\).\nWe calculate \\(M(T;L_1)\\) in the usual way:\n\\[\\begin{eqnarray*}\nT((1,0)) &=& (5,3) =5(1,0) + 3(0,1)\\\\\nT((0,1)) &=& (-4,-2) = -4(1,0) -2(0,1).\n\\end{eqnarray*}\\] Therefore \\[M(T;L_1) = \\begin{pmatrix}\n        5 & -4 \\\\\n        3 & -2\n\\end{pmatrix}.\\]\nThe matrix \\(M(T;L_2)\\) is precisely \\(P^{-1}M(T;L_1)P\\) where \\(P\\) is the matrix \\(M(L_1 \\to L_2)\\). We compute \\(P\\) as follows. First we express the elements of \\(L_2\\) inn terms of \\(L_1\\).\n\\[\\begin{eqnarray*}\n  (1,1) &=& 1(1,0) + 1(0,1) \\\\\n  (4,3) &=& 4(1,0) + 3(0,1).\n\\end{eqnarray*}\\]\nThe matrix \\(P\\) can now be computed as\n\\[\\begin{pmatrix}\n1 & 4 \\\\\n1 & 3\n\\end{pmatrix}.\\]\nWe have: \\[M(T;L_2)= P^{-1}M(T;L_1)P = \\begin{pmatrix}\n-3 & 4 \\\\\n1 & -1\n\\end{pmatrix} \\begin{pmatrix}\n        5 & -4 \\\\\n        3 & -2\n\\end{pmatrix}\\begin{pmatrix}\n1 & 4 \\\\\n1 & 3\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 2\n\\end{pmatrix}\\]"
  },
  {
    "objectID": "02-Week2.html#sec-Eigenvalues-and-Eigenvectors",
    "href": "02-Week2.html#sec-Eigenvalues-and-Eigenvectors",
    "title": "2  Linear Mappings",
    "section": "2.5 Eigenvalues and Eigenvectors",
    "text": "2.5 Eigenvalues and Eigenvectors\nIn this section we connect knowledge of matrices with our new knowledge of the link between linear mappings and matrices. We saw in the last section that the matrices which arise as the matrix of a linear transformation with respect to different bases is just the set of all matrices similar to a given matrix of the transformation. Now recall the final result from Algebra:\n\nLemma 2.11 Let \\(A\\) and \\(B\\) be \\(n\\times n\\) matrices which are similar. Then:\n\n\\(\\det(A)=\\det(B)\\).\n\\(\\chi_{A}(t)=\\chi_{B}(t)\\).\n\\(A\\) and \\(B\\) have the same eigenvalues.\n\\(A\\) is nonsingular if and only if \\(B\\) is nonsingular.\n\n\n\n\nProof. \nNotice that iii. is a consequence of ii. and iv. is a consequence of i.\nIt remains to demonstrate i. and ii. Let \\(P\\) be such that \\(B = P^{-1}AP\\), then\n\n\\[\\begin{eqnarray*}\\det(P^{-1}AP) &=& \\det(P^{-1})\\det(A)\\det(P)\\\\ &=& \\frac{1}{\\det(P)} \\det(A)\\det(P) \\\\ &=& \\det(A).\\end{eqnarray*}\\]\n\\[\\begin{eqnarray*}\\lambda_{B}(t) &=& \\det(B-xI)\\\\ &=& \\det(P^{-1}AP - tI)\\\\ &=& \\det(P^{-1}AP - P^{-1}tIP) \\\\ &=& \\det(P^{-1}(A - tI)P) \\\\ &=& \\det(P^{-1})\\det(A - tI) \\det(P) \\\\ &=& \\det(A - tI) = \\lambda_{A}(t).\\end{eqnarray*}\\]\n\n\n\n\nDefinition 2.7 Let \\(V\\) be a non-zero, finite dimensional vector space and let \\(T : V\\maps V\\) be a linear transformation. The characteristic polynomial \\(\\chi_{T}(t)\\) is the characteristic polynomial of each and every matrix representing \\(T\\).\n\n\nDefinition 2.8 (Eigenvalue and eigenvector) Let \\(T : V\\maps V\\) be a linear transformation. We say that a scalar \\(\\lambda\\in \\F\\) is an eigenvalue of \\(T\\) if there exists a non-zero vector \\(\\vecx\\in V\\) such that \\(T(\\vecx)=\\lambda\\vecx\\). When this is the case, we say that the vector \\(\\vecx\\) is an eigenvector of \\(T\\) corresponding to the eigenvalue \\(\\lambda\\).\n\n\nExample 2.9 Find the eigenvalues and corresponding eigenvectors of the linear transformation \\(T : \\C^2\\maps \\C^2\\) defined by \\[ T((x,y))=(2x+y,\\:2x+3y). \\]\n\nLet \\(B=((1,0),(0,1))\\) be the standard basis for \\(\\C^2\\). We compute \\(M_T\\) the matrix of \\(T\\) with respect to the basis \\(B\\): \\[T((1,0)) = (2,2), \\quad T((0,1)) = (1,3),\\] Therefore, \\[M_{T} = \\begin{pmatrix} 2 & 1 \\\\ 2 & 3 \\end{pmatrix}.\\] Now, \\[\\begin{eqnarray*}\n\\chi_{T}(t) = \\chi_{M_{T}}(t) = \\vert tI -M \\vert &=& \\\\  \\begin{vmatrix} t-2 & -1 \\\\ -2 & t-3 \\end{vmatrix} &=& (t-2)(t-3)-2 \\\\ &=& t^2 -5t +4 = (t-4)(t-1).\n\\end{eqnarray*}\\]\nTherefore the eigenvalues of \\(T\\) are \\(1\\) and \\(4\\).\nTo find the eigenvectors corresponding to an eigenvalue \\(\\lambda\\), we find all vectors \\(\\vecx\\) which are solutions to \\((tI - M_{T})\\vecx = \\veczero\\). We take each eigenvalue in turn.\n\n\\(\\lambda_{T}=1\\):\n\nWe solve \\((I - M_{T})\\vecx = \\veczero\\) for \\(X = \\begin{pmatrix} x \\\\ y \\end{pmatrix}\\) \\[(I-M)\\vecx = \\begin{pmatrix}-1 & -1 \\\\ -2 & -2\\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} ) \\\\ ) \\end{pmatrix}.\\] The Augmented matrix is: \\[\\left(\\begin{array}{cc|c} -1 & -1 & 0 \\\\ -2 & -2 & 0 \\end{array}\\right)\\] The second row is a multiple of the first thus we have one equation \\(x + y =0\\). Therefore, the set of solutions is: \\[\\left\\{ \\begin{pmatrix} -y \\\\ y \\end{pmatrix} : y \\in \\C \\right\\} = \\spn\\left(\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\right).\\] It follows that the eigenvectors of \\(T\\) corresponding to the eigenvalue \\(1\\) are all non-zero vectors in \\(\\spn((1,-1))\\).\n\n\\(\\lambda_t=4\\):\n\nWe proceed as before. We solve \\((4I - M_{T})\\vecx = \\veczero\\) for \\(\\vecx\\). This time the augmented matrix is: \\[\\left(\\begin{array}{cc|c} 2 & -1 & 0 \\\\ -2 & 1 & 0 \\end{array}\\right).\\] Again the second row is a multiple of the first, thus we have one equation: \\(2x - y =0\\). The eigenvectors of \\(M_{T}\\) corresponding to the eigenvalue \\(4\\) are all non-zero vectors in \\[\\spn\\left(\\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\\right).\\] It follows that the eigenvectors of \\(T\\) corresponding to eigenvalue \\(4\\) are the non-zero vectors in \\(\\spn((1,2))\\).\n\n\n\n\nNow, let \\(T:V \\maps V\\) be a linear transformation, where \\(V\\) is a finite dimensional vector space over some field \\(\\F\\). Fix an eigenvalue, \\(\\lambda\\) of \\(T\\) and let \\[E(\\lambda, T) = \\{ \\vecv \\in V \\where T(\\vecv) = \\lambda\\vecv \\}.\\] (Note that \\(\\veczero \\in E(\\lambda, T)\\) even though it is not an eigenvector by the definition.) If \\(\\vecu, \\vecv \\in E(\\lambda, T)\\) then \\[\\begin{eqnarray*}\nT(\\vecu + \\vecv) &=& T(\\vecu) + T(\\vecv) \\qquad \\text{$T$ is linear}\\\\\n&=& \\lambda\\vecu + \\lambda\\vecv \\qquad \\vecu, \\vecv \\in E(\\lambda, T)\\\\\n&=&\\lambda(\\vecu + \\vecv).\n\\end{eqnarray*}\\] So, \\(\\vecu + \\vecv \\in E(\\lambda, T)\\) and hence \\(E(\\lambda, T)\\) is closed under addition. Similarly, if \\(\\vecv \\in E(\\lambda, T)\\) and \\(\\mu \\in F\\), then \\[\\begin{eqnarray*}\nT(\\mu \\vecv) &=& \\mu T(\\vecv) \\qquad \\text{$T$ is linear}\\\\\n&=& \\mu(\\lambda\\vecv) \\qquad \\vecv \\in E(\\lambda, T)\\\\\n&=& \\lambda(\\mu \\vecv).\n\\end{eqnarray*}\\] So, \\(\\mu\\vecv \\in E(\\lambda, T)\\) and hence \\(E(\\lambda, T)\\) is closed under scalar multiplication. It then follows that \\(E(\\lambda, T)\\) is a subspace of \\(V\\) and so the set of all eigenvectors of \\(T\\) corresponding to \\(\\lambda\\), together with the zero vector, forms a vector space.\n\nDefinition 2.9 (Eigenspace) Let \\(T: V\\maps V\\) be a linear transformation and let \\(\\lambda\\) be an eigenvalue of \\(T\\). Then the set \\[ E(\\lambda,T)=\\{\\vecx\\in V \\where T(\\vecx)=\\lambda\\vecx\\} \\] is called the eigenspace of \\(T\\) corresponding to \\(\\lambda\\).\n\n\nLemma 2.12 Let \\(T: V\\maps V\\) be a linear transformation and let \\(\\lambda\\) and \\(\\mu\\) be different eigenvalues of \\(T\\). Then \\(E(\\lambda,T)\\cap E(\\mu,T)=\\{\\veczero\\}\\).\n\n\n\nProof. \nSuppose \\(\\vecx \\in E(\\lambda,T) \\cap E(\\mu, T)\\). Then \\[\\lambda \\vecx = T(\\vecx) = \\mu \\vecx.\\] It follows that \\((\\lambda - \\mu) \\vecx = \\veczero\\). Since \\(\\lambda - \\mu \\ne 0\\) (as \\(\\lambda \\ne \\mu\\)), then \\(\\vecx\\) must be \\(\\veczero\\) as required.\n\n\n\nCorollary 2.3 Let \\(T:V\\maps V\\) be a linear transformation and let \\(\\lambda_{1},\\lambda_{2}, \\ldots,\\lambda_{n}\\) be different eigenvalues of \\(T\\). Let \\(\\vecx_1, \\ldots,\\vecx_n\\) be eigenvectors of \\(T\\) corresponding to the eigenvalues \\(\\lambda_{1},...,\\lambda_{n}\\) respectively. Then the sequence \\((\\vecx_1, \\ldots,\\vecx_n)\\) is linearly independent.\n\n\n\nProof. \nWe prove this by induction.\nThe base case occurs when \\(n = 1\\). The result trivially holds for this case since a single eigenvector is linearly independent (eigenvectors are non-zero).\nAssume that for \\(\\lambda_{1},\\lambda_{2}, \\ldots,\\lambda_{n}\\) different eigenvalues of \\(T\\) and \\(\\vecx_1, \\ldots,\\vecx_n\\) eigenvectors of \\(T\\) corresponding to the eigenvalues \\(\\lambda_{1},\\ldots,\\lambda_{n}\\) respectively, the sequence \\((\\vecx_1, \\ldots,\\vecx_n)\\) is linearly independent.\nNow let \\(\\lambda_{1},\\lambda_{2}, \\ldots,\\lambda_{n+1}\\) be different eigenvalues of \\(T\\) and \\(\\vecx_1, \\ldots,\\vecx_{n+1}\\) eigenvectors of \\(T\\) corresponding to the eigenvalues \\(\\lambda_{1},...,\\lambda_{n+1}\\).\nSuppose there are scalars \\(a_1, a_2, \\ldots, a_{n+1} \\in \\F\\) such that \\[a_1 \\vecx_1 + a_2 \\vec x_2 + \\ldots + a_{n+1} \\vec x_{n+1} = \\vec{0}. \\tag{2.1}\\] Rearranging we have: \\[a_1 \\vecx_1 + a_2 \\vec x_2 + \\ldots + a_{n} \\vec x_{n} = -a_{n+1} \\vec x_{n+1}. \\tag{2.2}\\]\nApplying the map \\(T\\):\n\\[T(a_1 \\vecx_1 + a_2 \\vecx_2 + \\ldots + a_{n} \\vecx_{n}) = T(-a_{n+1} \\vecx_{n+1}).\\] Using linearity: \\[a_1 T(\\vecx_1) + a_2 T(\\vecx_2) + \\ldots + a_{n}T(\\vecx_{n}) = -a_{n+1} T(\\vecx_{n+1}).\\] Noting that \\(\\vecx_i\\) is an eigenvector with eigenvalue \\(\\lambda_i\\), we have \\[a_1 \\lambda_1 \\vecx_1 + a_2 \\lambda_2 \\vec x_2 + \\ldots + a_{n}\\lambda_nT(\\vec x_{n}) = -a_{n+1} \\lambda_{n+1} \\vec x_{n+1}. \\tag{2.3}\\]\nThere are two possibilities either \\(\\lambda_{n+1} = 0\\) or \\(\\lambda_{n+1} \\ne 0\\).\n\n\\(\\lambda_{n+1} = 0\\):\n\nIn this case \\(\\lambda_i \\ne 0\\) for any \\(1 \\le i \\le n\\) (since the eigenvalues are distinct). In particular: \\[a_1 \\lambda_1 \\vecx_1 + a_2 \\lambda_2 \\vec x_2 + \\ldots + a_{n}\\lambda_nT(\\vec x_{n}) = \\vec{0}.\\] However, since \\((\\vecx_1, \\vecx_2, \\ldots, \\vecx_n)\\) is linearly independent, \\(a_i\\lambda_i = 0\\) for all \\(1 \\le i \\le n\\). We conclude that \\(a_i = 0\\) for all \\(1 \\le i \\le n\\) since \\(\\lambda_i \\ne 0\\). Equation 2.1 now implies that \\(a_{n+1} = 0\\) as well. We conclude that the sequence \\((\\vecx_1, \\vecx_2,\\ldots, \\vecx_{n+1})\\) is linearly independent.\n\n\\(\\lambda_{n+1} \\ne 0\\):\n\nDividing Equation 2.3 by \\(\\lambda_{n+1}\\) and subtracting Equation 2.2 we have \\[a_1\\left(\\frac{\\lambda_1}{\\lambda_{n+1}} - 1\\right) \\vecx_1 + a_2\\left(\\frac{\\lambda_2}{\\lambda_{n+1}} - 1 \\right) \\vecx_2 + \\ldots + a_n\\left(\\frac{\\lambda_n}{\\lambda_{n+1}} - 1 \\right) \\vecx_n = \\veczero.\\] By The inductive assumption it must be the case that \\(a_i\\left(\\frac{\\lambda_i}{\\lambda_{n+1}} - 1 \\right) = 0\\) for all \\(1 \\le i \\le n\\). Now since \\(\\lambda_i \\ne \\lambda_{n+1}\\) for \\(1 \\le i \\le n\\), it follows that \\(\\frac{\\lambda_i}{\\lambda_{n+1}} \\ne 1\\) and so \\(\\frac{\\lambda_i}{\\lambda_{n+1}} - 1 \\ne 0\\). Therefore \\(a_i = 0\\) for all \\(1 \\le i \\le n\\). As in the previous case, we conclude that \\(a_{n+1} = 0\\) as well and the sequence \\((\\vecx_1, \\vecx_2,\\ldots, \\vecx_{n+1})\\) is linearly independent."
  },
  {
    "objectID": "02-Week2.html#sec-Diagonalisation",
    "href": "02-Week2.html#sec-Diagonalisation",
    "title": "2  Linear Mappings",
    "section": "2.6 Diagonalisation",
    "text": "2.6 Diagonalisation\n\nLemma 2.13 Let \\(T : V\\maps V\\) be a linear transformation of a finite dimensional vector space over \\(\\C\\) and let \\(L=(\\vece_1, \\ldots,\\vece_n)\\) be a basis of \\(V\\). Then the following statements are equivalent:\n\n\\(M(T;L)=\\diag(\\alpha_{1},...,\\alpha_{n})\\);\n\\(\\alpha_{1}, \\ldots,\\alpha_{n}\\) are eigenvalues of \\(T\\) and for each \\(k\\), \\(\\vece_k\\) is an eigenvector of \\(T\\) corresponding to the eigenvalue \\(\\alpha_{k}\\).\n\n\n\nProof. The proof below is unexaminable.\n\nSuppose \\(M(T;L) = \\diag(a_1,\\ldots, a_n)\\). Then as column \\(i\\) of \\(M(T;L)\\) is the coordinate vector of \\(T(\\vece_i)\\) written as a column, we have: \\[T(\\vece_i) = 0\\vece_1 + \\ldots + 0 \\vece_{i-1} + a_i \\vece_i + 0 \\vece_{i+1} + \\ldots + 0 \\vece_n = a_i \\vece_i.\\] Therefore \\(\\vece_i\\) is an eigenvector of \\(T\\) corresponding to the eigenvalue \\(a_i\\).\nOn the other hand, suppose that \\(\\alpha_{1}, \\ldots,\\alpha_{n}\\) are eigenvalues of \\(T\\) and for each \\(k\\), \\(\\vece_k\\) is an eigenvector of \\(T\\) corresponding to the eigenvalue \\(\\alpha_{k}\\). Then clearly the \\(i\\)th column of \\(M(T;L)\\) is the has entry \\(a_{i}\\) in position \\(i\\) and zero’s everywhere else since \\[T(\\vece_i) = a_i \\vece_i = 0\\vece_1 + \\ldots + 0 \\vece_{i-1} + a_i \\vece_i + 0 \\vece_{i+1} + \\ldots + 0 \\vece_n.\\]\n\n\n\nDefinition 2.10 (Diagonalisable) A linear transformation \\(T:V\\maps V\\) is diagonalisable if there exists a basis \\(L\\) of \\(V\\) such that \\(M(T;L)\\) is a diagonal matrix.\n\n\nTheorem 2.2 Suppose that the linear transformation \\(T: V\\maps V\\) has \\(n\\) different eigenvalues \\(\\alpha_{1}, \\ldots,\\alpha_{n}\\), where \\(n=\\dimn(V)\\). For each \\(k\\), let \\(\\vece_k\\) be an eigenvector of \\(T\\) corresponding to \\(\\alpha_{k}\\) and let \\(L=(\\vece_1, \\ldots,\\vece_n)\\). Then \\(L\\) is a basis of \\(V\\) and \\[M(T;L)=\\diag(\\alpha_{1}, \\ldots,\\alpha_{n}).\\]\n\n\n\nProof. \nBy Corollary 2.3, \\(L\\) is linearly independent, therefore \\(L\\) is a basis for \\(V\\) since \\(\\dimn(V) = n\\).\nBy Lemma 2.13, \\(M(T;L)=\\diag(\\alpha_{1}, \\ldots,\\alpha_{n}).\\)"
  },
  {
    "objectID": "02-Week2.html#sec-Algebraic-and-Geometric-Multiplicities-of-Eigenvalues",
    "href": "02-Week2.html#sec-Algebraic-and-Geometric-Multiplicities-of-Eigenvalues",
    "title": "2  Linear Mappings",
    "section": "2.7 Algebraic and Geometric Multiplicities of Eigenvalues",
    "text": "2.7 Algebraic and Geometric Multiplicities of Eigenvalues\n\nDefinition 2.11 (Algebraic and geometric multiplicity) Let \\(T:V\\longrightarrow V\\) be a linear mapping with \\(V\\) finite dimensional and let \\(\\lambda\\in \\F\\) be an eigenvalue of \\(T\\). Then\n\nThe power to which \\((t-\\lambda)\\) appears in the characteristic polynomial \\(\\chi_{T}(t)\\) is called the algebraic multiplicity of \\(\\lambda\\) as an eigenvalue of \\(T\\).\nThe quantity \\(\\dimn(E(\\lambda,T))\\) is called the geometric multiplicity of \\(\\lambda\\) as an eigenvalue of \\(T\\).\n\n\n\nLemma 2.14 Let \\(T:V\\longrightarrow V\\) be a linear mapping with \\(V\\) nonzero and finite dimensional. Let \\(\\lambda\\in F\\) be an eigenvalue of \\(T\\). Then \\[\\text{(geometric multiplicity of $\\lambda)\\leq$ (algebraic multiplicity of $\\lambda$).}\\]\n\n\n\nProof. \nLet \\(\\lambda\\) be an eigenvalue of \\(T\\), let \\(s\\) be the algebraic multiplicity of \\(T\\) and \\(r\\) be its geometric multiplicity.\nSince \\(\\dimn(E(\\lambda,T)) = r\\), there is a basis \\(l=(\\vece_1, \\ldots, \\vece_r)\\) for \\((\\lambda,T)\\). We can extend \\(l\\) to a basis \\(L =(\\vece_1, \\ldots, \\vece_r,\\ldots,\\vece_{s+1} \\vece_n)\\).\nWith respect to the basis \\(L\\), we have, \\[M(T;L) = \\begin{pmatrix}\n            \\lambda I_r & B  \\\\\n              0 &  C\n          \\end{pmatrix}\\] Where \\(I_r\\) is the \\(r \\times r\\) identity matrix, \\(B\\) is an \\(r \\times (n-r)\\) matrix and \\(C\\) is an \\((n-r) \\times (n-r)\\) matrix. Expanding the determinant of \\((tI = M(T;L))\\), we see that \\[\\det(T) = \\begin{vmatrix} tI_{r} - \\lambda I_r & -B \\\\ 0 & tI_{(n-r)} - C \\end{vmatrix} = (t-\\lambda)^{r} \\det(tI -C) =  (t-\\lambda)^{r}\\chi_{C}(t).\\] It follows that \\(r \\le s\\) since \\(\\det(C)\\) might have a power of \\((t-\\lambda)\\) as a factor.\n\n\n\nTheorem 2.3 Let \\(T:V\\longrightarrow V\\) be a linear mapping with \\(V\\) finite dimensional and let \\(\\lambda\\in F\\) be an eigenvalue of \\(T\\). Then \\(T\\) is diagonalisable if and only if, for each eigenvalue \\(\\lambda\\) of \\(T\\), the algebraic and geometric multiplicities of \\(\\lambda\\) coincide.\n\n\nProof. Omitted.\n\n\nExample 2.10 Consider the linear transformation of \\(\\C^{2}\\) defined by \\[\nT((x,y))=(x+y,\\:4x+y).\n\\] Show that \\(T\\) is diagonalisable.\n\nWith respect to the standard basis for \\(\\C^2\\), \\[M_{T} = \\begin{pmatrix} 1 & 1 \\\\ 4 & 1 \\end{pmatrix}.\\] Thus \\(\\chi_{T} = (t-1)^2 -4 = (t+3)(t-1)\\). The eigenvalues of \\(T\\) are therefore \\(-3\\) and \\(1\\).\nNotice that \\(\\dimn(E(\\lambda,T))\\) is at least \\(1\\) for any eigenvalue of \\(T\\) (since eigenvectors by definition are non-zero). However by Lemma 2.14, \\(\\dimn(E(\\lambda,T)) \\le 1\\) for any eigenvalue of \\(T\\) since the algebraic multiplicity of each of the eigenvalues of \\(T\\) is \\(1\\) (from the expression of \\(\\chi_{T}\\)).\nIt follows that the algebraic and geometric multiplicities of each eigenvalue of \\(T\\) are equal and so \\(T\\) is diagonalisable by Theorem 2.3.\n\n\n\nExample 2.11 Consider the linear transformation of \\(\\C^{2}\\) defined by \\[\nS((x,y))=(x+y,\\:y).\n\\] Show that \\(S\\) is not diagonalisable.\n\nWith respect to the standard basis for \\(\\C^2\\) \\[ M_{S} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}.\\] Thus \\(\\chi_{S} = (t-1)^2\\) and \\(s\\) has a single eigenvalue namely \\(1\\).\nWe find \\(\\dimn(E(1,S))\\). First we find a spanning set for \\(E(1,S)\\), we have \\[\\begin{eqnarray*}\n  E(1,S) &=& \\{ (x,y) \\in \\C^2: T((x,y)) = (x,y) \\} \\\\\n  &=& \\{ (x,y) \\in \\C^2: (x+y,y) = (x,y) \\} \\\\\n  &=& \\{ (x,y) \\in \\C^2:  y=0 \\} \\\\\n  &=& \\spn((1,0)).\n\\end{eqnarray*}\\]\nIt follows that \\(\\dimn(E(1,S)) = 1\\) and the algebraic multiplicity of the eigenvalue \\(1\\) is strictly greater than its geometric multiplicity — \\(S\\) is not diagonalisable by Theorem 2.3."
  },
  {
    "objectID": "02-Week2.html#sec-Vector-Space-Isomorphisms",
    "href": "02-Week2.html#sec-Vector-Space-Isomorphisms",
    "title": "2  Linear Mappings",
    "section": "2.8 Vector Space Isomorphisms",
    "text": "2.8 Vector Space Isomorphisms\n\nDefinition 2.12 (Isomorphism) Let \\(V\\) and \\(W\\) be vector spaces over \\(\\F\\) and let \\(T : V\\longrightarrow W\\) be a linear mapping. In the special case where \\(T\\) is a bijection we say that \\(T\\) is a vector space isomorphism and that the spaces \\(V\\) and \\(W\\) are isomorphic.\n\n\nLemma 2.15 Let \\(V\\) be a finite dimensional vector space over \\(\\F\\) with \\(\\dimn V=n\\). Then \\(V\\) is isomorphic to \\(\\F^{n}\\).\n\n\n\nProof. \nLet \\(B:=(\\vece_1,\\vece_2, \\ldots, \\vece_n)\\) be a basis for \\(V\\). Let \\(T: V \\to \\F^{n}\\) be defined by \\[T(a_1\\vece_1 + a_2 \\vece_2 + \\ldots + a_n \\vece_n) = (a_1,a_2,\\ldots, a_n).\\]\nThe map \\(T\\) is well-defined since, as \\(B\\) is a basis, every element of \\(V\\) can be expressed uniquely as a linear combination of elements of \\(B\\). Clearly \\(T\\) is surjective and injective.\nThat \\(T\\) is linear is easily verified, in particular \\(T\\) is the unique linear map which maps \\(\\vece_i\\) to the element \\((0, \\ldots,0,1,0\\ldots, 0)\\) where \\(1\\) occurs in position \\(i\\).\nTherefore \\(T\\) is an isomorphism of vector spaces and \\(V \\cong \\F^{n}\\).\n\n\n\nCorollary 2.4 Let \\(V\\) and \\(W\\) be finite dimensional vector spaces over \\(\\F\\). Then \\[\nV\\cong W\\Leftrightarrow\\:\\: \\dimn V= \\dimn W.\n\\]\n\n\n\nProof. \nIf \\(V \\cong W\\), then there exist is a vector space isomorphism \\(T: V \\to W\\). Applying the Rank-Nullity Theorem (Theorem 2.1) \\[ \\dimn(V) = \\dimn(im(T)) + \\dimn(\\ker(T)) = \\dimn(W) + 0 = \\dim(W).\\]\nIf \\(\\dimn(V) = \\dimn(W) = n\\). Then \\(V \\cong \\F^n \\cong W\\) and so \\(V \\cong W\\)."
  },
  {
    "objectID": "02-Week2.html#sec-sheet2",
    "href": "02-Week2.html#sec-sheet2",
    "title": "2  Linear Mappings",
    "section": "2.9 Problem Sheet 2",
    "text": "2.9 Problem Sheet 2\n_ Questions 2.1 – 2.16 for Week 4; Questions 2.17 – 2.21 for Week 6._\n\n\n\n\nQuestion 2.1\n\n\nLet \\(S : V\\longrightarrow W\\) and \\(T : U\\longrightarrow V\\) be mappings.\n\nProve that if \\(S\\circ T\\) is injective then \\(T\\) is also injective.\nProve that if \\(S\\circ T\\) is surjective then \\(S\\) is also surjective.\n\n\n\n\n\n\nShow Solution 2.1\n\n\n\n\nSolution 2.1\n\n\n\nLet \\(x,y\\in U\\). Then \\[\\begin{align*}\nT(x)=T(y) & \\Rightarrow S(T(x))=S(T(y)) \\\\\n& \\Rightarrow (S\\circ T)(x)=(S\\circ T)(y) \\\\\n& \\Rightarrow x=y\n\\end{align*}\\] since \\(S\\circ T\\) is injective. Hence \\(T\\) is injective.\nLet \\(y\\in W\\). Then, since \\(S\\circ T\\) is surjective, \\(y=(S\\circ T)(x)\\) for some \\(x\\in U\\) i.e. \\(y=S(T(x))\\in{\\rm{im}}(S)\\). Hence \\(S\\) is surjective.\n\n\n\n\n\n\n\n\nQuestion 2.2\n\n\nFor each of the following mappings \\(T: U\\longrightarrow V\\), decide whether \\(T\\) is linear.\n\n\\(U=\\mathbb{R}^{4}\\), \\(V=\\mathbb{R}^{3}\\), \\(T((x_{1},x_{2},x_{3},x_{4}))=(x_{2}+x_{3},\\:x_{1}-x_{2}^{2},\\: x_{3}+x_{4})\\),\n\\(U=\\mathbb{R}^{3}\\), \\(V=\\mathbb{R}^{2}\\), \\(T((x_{1},x_{2},x_{3}))=(x_{2}-x_{1},\\:x_{3}+3x_{2})\\),\n\\(U=P_{2}\\), \\(V=P_{5}\\), \\(T(p(x))=xp(x^{2})+p(1)\\),\n\\(U=\\mathbb{R}^{2}\\), \\(V=\\mathbb{R}\\), \\(T((x,y))=xy\\).\n\n\n\n\n\n\nShow Solution 2.2\n\n\n\n\nSolution 2.2\n\n\n\nThis is not linear. For example, take \\(\\textbf{x}=(0,1,0,0)\\in U\\) and \\(\\lambda=2\\). Then \\[\\begin{equation*}\nT(\\lambda\\textbf{x})=T((0,2,0,0))=(2,-4,0).\n\\end{equation*}\\] However, \\[\\begin{equation*}\n\\lambda{T}(\\textbf{x})=2T((0,1,0,0))=2(1,-1,0)=(2,-2,0).\n\\end{equation*}\\] Hence we have \\(T(\\lambda\\textbf{x})\\neq \\lambda{T}(\\textbf{x})\\) and so \\(T\\) is not linear.\nThis is linear. Let \\(\\textbf{x}=(x_{1},x_{2},x_{3}),\\textbf{y}=(y_{1},y_{2},y_{3})\\in U\\) and \\(\\lambda\\in\\mathbb{R}\\). Then \\[\\begin{align*}\nT(\\textbf{x}+\\textbf{y}) & = T((x_{1}+y_{1},\\:x_{2}+y_{2},\\:x_{3}+y_{3})) \\\\\n& = (x_{2}+y_{2}-(x_{1}+y_{1}),\\:x_{3}+y_{3}+3(x_{2}+y_{2})) \\\\\n& = (x_{2}-x_{1}+y_{2}-y_{1},\\:x_{3}+3x_{2}+y_{3}+3y_{2}) \\\\\n& = (x_{1}-x_{1},\\:x_{3}+3x_{2})+(y_{2}-y_{1},\\:y_{3}+3y_{2}) \\\\\n& = T(\\textbf{x})+T(\\textbf{y})\n\\end{align*}\\] and \\[\\begin{align*}\nT(\\lambda\\textbf{x}) & = T(\\lambda(x_{1},x_{2},x_{3})) \\\\\n& = T(\\lambda{x_{1}},\\lambda{x_{2}},\\lambda{x_{3}}) \\\\\n& = (\\lambda{x_{2}}-\\lambda{x_{1}},\\:\\lambda{x_{3}}+3(\\lambda{x_{2}})) \\\\\n& = \\lambda(x_{2}-x_{1},\\:x_{3}+3x_{2}) \\\\\n& = \\lambda{T}(\\textbf{x}).\n\\end{align*}\\]\nThis is linear. Let \\(p,q\\in P_{2}\\) and \\(\\lambda\\in\\mathbb{R}\\). Then \\[\\begin{align*}\nT((p+q)(x)) & = x(p+q)(x^{2})+(p+q)(1) \\\\\n& = xp(x^{2})+xq(x^{2})+p(1)+q(1) \\\\\n& = (xp(x^{2})+p(1))+(xq(x^{2})+q(1)) \\\\\n& = T(p(x))+T(q(x))\n\\end{align*}\\] and \\[\\begin{align*}\nT((\\lambda{p})(x)) & = x(\\lambda{p})(x^{2})+(\\lambda{p})(1) \\\\\n& = \\lambda(xp(x^{2}))+\\lambda(p(1)) \\\\\n& = \\lambda(xp(x^{2})+p(1)) \\\\\n& = \\lambda{T}(p(x)).\n\\end{align*}\\]\nThis is not linear. For example, consider \\(\\textbf{x}=(1,1)\\in U\\) and \\(\\lambda=2\\). Then \\[\\begin{equation*}\nT(\\lambda{x})=T(2(1,1))=T((2,2))=2\\times 2 =4.\n\\end{equation*}\\] However \\[\\begin{equation*}\n\\lambda{T}(\\textbf{x})=2T((1,1))=2(1\\times 1)=2.\n\\end{equation*}\\] Hence \\(T(\\lambda\\textbf{x})\\neq\\lambda{T}(\\textbf{x})\\) and so \\(T\\) is not linear.\n\n\n\n\n\n\n\n\nQuestion 2.3\n\n\nA linear mapping \\(S : \\mathbb{R}^{3}\\longrightarrow\\mathbb{R}^{4}\\) is such that \\[\\begin{eqnarray*}\nS((1,0,0))&=&(2,-1,0,4), \\\\\nS((0,1,0))&=&(1,3,-4,7), \\text{ and } \\\\\nS((0,0,1))&=&(0,0,5,2)\n\\end{eqnarray*}\\]\nFind a general formula for \\(S((x_{1},x_{2},x_{3}))\\).\n\n\n\n\n\nShow Solution 2.3\n\n\n\n\nSolution 2.3\n\n\nSince \\(S\\) is a linear mapping, \\(S(\\lambda\\textbf{x})=\\lambda{S(\\textbf{x})}\\) for all \\(\\textbf{x}\\in\\mathbb{R}^{3}\\), \\(\\lambda\\in\\mathbb{R}\\). Hence \\[\\begin{align*}\nS((x_{1},0,0)) & = x_{1}S((1,0,0))= (2x_{1},-x_{1},0,4x_{1}) \\\\\nS((0,x_{2},0)) & = x_{2}S((0,1,0))=(x_{2},3x_{2},-4x_{2},7x_{2}) \\\\\nS((0,0,x_{3})) & = x_{3}S((0,0,1))=(0,0,5x_{3},2x_{3}).\n\\end{align*}\\] Now, once again since \\(S\\) is linear, \\[\\begin{align*}\nS((x_{1},x_{2},x_{3})) & = S((x_{1},0,0)+(0,x_{2},0)+(0,0,x_{3})) \\\\\n& = S((x_{1},0,0))+S((0,x_{2},0))+S((0,0,x_{3})) \\\\\n& =(2x_{1},-x_{1},0,4x_{1})+(x_{2},3x_{2},-4x_{2},7x_{2})+(0,0,5x_{3},2x_{3}) \\\\\n& = (2x_{1}+x_{2},-x_{1}+3x_{2},-4x_{2}+5x_{3},4x_{1}+7x_{2}+2x_{3}).\n\\end{align*}\\]\n\n\n\n\n\n\n\nQuestion 2.4\n\n\nA linear mapping \\(T : \\mathbb{R}^{3}\\longrightarrow\\mathbb{R}^{3}\\) is such that \\[\\begin{eqnarray*}\nT((1,1,1))&=&(1,-1,1) \\\\\nT((1,1,0))&=&(-2,1,-1), \\text{ and } \\\\\nT((1,0,0))&=&(3,1,0).\n\\end{eqnarray*}\\] Obtain a general formula for \\(T((x_{1},x_{2},x_{3}))\\).\n\n\n\n\n\nShow Solution 2.4\n\n\n\n\nSolution 2.4\n\n\nWe have \\[\\begin{align*}\nT((0,1,0)) & = T((1,1,0)-(1,0,0)) \\\\\n& = T((1,1,0))-T((1,0,0))\\:\\:\\:\\:\\: (T\\:\\:{\\rm{linear}}) \\\\\n& = (-2,1,-1)-(3,1,0) \\\\\n& = (-5,0,-1).\n\\end{align*}\\] In addition, \\[\\begin{align*}\nT((0,0,1)) & = T((1,1,1)-(1,1,0)) \\\\\n& = T((1,1,1))-T((1,1,0))\\:\\:\\;\\:\\: (T\\:\\:{\\rm{linear}}) \\\\\n& = (1,-1,1)-(-2,1,-1) \\\\\n& = (3,-2,2).\n\\end{align*}\\] Hence \\[\\begin{align*}\nT((x_{1},0,0)) & = x_{1}T((1,0,0))\\:\\:\\:\\:\\: (T\\:\\:{\\rm{linear}}) \\\\\n& = (3x_{1},x_{1},0).\n\\end{align*}\\] \\[\\begin{align*}\nT((0,x_{2},0)) & = x_{2}T((0,1,0))\\:\\:\\:\\:\\: (T\\:\\:{\\rm{linear}}) \\\\\n& = (-5x_{2},0,-x_{2}).\n\\end{align*}\\] \\[\\begin{align*}\nT((0,0,x_{3})) & = x_{3}T((0,0,1))\\:\\:\\:\\:\\: (T\\:\\:{\\rm{linear}}) \\\\\n& = (3x_{3},-2x_{3},2x_{3}).\n\\end{align*}\\] Finally, \\[\\begin{align*}\nT((x_{1},x_{2},x_{3})) & = T((x_{1},0,0)+(0,x_{2},0)+(0,0,x_{3})) \\\\\n& = T((x_{1},0,0))+T((0,x_{2},0))+T((0,0,x_{3}))\\:\\:\\:\\:\\: (T\\:\\:{\\rm{linear}}) \\\\\n& = (3x_{1},x_{1},0)+(-5x_{2},0,-x_{2})+(3x_{3},-2x_{3},2x_{3}) \\\\\n& = (3x_{1}-5x_{2}+3x_{3},x_{1}-2x_{3},-x_{2}+2x_{3}).\n\\end{align*}\\]\n\n\n\n\n\n\n\nQuestion 2.5\n\n\nLet \\(T : V\\longrightarrow W\\) be a linear mapping. Prove that if the sequence \\((\\textbf{v}_{1},...,\\textbf{v}_{k})\\) is linearly dependent in \\(V\\) then \\((T(\\textbf{v}_{1}),...,T(\\textbf{v}_{k}))\\) is linearly dependent in \\(W\\).\n\n\n\n\n\nShow Solution 2.5\n\n\n\n\nSolution 2.5\n\n\nSince the sequence \\((\\textbf{v}_{1},...,\\textbf{v}_{k})\\) is linearly dependent there exists scalars \\(\\lambda_{1},...,\\lambda_{k}\\in F\\) not all zero such that \\[\\begin{equation*}\n\\lambda_{1}\\textbf{v}_{1}+...+\\lambda_{k}\\textbf{v}_{k}=\\textbf{0}_{V}.\n\\end{equation*}\\] Hence \\[\\begin{equation*}\nT(\\lambda_{1}\\textbf{v}_{1}+...+\\lambda_{k}\\textbf{v}_{k})=T(\\textbf{0}_{V}).\n\\end{equation*}\\] Since \\(T\\) is linear, this implies that \\[\\begin{equation*}\n\\lambda_{1}T(\\textbf{v}_{1})+...+\\lambda_{k}T(\\textbf{v}_{k})=\\textbf{0}_{W}.\n\\end{equation*}\\] Hence we see that a non-trivial linear combination of the vectors \\(T(\\textbf{v}_{1}),...,T(\\textbf{v}_{k})\\) is equal to the zero vector and so the sequence \\((T(\\textbf{v}_{1}),...,T(\\textbf{v}_{k}))\\) is linearly dependent.\n\n\n\n\n\n\n\nQuestion 2.6\n\n\nLet \\(T : V\\longrightarrow W\\) be a linear mapping. Show that if the sequence \\((\\textbf{v}_{1},...,\\textbf{v}_{k})\\) is linearly independent in \\(V\\) and \\(\\krn(T)\\) is trivial then the sequence \\((T(\\textbf{v}_{1}),...,T(\\textbf{v}_{k}))\\) is linearly independent in \\(W\\).\n\n\n\n\n\nShow Solution 2.6\n\n\n\n\nSolution 2.6\n\n\nSuppose that \\[\\begin{equation*}\n\\lambda_{1}T(\\textbf{v}_{1})+...+\\lambda_{k}T(\\textbf{v}_{k})=\\textbf{0}_{W}\\:\\:\\:\\:\\:\\:\\:\\: (\\lambda_{i}\\in \\F)\n\\end{equation*}\\] Then, since \\(T\\) is linear, \\[\\begin{equation*}\nT(\\lambda_{1}\\textbf{v}_{1}+...+\\lambda_{k}\\textbf{v}_{k})=\\textbf{0}_{W}.\n\\end{equation*}\\] Hence \\(\\lambda_{1}\\textbf{v}_{1}+...+\\lambda_{k}\\textbf{v}_{k}\\in{\\rm{ker}}(T)=\\{\\textbf{0}_{V}\\}\\).\nThus, \\[\\begin{equation*}\n\\lambda_{1}\\textbf{v}_{1}+...+\\lambda_{k}\\textbf{v}_{k}=\\textbf{0}_{V}.\n\\end{equation*}\\] Now since the sequence \\((\\textbf{v}_{1},...,\\textbf{v}_{k})\\) is linearly independent we have \\(\\lambda_{1}=...=\\lambda_{k}=0\\).\nHence the sequence \\((T(\\textbf{v}_{1}),...,T(\\textbf{v}_{k}))\\) is linearly independent.\n\n\n\n\n\n\n\nQuestion 2.7\n\n\nFind bases of the image and kernel of the linear mapping \\(S : \\mathbb{R}^{3}\\longrightarrow\\mathbb{R}^{3}\\) defined by \\[\\begin{equation*}\nS((x,y,z))=(x+2y+z,\\:x+2y+z,\\:2x+4y+2z).\n\\end{equation*}\\]\n\n\n\n\n\nShow Solution 2.7\n\n\n\n\nSolution 2.7\n\n\n\\(\\im(S)\\) is the set of all vectors of the form \\[\\begin{equation*}\n(x+2y+z,\\:x+2y+z,\\:2x+4y+2z)=x(1,1,2)+y(2,2,4)+z(1,1,2)\n\\end{equation*}\\] so \\(\\im(S)=\\:\\spn((1,1,2),\\:(2,2,4),\\:(1,1,2))\\). Now \\[\\begin{equation*}\n\\left(\\begin{array}{ccc} 1 & 1 & 2 \\\\ 2 & 2 & 4 \\\\ 1 & 1 & 2\\end{array}\\right)\\sim\\left(\\begin{array}{ccc} 1 & 1 & 2 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Hence a basis of \\(\\im(S)\\) is \\(((1,1,2))\\) and \\(\\rank(S)=1\\). It follows from the Rank-Nullity Theorem that \\(\\nul(S)=\\dimn(\\mathbb{R}^{3})-\\rank(S)=3-1=2\\)..\nNow let \\((x,y,z)\\in\\mathbb{R}^{3}\\). Then \\[\\begin{align*}\n(x,y,z)\\in{\\rm ker}(S)  & \\Leftrightarrow T((x,y,z))=(0,0,0) \\\\\n& \\Leftrightarrow x+2y+z=0,\\:\\:\\: x+2y+z=0,\\:\\:\\: 2x+4y+2z=0.\n\\end{align*}\\] We have the augmented matrix \\[\\begin{equation*}\n\\left(\\begin{array}{ccc|c} 1 & 2 & 1 & 0 \\\\ 1 & 2 & 1 & 0 \\\\ 2 & 4 & 2 & 0\\end{array}\\right)\\sim\\left(\\begin{array}{ccc|c} 1 & 2 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Hence \\(\\krn(S)=\\{(-2s-t,s,t) : s,t\\in\\mathbb{R}\\}\\). Let \\((-2s-t,s,t)\\in \\krn(S)\\). Then \\[\\begin{equation*}\n(-2s-t,s,t)=s(-2,1,0)+t(-1,0,1).\n\\end{equation*}\\] Hence a basis of \\(\\krn(S)\\) is \\(((-2,1,0),\\:(-1,0,1))\\) by the \\(2\\) of \\(3\\) properties proposition.\n\n\n\n\n\n\n\nQuestion 2.8\n\n\nDefine \\(T : \\mathbb{R}_{n\\times n}\\longrightarrow \\mathbb{R}_{n\\times n}\\) by \\(T(A)=A^{T}\\) for all \\(A\\in\\mathbb{R}_{n\\times n}\\).\nShow that \\(T\\) is a linear mapping and find explicitly the kernel and image of \\(T\\). State also the rank and nullity of \\(T\\).\n\n\n\n\n\nShow Solution 2.8\n\n\n\n\nSolution 2.8\n\n\nLet \\(A,B\\in\\mathbb{R}_{n\\times n}\\) and \\(\\lambda\\in\\mathbb{R}\\). Then \\[\\begin{equation*}\nT(A+B)=(A+B)^{T}=A^{T}+B^{T}=T(A)+T(B)\n\\end{equation*}\\] and \\[\\begin{equation*}\nT(\\lambda{A})=(\\lambda{A})^{T}=\\lambda{A^{T}}=\\lambda{T(A)}.\n\\end{equation*}\\] Hence \\(T\\) is linear. Now \\(T\\) is surjective since for all \\(A\\in\\mathbb{R}_{n\\times n}\\), \\[\\begin{equation*}\nA=(A^{T})^{T}=T(A^{T})\n\\end{equation*}\\] and hence \\(\\im(T)=\\mathbb{R}_{n\\times n}\\) and \\(\\rank(T)=\\dimn(\\mathbb{R}_{n\\times n})=n^{2}\\).\nAlso, \\[\\begin{align*}\nA\\in\\krn(T) & \\Leftrightarrow T(A)=0 \\\\\n& \\Leftrightarrow A^{T}=0 \\\\\n& \\Leftrightarrow A=0.\n\\end{align*}\\] Hence \\(\\krn(T)=\\{\\textbf{0}\\}\\) and \\(\\nul(T)=0\\).}\n\n\n\n\n\n\n\nQuestion 2.9\n\n\nShow that the mapping \\(T : P_{2}\\longrightarrow P_{3}\\) defined by \\(T(p(x))=xp(x)\\) for all \\(p\\in P_{2}\\) is linear. Find the rank and nullity of \\(T\\).\n\n\n\n\n\nShow Solution 2.9\n\n\n\n\nSolution 2.9\n\n\nLet \\(p,q\\in P_{2}\\) and \\(\\lambda\\in\\mathbb{R}\\). Then \\[\\begin{equation*}\nT((p+q)(x))=x(p+q)(x)=x(p(x)+q(x))=xp(x)+xq(x)=T(p(x))+T(q(x))\n\\end{equation*}\\] and \\[\\begin{equation*}\nT((\\lambda{p})(x))=x(\\lambda{p})(x)=x\\lambda{p}(x)=\\lambda(xp(x))=\\lambda{T}(p(x))\n\\end{equation*}\\] Hence \\(T\\) is linear.\n\\(\\im(T)=\\{ax+bx^{2}+cx^{3} : a,b,c\\in\\mathbb{R}\\}\\). Hence a basis for \\(\\im(T)\\) is \\((x,x^{2},x^{3})\\) and so \\(\\rank(T)=3\\). Now \\[\\begin{align*}\np(x)=a+bx+cx^{2}\\in\\krn(T) & \\Leftrightarrow T(p(x))=0 \\\\\n& \\Leftrightarrow ax+bx^{2}+cx^{3}=0 \\\\\n& \\Leftrightarrow a=b=c=0 \\\\\n& \\Leftrightarrow p(x)=0\n\\end{align*}\\] Hence \\(\\krn(T)=\\{\\textbf{0}\\}\\) and \\(\\nul(T)=0\\).}\n\n\n\n\n\n\n\nQuestion 2.10\n\n\nLet \\(W\\) denote the vector space of all symmetric \\(2\\times 2\\) real matrices. Find the nullity of the linear mapping \\(T : W\\longrightarrow P_{2}\\) defined by \\[\\begin{equation*}\nT\\Bigg(\\left(\\begin{array}{cc} a & b\\\\ b & c\\end{array}\\right)\\Bigg)=(a-b)+(b-c)x+(c-a)x^{2}\n\\end{equation*}\\] and use this to deduce the value of rank\\((T)\\) from the Rank-Nullity Theorem. Use this information to find a basis of im\\((T)\\).\n\n\n\n\n\nShow Solution 2.10\n\n\n\n\nSolution 2.10\n\n\nWe have \\[\\begin{align*}\nA=\\left(\\begin{array}{cc} a & b \\\\ b & c\\end{array}\\right)\\in{\\rm{ker}}(T) & \\Leftrightarrow T(A)=0 \\\\\n& \\Leftrightarrow (a-b)+(b-c)x+(c-a)x^{2}=0 \\\\\n& \\Leftrightarrow a-b=0,\\:b-c=0,\\:c-a=0 \\\\\n& \\Leftrightarrow a=b=c.\n%& \\Leftrightarrow A=\\left(\\begin{array}{cc} a & a \\\\ a & a\\end{array}\\right).\n\\end{align*}\\] Hence \\(\\krn(T)=\\Bigg\\{\\left(\\begin{array}{cc} a & a \\\\ a & a\\end{array}\\right) : a\\in\\mathbb{R}\\Bigg\\}\\) and so a basis for \\(\\krn(T)\\) is \\(\\Bigg(\\left(\\begin{array}{cc} 1 & 1 \\\\ 1 & 1\\end{array}\\right)\\Bigg)\\) and \\(\\nul(T)=1\\).\nIt follows from the Rank-Nullity Theorem that \\(\\rank(T)=\\:\\dimn W-\\:\\nul(T)=3-1=2\\).\nNow \\(\\im(T)=\\{a(1-x^{2})+b(-1+x)+c(-x+x^{2}) : a,b,c\\in\\mathbb{R}\\}\\). Hence \\[\\im(T)=\\:\\spn(1-x^{2},\\:-1+x,\\:-x+x^{2}).\\] The sequence \\((1-x^{2},\\:-1+x,\\:-x+x^{2})\\) has length \\(3\\) and hence must be linearly dependent as rank\\((T)=2\\). It follows from the Minus Theorem that we should be able to throw away one of the vectors in this sequence and retain a spanning sequence for \\(\\im(T)\\). We have \\[\\begin{equation*}\n-x+x^{2}=-(1-x^{2})-(-1+x)\n\\end{equation*}\\] Hence the sequence \\((1-x^{2},\\:-1+x)\\) still spans \\(\\im(T)\\) by the Minus Theorem and has length equal to \\(\\rank(T)\\) so by the ‘two of three’ properties proposition, \\((1-x^{2},\\:-1+x)\\) is a basis of \\(\\im(T)\\).\n\n\n\n\n\n\n\nQuestion 2.11\n\n\nLet \\(V\\) and \\(W\\) be vector spaces of dimensions \\(3\\) and \\(4\\) respectively and let \\(L_{V}=(\\textbf{e}_{1},\\textbf{e}_{2},\\textbf{e}_{3})\\) and \\(L_{W}=(\\textbf{f}_{1},\\textbf{f}_{2},\\textbf{f}_{3},\\textbf{f}_{4})\\) be bases of \\(V\\) and \\(W\\) respectively. Given that \\[\\begin{equation*}\nM(T;L_{V},L_{W})=\\left(\\begin{array}{ccc} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ 10 & 11 & 12\\end{array}\\right)\n\\end{equation*}\\]\n\nWrite down \\(T(\\textbf{e}_{2})\\) as a linear combination of \\(\\textbf{f}_{1},\\textbf{f}_{2},\\textbf{f}_{3},\\textbf{f}_{4}\\).\nBy evaluating one matrix product, obtain \\(T(2\\textbf{e}_{1}+\\textbf{e}_{2}-\\textbf{e}_{3})\\) as a linear combination of \\(\\textbf{f}_{1},\\textbf{f}_{2},\\textbf{f}_{3},\\textbf{f}_{4}\\).\n\n\n\n\n\n\nShow Solution 2.11\n\n\n\n\nSolution 2.11\n\n\n\nWe have \\[\\begin{equation*}\nT(\\textbf{e}_{2})=2\\textbf{f}_{1}+5\\textbf{f}_{2}+8\\textbf{f}_{3}+11\\textbf{f}_{4}.\n\\end{equation*}\\]\nWe evaluate \\[\\begin{equation*}\n\\left(\\begin{array}{ccc} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\\\ 10 & 11 & 12\\end{array}\\right)\\left(\\begin{array}{c} 2 \\\\ 1 \\\\ -1\\end{array}\\right)=\\left(\\begin{array}{c} 1 \\\\ 7 \\\\ 13 \\\\ 19\\end{array}\\right).\n\\end{equation*}\\] Hence \\(T((2,1,-1))=\\textbf{f}_{1}+7\\textbf{f}_{2}+13\\textbf{f}_{3}+19\\textbf{f}_{4}\\).\n\n\n\n\n\n\n\n\nQuestion 2.12\n\n\nLet \\(D : P_{3}\\longrightarrow P_{2}\\) be the linear mapping defined by \\(D(p(x))=p^{'}(x)\\) for all \\(p\\in P_{3}\\). Let \\(B=(1,x,x^{2},x^{3})\\) and \\(C=(1,x,x^{2})\\) be the standard bases for \\(P_{3}\\) and \\(P_{2}\\) respectively. Find the matrix of \\(D\\) with respect to \\(B\\) and \\(C\\).\n\n\n\n\n\nShow Solution 2.12\n\n\n\n\nSolution 2.12\n\n\nWe have \\[\\begin{align*}\nD(1) & = 0 \\\\\nD(x) & = 1 \\\\\nD(x^{2}) & = 2x \\\\\nD(x^{3}) & = 3x^{2}.\n\\end{align*}\\] Hence the matrix of \\(D\\) w.r.t. the bases \\(B\\) and \\(C\\) is \\[\\begin{equation*}\nM_{D}=\\left(\\begin{array}{cccc} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 2 & 0 \\\\ 0 & 0 & 0 & 3\\end{array}\\right).\n\\end{equation*}\\]\n\n\n\n\n\n\n\nQuestion 2.13\n\n\nLet \\(S : V\\longrightarrow W\\) and \\(T : U\\longrightarrow V\\) be linear mappings and let \\(L_{U},L_{V},L_{W}\\) be bases of \\(U,V\\) and \\(W\\) respectively. Show that \\(M(ST;L_{U},L_{W})=M(S;L_{V},L_{W})M(T;L_{U},L_{V})\\).\n(Here, as usual, \\(ST\\) means the composition \\(S\\circ T\\).)\n\n\n\n\n\nShow Solution 2.13\n\n\n\n\nSolution 2.13\n\n\nLet \\(\\textbf{x}\\) be an arbitrary vector in \\(U\\). Let the coordinate column vectors of \\(\\textbf{x}\\), \\(T(\\textbf{x})\\) and \\((ST)(\\textbf{x})\\) (with respect to \\(L_{U},L_{V}\\) and \\(L_{W}\\) respectively) be \\(X,Y\\) and \\(Z\\). Then \\(Y=M_{T} X\\) and \\(Z=M_{S}Y\\). Hence \\(Z=(M_{S}M_{T})X\\). Since \\(\\textbf{x}\\in U\\) was arbitrary, it follows that \\(M_{ST}=M_{S}M_{T}\\) as required.\n\n\n\n\n\n\n\nQuestion 2.14\n\n\nConsider the bases \\[\\begin{equation*}\nB_{1}=((1,0,0),(0,1,0),(0,0,1))\n\\end{equation*}\\] and \\[\\begin{equation*}\nB_{2}=((1,1,0),(0,1,1),(1,0,1))\n\\end{equation*}\\] of \\(\\mathbb{R}^{3}\\).\n\nFind the change of basis matrices \\(M(B_{1}\\rightarrow B_{2})\\) and \\(M(B_{2}\\rightarrow B_{1})\\).\nUse your answer to express \\(\\textbf{x}=(1,2,3)\\) as a linear combination of the vectors in \\(B_{2}\\).\n\n\n\n\n\n\nShow Solution 2.14\n\n\n\n\nSolution 2.14\n\n\n\nTo find the change of basis matrix \\(M(B_{1}\\rightarrow B_{2})\\) we must express each of the basis vectors in \\(B_{2}\\) as linear combinations of the basis vectors in \\(B_{1}\\). We have \\[\\begin{align*}\n(1,1,0) & =1(1.0,0)+1(0,1,0) \\\\\n(0,1,1) & = 1(0,1,0)+1(0,0,1) \\\\\n(1,0,1) & = 1(1,0,0)+1(0,0,1)\n\\end{align*}\\] Hence \\[\\begin{equation*}\nM(B_{1}\\rightarrow B_{2})=\\left(\\begin{array}{ccc} 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1\\end{array}\\right).\n\\end{equation*}\\] Now via EROs on the augmented system \\((P\\:|\\:I)\\) where \\(P=M(B_{1}\\longrightarrow B_{2})\\), we see that \\[\\begin{equation*}\nP^{-1}=M(B_{2}\\rightarrow B_{1})=\\left(\\begin{array}{ccc} \\frac{1}{2} & \\frac{1}{2} & -\\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & -\\frac{1}{2} & \\frac{1}{2}\\end{array}\\right).\n\\end{equation*}\\]\nFinally, to express \\((1,2,3)\\) in terms of the basis \\(B_{2}\\), we calculate the product \\[\\begin{equation*}\n\\left(\\begin{array}{ccc} \\frac{1}{2} & \\frac{1}{2} & -\\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{1}{2} & \\frac{1}{2} \\\\ \\frac{1}{2} & -\\frac{1}{2} & \\frac{1}{2}\\end{array}\\right)\\left(\\begin{array}{c} 1 \\\\ 2 \\\\ 3\\end{array}\\right)=\\left(\\begin{array}{c} 0 \\\\ 2 \\\\ 1\\end{array}\\right)\n\\end{equation*}\\] and so \\[\\begin{equation*}\n(1,2,3)=2(0,1,1)+1(1,0,1).\n\\end{equation*}\\]\n\n\n\n\n\n\n\n\nQuestion 2.15\n\n\nConsider the standard basis \\(B_{1}\\) of \\(\\mathbb{R}^{3}\\) and the basis \\(B_{2}=(\\textbf{w}_{1},\\textbf{w}_{2},\\textbf{w}_{3})\\) where \\[\\begin{equation*}\n\\textbf{w}_{1}=(2,1,1),\\:\\:\\: \\textbf{w}_{2}=(0,1,3),\\:\\:\\: \\textbf{w}_{3}=(0,0,2).\n\\end{equation*}\\]\n\nFind the change of basis matrices \\(M(B_{1}\\rightarrow B_{2})\\) and \\(M(B_{2}\\rightarrow B_{1})\\).\nUse an appropriate change of basis matrix to find the coordinate vector of \\((x,y,z)\\in\\mathbb{R}^{3}\\) w.r.t. the basis \\(B_{2}\\). Verify your answer.\n\n\n\n\n\n\nShow Solution 2.15\n\n\n\n\nSolution 2.15\n\n\n\nThe change of basis matrix from \\(B_{1}\\) to \\(B_{2}\\) is \\[\\begin{equation*}\nM(B_{1}\\rightarrow B_{2})=\\left(\\begin{array}{ccc} 2 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 3 & 2\\end{array}\\right).\n\\end{equation*}\\] Now let \\(\\textbf{e}_{1}=(1,0,0)\\), \\(\\textbf{e}_{2}=(0,1,0)\\) and \\(\\textbf{e}_{3}=(0,0,1)\\) so that \\(B_{1}=(\\textbf{e}_{1},\\textbf{e}_{2},\\textbf{e}_{3})\\). Then \\[\\begin{align*}\n(0,0,1) & = \\frac{1}{2}\\textbf{w}_{3} \\\\\n(0,1,0) & = \\textbf{w}_{2}-\\frac{3}{2}\\textbf{w}_{3} \\\\\n(1,0,0) & = \\frac{1}{2}(\\textbf{w}_{1}-\\textbf{e}_{2}-\\textbf{e}_{3})=\\frac{1}{2}\\textbf{w}_{1}-\\frac{1}{2}\\textbf{w}_{2}+\\frac{1}{2}\\textbf{w}_{3}.\n\\end{align*}\\] Hence \\[\\begin{equation*}\nM(B_{2}\\rightarrow B_{1})=\\left(\\begin{array}{ccc} \\frac{1}{2} & 0 & 0 \\\\ -\\frac{1}{2} & 1 & 0 \\\\ \\frac{1}{2} & -\\frac{3}{2} & \\frac{1}{2}\\end{array}\\right).\n\\end{equation*}\\] Obviously we could have used the fact that if \\(P=M(B_{1}\\rightarrow B_{2})\\) then \\(M(B_{2}\\rightarrow B_{1})=P^{-1}\\) as we did in Q1 and calculated \\(P^{-1}\\) by applying EROs to the augmented matrix \\((P\\:|\\:I)\\).\nNow to find the coordinate column vector of \\(\\textbf{x}=(x,y,z)\\) w.r.t. \\(B_{2}\\) we just need to calculate the product \\[\\begin{equation*}\n\\left(\\begin{array}{ccc} \\frac{1}{2} & 0 & 0 \\\\ -\\frac{1}{2} & 1 & 0 \\\\ \\frac{1}{2} & -\\frac{3}{2} & \\frac{1}{2}\\end{array}\\right)\\left(\\begin{array}{c} x \\\\ y \\\\ z\\end{array}\\right)=\\left(\\begin{array}{c} \\frac{1}{2}x \\\\ -\\frac{1}{2}x+y \\\\ \\frac{1}{2}x-\\frac{3}{2}y+\\frac{1}{2}z\\end{array}\\right).\n\\end{equation*}\\] To verify, \\[\\begin{align*}\n& \\frac{1}{2}x(2,1,1)+(-\\frac{1}{2}x+y)(0,1,3)+(\\frac{1}{2}x-\\frac{3}{2}y+\\frac{1}{2}z)(0,0,2) \\\\\n& = (x,\\:\\frac{1}{2}x+(-\\frac{1}{2}x+y),\\:\\frac{1}{2}x+(-\\frac{3}{2}x+3y)+(x-3y+z)) \\\\\n& = (x,y,z).\n\\end{align*}\\]\n\n\n\n\n\n\n\n\nQuestion 2.16\n\n\n(A number crunch workout) Consider the bases \\[\\begin{equation*}\nB_{1}=\\Bigg(\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 1 \\\\ 0 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 0 \\\\ 1 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 0 & 0 \\\\ 0 & 1\\end{array}\\right)\\Bigg)\n\\end{equation*}\\] and \\[\\begin{equation*}\nB_{2}=\\Bigg(\\left(\\begin{array}{cc} 1 & 2 \\\\ 0 & -1\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 2 & 1 \\\\ 1 & 0\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right),\\:\\:\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 1\\end{array}\\right)\\Bigg)\n\\end{equation*}\\] of \\(\\mathbb{R}_{2\\times 2}\\).\n\nFind the change of basis matrix \\(M(B_{2}\\rightarrow B_{1})\\).\nUse part a. to express the matrix \\(A=\\left(\\begin{array}{cc} 4 & 2 \\\\ 0 & -1\\end{array}\\right)\\) as a linear combination of the matrices in \\(B_{2}\\).\n\n\n\n\n\n\nShow Solution 2.16\n\n\n\n\nSolution 2.16\n\n\n\nWe must first of all express each basis matrix in \\(B_{1}\\) as a linear combination of the basis matrices in \\(B_{2}\\). For example, to express \\(\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right)\\) in terms of the matrices in \\(B_{2}\\) we need \\[\\begin{equation*}\n\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right) = \\alpha\\left(\\begin{array}{cc} 1 & 2 \\\\ 0 & -1\\end{array}\\right)+\\beta\\left(\\begin{array}{cc} 2 & 1 \\\\ 1 & 0\\end{array}\\right)+\\gamma\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right)+\\delta\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 1\\end{array}\\right)\n\\end{equation*}\\] leading to the system of equations \\[\\begin{align*}\n\\alpha+2\\beta+\\gamma+\\delta & = 1 \\\\\n2\\alpha+\\beta+\\gamma & = 0 \\\\\n\\beta & = 0 \\\\\n-\\alpha+\\gamma+\\delta & = 0.\n\\end{align*}\\] This has the unique solution \\(\\alpha=\\frac{1}{2}\\), \\(\\beta=0\\), \\(\\gamma=-1\\), \\(\\delta=\\frac{3}{2}\\). Once we have expressed the other matrices in \\(B_{1}\\) as lin combs of the matrices in \\(B_{2}\\), we see that the change of basis matrix is then \\[\\begin{equation*}\nM(B_{2}\\rightarrow B_{1})=\\left(\\begin{array}{cccc} \\frac{1}{2} & 0 & -1 & -\\frac{1}{2} \\\\ 0 & 0 & 1 & 0 \\\\ -1 & 1 & 1 & 1 \\\\ \\frac{3}{2} & -1 & -2 & -\\frac{1}{2}\\end{array}\\right).\n\\end{equation*}\\]\nNow the coordinate vector of \\(A=\\left(\\begin{array}{cc} 4 & 2 \\\\ 0 & -1\\end{array}\\right)\\) w.r.t. \\(B_{1}\\) is \\((4,2,0,-1)\\) and hence we can find the coordinate vector of \\(A\\) w.r.t. \\(B_{2}\\) by finding the matrix product \\[\\begin{equation*}\n\\left(\\begin{array}{cccc} \\frac{1}{2} & 0 & -1 & -\\frac{1}{2} \\\\ 0 & 0 & 1 & 0 \\\\ -1 & 1 & 1 & 1 \\\\ \\frac{3}{2} & -1 & -2 & -\\frac{1}{2}\\end{array}\\right)\\left(\\begin{array}{c} 4 \\\\ 2 \\\\ 0 \\\\ -1\\end{array}\\right)=\\left(\\begin{array}{c} \\frac{5}{2} \\\\ 0 \\\\ -3 \\\\ \\frac{9}{2}\\end{array}\\right).\n\\end{equation*}\\] Hence \\[\\begin{equation*}\n\\left(\\begin{array}{cc} 4 & 2 \\\\ 0 & -1\\end{array}\\right)=\\frac{5}{2}\\left(\\begin{array}{cc} 1 & 2 \\\\ 0 & -1\\end{array}\\right)-3\\left(\\begin{array}{cc} 1 & 1 \\\\ 0 & 1\\end{array}\\right)+\\frac{9}{2}\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 1\\end{array}\\right).\n\\end{equation*}\\]\n\n\n\n\n\n\n\n\nQuestion 2.17\n\n\nFind the two eigenvalues of the linear mapping \\(T : \\mathbb{C}^{2}\\longrightarrow \\mathbb{C}^{2}\\) defined by \\[\\begin{equation*}\nT((x,y))=(-x+3y,\\:3x-y)  \n\\end{equation*}\\] for all \\((x,y)\\in\\mathbb{C}^{2}\\). Find a basis of each corresponding eigenspace.\n\n\n\n\n\nShow Solution 2.17\n\n\n\n\nSolution 2.17\n\n\nWe calculate the matrix \\(M_{T}\\) of \\(T\\) w.r.t the standard basis \\(((1,0),\\:(0,1))\\) of \\(\\mathbb{C}^{2}\\). Hence \\[\\begin{equation*}\nM_{T}=\\left(\\begin{array}{cc} -1 & 3 \\\\ 3 & -1\\end{array}\\right).\n\\end{equation*}\\] Now we calculate the eigenvalues of \\(M_{T}\\). We have \\[\\begin{align*}\n\\det(\\lambda{I}-M_{T}) & =\\left|\\begin{array}{cc} \\lambda+1 & -3 \\\\ -3 & \\lambda+1\\end{array}\\right| \\\\\n& = (\\lambda+1)^{2}-9 \\\\\n& = \\lambda^{2}+2\\lambda-8 \\\\\n& = (\\lambda+4)(\\lambda-2)\n\\end{align*}\\] Hence the eigenvalues of \\(T\\) are the roots of this polynomial. Hence we have the two eigenvalues \\(\\lambda_{1}=-4\\) and \\(\\lambda_{2}=2\\).\nWe now find corresponding eigenvectors of \\(M_{T}\\):\n\n\\(\\lambda_{1}=-4\\):\n\nWe have the homogeneous system \\((-4I-M_{T})X=0\\) where \\(X={\\rm col}(x,y)\\) with augmented matrix \\[\\begin{equation*}\n\\left(\\begin{array}{cc|c} -3 & -3 & 0 \\\\ -3 & -3 & 0\\end{array}\\right)\\sim \\left(\\begin{array}{cc|c} 1 & 1 & 0 \\\\ 0 & 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Hence we require \\(x+y=0\\Rightarrow y=-x\\). So the eigenspace \\(E(-4,T)\\) consist of all vectors of the form \\((t,-t)=t(1,-1)\\). Hence \\(E(-4,T)\\) is one-dimensional and a basis is \\(((1,-1))\\).\n\n\\(\\lambda_{2}=2\\):\n\nWe have the homogeneous system \\((2I-M_{T})X=0\\) where \\(X={\\rm col}(x,y)\\) with augmented matrix \\[\\begin{equation*}\n\\left(\\begin{array}{cc|c} 3 & -3 & 0 \\\\ -3 & 3 & 0\\end{array}\\right)\\sim \\left(\\begin{array}{cc|c} 1 & -1 & 0 \\\\ 0 & 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Hence we require \\(x-y=0\\Rightarrow x=y\\). So the eigenspace \\(E(2,T)\\) consists of all vectors of the form \\((t,t)=t(1,1)\\). Hence \\(E(2,T)\\) is one-dimensional and a basis is \\(((1,1))\\).\n\n\n\n\n\n\n\n\n\nQuestion 2.18\n\n\nConsider the linear mapping \\(T : P_{2}\\longrightarrow P_{2}\\) defined by \\[\\begin{equation*}\nT(p(x))=p(3x+2)\n\\end{equation*}\\] for all \\(p\\in P_{2}\\).\n\nFind the eigenvalues of \\(T\\) and hence find bases of each corresponding eigenspace.\nState a basis of \\(P_{2}\\) with respect to which the matrix of \\(T\\) is a diagonal matrix. Verify this directly.\n\n\n\n\n\n\nShow Solution 2.18\n\n\n\n\nSolution 2.18\n\n\n\nWe find the matrix of \\(T\\) w.r.t. the standard basis \\((1,x,x^{2})\\) of \\(P_{2}\\). We have \\[\\begin{align*}\nT(1) & = 1 \\\\\nT(x) & = 2+3x \\\\\nT(x^{2}) & = (3x+2)^{2}=4+12x+9x^{2}.\n\\end{align*}\\] Hence \\[M_{T}=\\left(\\begin{array}{ccc} 1 & 2 & 4 \\\\ 0 & 3 & 12 \\\\ 0 & 0 & 9\\end{array}\\right)\\] and so \\(M_{T}\\) is upper triangular. It follows that the eigenvalues are \\(\\lambda_{1}=1\\), \\(\\lambda_{2}=3\\) and \\(\\lambda_{3}=9\\). (NO CALCULATION REQUIRED!)\nNow we consider the homogenous system \\((\\lambda{I}-M_{T})X=0\\) where \\(X=\\:\\)col\\((x,y,z)\\).\n\n\\(\\lambda_{1}=1\\):\n\nThe augmented matrix of the homogeneous system here is \\[\\begin{equation*}\n\\left(\\begin{array}{ccc|c} 0 & -2 & -4 & 0 \\\\ 0 & -2 & -12 & 0 \\\\ 0 & 0 & -8 & 0\\end{array}\\right)\\sim\\left(\\begin{array}{ccc|c} 0 & 1 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0\\end{array}\\right).\n\\end{equation*}\\] Hence the eigenvectors of \\(M_{T}\\) corresponding to \\(\\lambda_{1}=1\\) all have the form \\({\\rm col}(t,0,0)=t\\times {\\rm col}(1,0,0)\\) for nonzero \\(t\\) and hence the of the basis vector of the eigenspace \\(E(1,T)\\) is \\((1,0,0)\\). On transferring back to \\(T\\), the eigenspace \\(E(1,T)\\) is \\(1\\)-dimensional with basis \\((1)\\).\n\n\\(\\lambda_{2}=3\\):\n\nThe augmented matrix of the homogeneous system here is \\[\\begin{equation*}\n\\left(\\begin{array}{ccc|c} 2 & -2 & -4 & 0 \\\\ 0 & 0 & -12 & 0 \\\\ 0 & 0 & -6 & 0\\end{array}\\right)\\sim\\left(\\begin{array}{ccc|c} 1 & -1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Hence the eigenvectors of \\(M_{T}\\) corresponding to \\(\\lambda_{2}=3\\) all have the form \\({\\rm col}(t,t,0)=t\\times{\\rm col}(1,1,0)\\) for nonzero \\(t\\) and so the of the basis vector of the eigenspace \\(E(3,T)\\) is \\((1,1,0)\\). On transferring back to \\(T\\), the eigenspace \\(E(3,T)\\) is \\(1\\)-dimensional with basis \\((1+x)\\).\n\n\\(\\lambda_{3}=9\\):\n\nThe augmented matrix of the homogeneous system here is \\[\\begin{equation*}\n\\left(\\begin{array}{ccc|c} 8 & -2 & -4 & 0 \\\\ 0 & 6 & -12 & 0 \\\\ 0 & 0 & 0 & 0\\end{array}\\right)\\sim\\left(\\begin{array}{ccc|c} 1 & 0 & -1 & 0 \\\\ 0 & 1 & -2 & 0 \\\\ 0 & 0 & 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Hence the eigenvectors of \\(M_{T}\\) corresponding to \\(\\lambda_{3}=9\\) all have the form \\({\\rm col}(t,2t,t)=t\\times {\\rm col}(1,2,1)\\) for nonzero \\(t\\) and so the of the basis vector of the eigenspace \\(E(9,T)\\) is \\((1,2,1)\\). On transferring back to \\(T\\), the eigenspace \\(E(9,T)\\) is \\(1\\)-dimensional with basis \\((1+2x+x^{2})\\).\n\n\nThe algebraic multiplicity of each eigenvalue is equal to its geometric multiplicity and so \\(T\\) is diagonalisable. Take \\(B=(1,\\:1+x,\\:1+2x+x^{2})\\). Then \\(B\\) is a basis of \\(P_{2}\\) and \\(M(T;B)={\\rm diag}(1,3,9)\\). This can be verified directly. We have \\[\\begin{align*}\nT(1) & =1 \\\\\nT(1+x) & = 1+(3x+2)=3+3x=3(1+x) \\\\\nT(1+2x+x^{2}) & = 1+2(3x+2)+(3x+2)^{2}=9+18x+9x^{2}\\\\&=9(1+2x+x^{2}).\n\\end{align*}\\] Hence \\[\\begin{equation*}\nM(T;B)=\\left(\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 9\\end{array}\\right)={\\rm diag}(1,3,9)\n\\end{equation*}\\] as expected.\n\n\n\n\n\n\n\n\nQuestion 2.19\n\n\nConsider the linear transformation \\(S\\) of \\(\\mathbb{C}^{2}\\) defined by \\[\\begin{equation*}\nS((x,y))=(4x+2y,\\:3x-y)\n\\end{equation*}\\] for all \\((x,y)\\in\\mathbb{C}^{2}\\). Find the eigenvalues of \\(S\\) and decide whether \\(S\\) is diagonalisable.\n\n\n\n\n\nShow Solution 2.19\n\n\n\n\nSolution 2.19\n\n\nThe matrix of \\(S\\) w.r.t. the standard basis \\(((1,0),\\:(0,1))\\) is \\[\\begin{equation*}\nM_{S}=\\left(\\begin{array}{cc} 4 & 2 \\\\ 3 & -1\\end{array}\\right).\n\\end{equation*}\\] Now we have \\[\\begin{align*}\n\\det(\\lambda{I}-M_{S}) & = \\left|\\begin{array}{cc} \\lambda-4 & -2 \\\\ -3 & \\lambda+1\\end{array}\\right| \\\\\n& = (\\lambda-4)(\\lambda+1)-6 \\\\\n& = \\lambda^{2}-3\\lambda-10 \\\\\n& = (\\lambda-5)(\\lambda+2)\n\\end{align*}\\] Hence the eigenvalues are the roots of this polynomial so we have the distinct eigenvalues \\(\\lambda_{1}=-2\\) and \\(\\lambda_{2}=5\\), each with algebraic multiplicity \\(1\\).\nNow we find an eigenvector of \\(M_{S}\\) corresponding to \\(\\lambda_{1}=-2\\). We have the homogeneous system \\((-2I-M_{S})X=0\\) where \\(x=\\:\\)col\\((x,y)\\) with augmented matrix \\[\\begin{equation*}\n\\left(\\begin{array}{cc|c} -6 & -2 & 0 \\\\ -3 & -1 & 0\\end{array}\\right)\\sim\\left(\\begin{array}{cc|c} 3 & 1 & 0 \\\\ 0 & 0 & 0\\end{array}\\right)\n\\end{equation*}\\] Hence eigenvectors of \\(M_{S}\\) corresponding to \\(\\lambda_{1}=-2\\) take the form \\((t,-3t)\\) for some nonzero \\(t\\in\\mathbb{C}\\). It follows that a basis of \\(E(-2,S)\\) is \\(((1,-3))\\).\nNext we find an eigenvector of \\(M_{S}\\) corresponding to \\(\\lambda_{2}=5\\). We have the homogeneous system \\((5I-M_{S})X=0\\) where \\(X=\\:\\)col\\((x,y)\\) with augmented matrix \\[\\begin{equation*}\n\\left(\\begin{array}{cc|c} 1 & -2 & 0 \\\\ -3 & 6 & 0\\end{array}\\right)\\sim\\left(\\begin{array}{cc|c} 1 & -2 & 0 \\\\ 0 & 0 & 0\\end{array}\\right)\n\\end{equation*}\\] Hence eigenvectors of \\(M_{S}\\) corresponding to \\(\\lambda_{2}=5\\) take the form \\((2t,t)\\) for nonzero \\(t\\in\\mathbb{C}\\). It follows that a basis of \\(E(5,S)\\) is \\(((2,1))\\). Hence the geometric multiplicity of each eigenvalue of \\(S\\) is equal to its algebraic multiplicity and so \\(S\\) is diagonalisable. Take the basis \\(B=((1,-3),\\:(2,1))\\) of \\(\\mathbb{C}^{2}\\). Then the matrix of \\(S\\) w.r.t this basis is diagonal. Precisely, \\(M(S;B)={\\rm diag}(-2,5)\\).}\n\n\n\n\n\n\n\nQuestion 2.20\n\n\nConsider the linear transformation \\(T\\) of \\(\\mathbb{C}^{2}\\) defined by \\[\\begin{equation*}\nT((x,y))=(10x-9y,\\:4x-2y)\n\\end{equation*}\\] for all \\((x,y)\\in\\mathbb{C}^{2}\\). Find the eigenvalues of \\(T\\) and decide whether \\(T\\) is diagonalisable.\n\n\n\n\n\nShow Solution 2.20\n\n\n\n\nSolution 2.20\n\n\nThe matrix of \\(T\\) w.r.t. the standard basis \\(((1,0),\\:(0,1))\\) of \\(\\mathbb{C}^{2}\\) is \\[\\begin{equation*}\nM_{T}=\\left(\\begin{array}{cc} 10 & -9 \\\\ 4 & -2\\end{array}\\right)\n\\end{equation*}\\] Now we have \\[\\begin{align*}\n\\det(\\lambda{I}-M_{T}) & = \\left|\\begin{array}{cc} \\lambda-10 & 9 \\\\ -4 & \\lambda+2\\end{array}\\right| \\\\\n& = (\\lambda-10)(\\lambda+2)+36 \\\\\n& = \\lambda^{2}-8\\lambda+16 \\\\\n& = (\\lambda-4)^{2}.\n\\end{align*}\\] The eigenvalues of \\(M_{T}\\) are the roots of this polynomial and so we have the repeated eigenvalues \\(\\lambda_{1}=\\lambda_{2}=4\\). The algebraic multiplicity of \\(4\\) as an eigenvalue is \\(2\\).\nNow we find corresponding eigenvectors. We consider the homogeneous system \\[(4I-M_{T})X=0\\] where \\(X=\\:\\)col\\((x,y)\\) with augmented matrix \\[\\begin{equation*}\n\\left(\\begin{array}{cc|c} -6 & 9 & 0 \\\\ -4 & 6 & 0\\end{array}\\right)\\sim\\left(\\begin{array}{cc|c} -2 & 3 & 0 \\\\ 0 & 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Hence eigenvectors of \\(M_{T}\\) corresponding to the eigenvalue \\(4\\) take the form \\((3t,2t)\\) for nonzero \\(t\\in\\mathbb{C}\\) and so a basis of \\(E(4,T)\\) is \\(((3,2))\\). The geometric multiplicity of \\(4\\) as an eigenvalue is therefore dim\\((E(4,T))=1\\) which is not equal to the algebraic multiplicity of \\(4\\) as an eigenvalue and so it follows that \\(T\\) is not diagonalisable.\n\n\n\n\n\n\n\nQuestion 2.21\n\n\nConsider the linear transformation \\(S\\) of \\(\\mathbb{C}^{3}\\) defined by \\[\\begin{equation*}\nS((x,y,z))=(3x-2z,\\:y,\\:x).\n\\end{equation*}\\] for all \\((x,y,z)\\in\\mathbb{C}^{3}\\). Given that \\(\\chi_{S}(t)=(t-1)^{2}(t-2)\\), find bases of the relevant eigenspaces. Is \\(S\\) diagonalisable?\n\n\n\n\n\nShow Solution 2.21\n\n\n\n\nSolution 2.21\n\n\nWe have the three eigenvalues \\(\\lambda_{1}=\\lambda_{2}=1\\) and \\(\\lambda_{3}=2\\). The important idea here is to find the geometric multiplicity of \\(1\\) as an eigenvalue. We have \\[\\begin{align*}\n(x,y,z)\\in E(1,S) & \\Leftrightarrow S((x,y,z))=(x,y,z) \\\\\n& \\Leftrightarrow (3x-2z,\\:y,\\:x)=(x,y,z) \\\\\n& \\Leftrightarrow x=z.\n\\end{align*}\\] Hence eigenvectors of \\(S\\) corresponding to the eigenvalue \\(1\\) take the form \\((t,s,t)=t(1,0,1)+s(0,1,0)\\) for nonzero \\(s,t\\in\\mathbb{C}\\). It follows that a basis of \\(E(1,S)\\) is \\(((1,0,1),\\:(0,1,0))\\) and so the geometric multiplicity of \\(1\\) as an eigenvalue of \\(S\\) is equal to its algebraic multiplicity. Hence \\(S\\) is diagonalisable. Now \\[\\begin{align*}\n(x,y,z)\\in E(2,S) & \\Leftrightarrow S((x,y,z))=2(x,y,z) \\\\\n& \\Leftrightarrow (3x-2z,\\:y,\\:x)=(2x,\\:2y,\\:2z) \\\\\n& \\Leftrightarrow x=2z,\\:y=0.\n\\end{align*}\\] Hence eigenvectors of \\(S\\) corresponding to the eigenvalue \\(1\\) take the form \\((2t,0,t)=t(2,0,1)\\) for nonzero \\(t\\in\\mathbb{C}\\) and so \\(((2,0,1))\\) is a basis of the eigenspace \\(E(2,S)\\). Take \\(B=((1,0,1),\\:(0,1,0),\\:(2,0,1))\\). Then \\(B\\) is a basis of \\(\\mathbb{C}^{3}\\) and \\(M(S;B)={\\rm diag}(1,1,2)\\)."
  },
  {
    "objectID": "03-Week3.html#sec-Introduction",
    "href": "03-Week3.html#sec-Introduction",
    "title": "3  Ring Theory Fundamentals",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nRings are extremely important algebraic structures. A wide array of well known systems can be identified as rings. Our goal will be to study the structure of rings in the abstract setting in order to obtain some powerful results. The term ‘ring’ was first coined in the early 20th Century and the study of rings as algebraic structures is still a relatively modern concept. As always, we begin with a definition.\n\nDefinition 3.1 (Ring) A ring is a triple consisting of a non-empty set \\(R\\) equipped with two binary operations that we denote + and \\(\\cdot\\), such that the following are satisfied.\n\n(R1)\n\nFor all \\(a, b \\in R\\), \\(a+b \\in R\\) (closure under +).\n\n(R2)\n\nFor all \\(a, b, c \\in R\\), \\((a+b)+c=a+(b+c)\\) (+ is associative on \\(R\\)).\n\n(R3)\n\nThere exists \\(0_R\\) such that, \\(\\forall \\, a \\in R, a+0_R=0_R+a=a\\) (+ has an identity).\n\n(R4)\n\nFor all \\(a \\in R, \\, \\exists \\, -a \\in R\\) such that \\(a+(-a)=(-a)+a=0_R\\) (inverses under +).\n\n(R5)\n\nFor all \\(a, b \\in R\\), \\(a+b=b+a\\) (+ is commutative on \\(R\\)).\n\n(R6)\n\nFor all \\(a, b \\in R\\), \\(a\\cdot b \\in R\\) (closure under \\(\\cdot\\)).\n\n(R7)\n\nFor all \\(a, b, c \\in R\\), \\((a\\cdot b)\\cdot c=a\\cdot(b\\cdot c)\\) (\\(\\cdot\\) is associative on \\(R\\)).\n\n(R8)\n\nFor all \\(a, b, c \\in R\\), \\(a\\cdot(b+c)=a\\cdot b+a\\cdot c\\) and \\((a+b)\\cdot c=a\\cdot c+b\\cdot c\\) (distributive laws).\n\n\n\nNote that we will usually write \\(ab\\) for \\(a \\cdot b\\).\n\nCommutative Rings\nFrom the axioms we see that, while the addition operation is commutative in any ring, we do not require a ring to have a commutative multiplicative operation. In the case where a ring \\(R\\) has \\(ab=ba\\) for all \\(a,b\\in R\\) we say that \\(R\\) is commutative. In this course we will mostly be dealing with commutative rings. The ring of integers \\(\\Z\\) is an example of a commutative ring, as is \\(\\zn\\). Note that, in a ring, elements do not need to have multiplicative inverses (unlike groups).\n\nDefinition 3.2 (Identity Elements) An identity element in a ring \\(R\\) is a multiplicative identity. Hence, an identity element \\(1_{R}\\in R\\) is such that \\(a1_{R}=1_{R}a=a\\) for all \\(a\\in R\\).\n\nIn this module we will assume that all rings have multiplicative identities. A ring with an identity element is sometimes referred to as a ring with a one.\n\n\nZero Ring\nThe zero ring is the ring \\(\\{0_{R}\\}\\) consisting of just one element \\(0_{R}\\). It should be noted that in this ring, \\(1_{R}=0_{R}\\).\n\nDefinition 3.3 (Unit) Let \\(R\\) be a ring and \\(a \\in R\\). Then \\(a\\) is said to be a unit in \\(R\\) if it has a multiplicative inverse in \\(R\\), that is there exists \\(a^{-1} \\in R\\) such that \\(aa^{-1}=a^{-1}a=1_R\\).\n\nWe denote and define the set of units of a ring \\(R\\) as \\[R^{\\times} = \\{a \\in R \\where \\exists \\, b \\in R \\; \\text{such that} \\; ab=1_R\\}.\\]\n\nExample 3.1  \n\n\nIf \\(R\\) is the ring of integers, then \\(R^{\\times} = \\{-1,1\\}\\).\nThe set of units of a ring forms a group.\n\n\n\n\n\nDivision Rings\nA division ring is a nonzero ring in which every nonzero element has a multiplicative inverse. Note that we do not demand that a division ring is commutative. If a division ring is commutative then it is a field. The familiar systems \\(\\mathbb{Q}\\), \\(\\mathbb{R}\\) and \\(\\mathbb{C}\\) are all fields. Note that \\(\\mathbb{Z}\\) is not a field as the only units in \\(\\mathbb{Z}\\) are the elements \\(\\pm{1}\\).\n\n\nMatrix Rings\nLet \\(R\\) be a ring. Then the set of all \\(n\\times n\\) matrices with entries from \\(R\\) is a ring under the usual matrix addition and multiplication. This ring is denoted \\(M_{n}(R)\\). Observe that \\(M_{n}(R)\\) is not a commutative ring (unless \\(n=1\\) and \\(R\\) itself is commutative).\n\nExample 3.2  \n\nIn first year Algebra we considered only matrices with entries from \\(\\R\\). For such matrices, having a non-zero determinant is equivalent to being invertible. That is, for \\(A \\in M_{n}(\\R)\\), \\(\\det(A) \\ne 0\\) if and only if there is some \\(B \\in M_{n}(\\R)\\) such that \\(AB = BA = I_{n}\\).\nHowever, having a non-zero determinant is not enough for invertibility in \\(M_{n}(R)\\) for a general ring \\(\\R\\). As an example take \\(\\R = \\Z_{4}\\) and consider the element \\(A \\in M_{2}(\\Z_{4})\\) below \\[A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}.\\] Then \\(\\det(A) = 2 \\ne 0 \\in \\Z_{4}\\) however, we claim that \\(A\\) does not have an inverse in \\(\\Z_{4}\\). Let us demonstrate this claim.\nSuppose \\(A\\) has an inverse \\(B \\in M_{n}(\\Z_{4})\\). Suppose \\[B = \\begin{pmatrix} a & b \\\\ c & d.\\end{pmatrix}\\] Then, \\[AB = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} a & b \\\\ 2c & 2d \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\\] It follows that \\(a = 1\\), \\(b=0\\), \\(c =0\\) and \\(2d = 1\\). However, there is no element \\(d \\in \\Z_{4}\\) such that \\(2d = 1\\).\nIf on the other hand we considered \\(A\\) as an element of \\(M_{2}(\\Z_{5})\\), then we can solve the equation \\(2d = 1\\). Since \\(3\\) is the unique element of \\(\\Z_{5}\\) satisfying \\(2 \\times 3 =1\\). Therefore \\(A\\) is in fact invertible as an element of \\(M_{2}(\\Z_{5})\\).\nThe difference is that every non-zero element of \\(\\Z_{5}\\) is a unit in \\(\\Z_{5}\\) since \\(\\Z_{5}\\) is a field (\\(\\Z_{n}\\) is a field when \\(n\\) is prime).\nIn fact what is true is that an element \\(A \\in M_{n}(R)\\), for \\(R\\) a commutative ring, is invertible if and only if its determinant is a unit in the ring.\n\n\n\n\nPolynomial Rings\nLet \\(R\\) be a commutative ring and let \\(X\\) be an indeterminate. We define the ring of polynomials \\(R[X]\\) to be the set of all polynomials in \\(X\\) with coefficients from the ring \\(R\\). Hence \\[ R[X]=\\Bigg\\{\\sum_{i=0}^{n} r_{i}X^{i} \\where r_{i}\\in R,\\:\\:n\\geq 0\\Bigg\\}. \\] Addition and multiplication are defined in the intuitive way. Hence if \\(f=a_{0}+a_{1}X+...+a_{m}X^{m}\\), \\(g=b_{0}+b_{1}X+...+b_{n}X^{n}\\in R[X]\\) with \\(n>m\\) then we have \\[\\begin{equation*}\nf+g=(a_{0}+b_{0})+(a_{1}+b_{1})X+...+(a_{m}+b_{m})X^{m}+b_{m+1}X^{m+1}+...+b_{n}X^{n}\n\\end{equation*}\\] and \\[\\begin{equation*}\nfg=a_{0}b_{0}+(a_{0}b_{1}+a_{1}b_{0})X+(a_{0}b_{2}+a_{1}b_{1}+a_{2}b_{0})X^{2}+...+a_{m}b_{n}X^{m+n}.\n\\end{equation*}\\]\nContrast the situation with that of \\(P_n\\) forming a vector space, where \\(P_n\\) is the set of all polynomials of maximum degree \\(n\\). Note that in the case of polynomials of some maximum degree, the set remains closed under the operations of addition and scalar multiplication in the vector space. However, in rings we have a multiplication operation, so we cannot have rings of polynomials of some fixed degree as the set would not remain closed under multiplication.\nAt this point it is worth recalling the factor theorem for polynomials over a field. Let \\(\\F[X]\\) be the set of all polynomials over the field \\(\\F\\) and let \\(f(x) \\in \\F[X]\\). Then \\(f(x)\\) has a factor \\((x-k)\\), say, if and only if \\(f(k)=0\\) (that is, \\(k\\) is a root).\n\nExample 3.3  \n\nRecall that for a ring\\(R\\), \\(R^{\\times}\\) is the set of units (or invertible elements) of \\(R\\).\nIf \\(R\\) is a field then \\(R[x]^{\\times} = R^{\\times}\\). However this is not the case for an arbitrary ring \\(R\\).\nFor example consider the ring \\(\\Z_{4}\\). The polynomials \\((1+2x),(1-2x) \\in \\Z_{4}[x]\\) satisfy: \\[(1+ 2x)(1-2x) = 1 + (2-2)x - 4x^2 = 1.\\] So the units of \\(\\Z_{4}[x]\\) is a strictly larger set than the units of \\(\\Z_{4}\\).\n\n\n\n\nQuadratic Integer Rings\nLet \\(d\\) be a square-free integer. Then the set denoted and defined by \\[\\Z[\\sqrt{d}] = \\{a+b\\sqrt{d} \\where a, b \\in \\Z\\}\\] is a ring.\nIn the specific case in which \\(d=-1\\), the ring \\(\\Z[i]\\) is called the ring of Gaussian integers.\nWe could show that \\(\\Z[\\sqrt{d}]\\) is a ring by checking all of the axioms, but we shall provide a much more slick, alternative proof a little later.\n\nExample 3.4  \n\nLet \\(x = a + b \\sqrt{d} \\in \\Z[\\sqrt{d}]\\). Define \\(\\bar{x} = a - b \\sqrt{d}\\). Note that when \\(d\\) is a negative number, then \\(x\\) and \\(\\bar{x}\\) are both complex numbers and \\(\\bar{x}\\) is precisely the complex conjugate of \\(x\\). If \\(d\\) is positive, then \\(x\\) is real and so is its own complex conjugate. In particular for \\(x\\) real \\(\\bar{x}\\) is not ( in general) the complex conjugate of \\(x\\). However, \\(\\bar{x}\\) generalises some properties of the complex conjugate.\nLet \\(x \\in \\Z{\\sqrt{d}}\\), and write \\(N(x)\\) for the product \\(x\\bar{x} = a^2 - bd^2 \\in \\Z\\). We call \\(N(x)\\) the norm of \\(x\\). (This can be thought of as generalising the modulus function for complex numbers \\(|z|^2 = z\\bar{z}\\).)\nWe can use the norm to characterise the units of \\(\\Z[\\sqrt{d}]\\).\nIf \\(x\\) is a unit then there is some \\(y \\in \\Z[\\sqrt{d}]\\) such that \\(xy = 1\\). Then \\(N(xy) = 1\\). However, \\(N(xy) = N(x)N(y)\\) and so \\(N(x)N(y) = 1\\). This means that either \\(N(x) = N(y) = 1\\) or \\(N(x) = N(y) = -1\\). Therefore if \\(x\\) is a unit of \\(\\Z[\\sqrt{d}]\\), then \\(N(x) = \\pm 1\\).\nOn the other hand if \\(N(x) = \\pm 1\\), then observe that \\(N(x)\\bar{x}x = N(x)^2 = 1\\) and \\(x\\) is a units.\n\n\n\nDefinition 3.4 (Division in a ring) Let \\(R\\) be a commutative ring and let \\(a,b\\in R\\). We say that \\(a\\) divides \\(b\\) if \\(b=ta\\) for some \\(t\\in R\\). We will often use the notation \\(a \\divides b\\) for this.\n\n\nExample 3.5  \n\n\nIn \\(\\Z\\) we have that \\(3|21\\) and \\(-7|42\\).\nIn \\(\\R[x]\\), \\((x^3 + 3)|(x^5 + 2x^3 - x^2 -3x -3)\\) since \\[x^5 + 2x^3 - x^2 -3x -3 = (x^3 -x -1)(x^2 + 3).\\]\nIn the ring of Gaussian integers, we have \\(-i | (2-3i)\\) since \\(2-3i = (-i)(3+2i)\\).\n\n\n\n\nDefinition 3.5 (Prime element) Let \\(R\\) be a commutative ring. A nonzero element \\(p \\in R\\) is a prime element if, for all \\(x, y \\in R\\), \\[p \\divides xy \\imp p\\divides x \\; \\text{or} \\; p \\divides y.\\]\n\n\nExample 3.6  \n\n\nIn \\(\\Z\\), the prime elements are precisely \\(\\pm p\\) where \\(p\\) is a prime number.\nAny polynomial of degree 1 is prime in \\(\\R[x]\\). Consider \\(ax - b \\in \\R[x]\\) for \\(a,b \\in \\R\\) , \\(a \\ne 0\\). This has a unique root \\(b/a\\). Let \\(f(x),g(x) \\in \\R[x]\\) and suppose \\((ax-b)| f(x)g(x)\\). Then \\(f(x)g(x) = (ax-b)h(x)\\) for some \\(h(x) \\in \\R[x]\\). However, this now means that \\(f(b/a)g(b/a) = 0\\) meaning that \\(f(b/a) =0\\) or \\(g(b/a) = 0\\). We conclude that \\((ax-b)|f(x)\\) or \\((ax-b)|g(x)\\). Therefore \\((ax-b)\\) is prime in \\(\\R[x]\\).\nIn the ring \\(\\Z[\\sqrt{-5}]\\), \\(3\\) is not prime. For example \\(9 = (2 + \\sqrt{-5})(2-\\sqrt{-5})\\) but \\(3\\) does not divide \\(2 + \\sqrt{-5}\\) or \\(2 - \\sqrt{-5}\\). For instance if \\(2 + \\sqrt{-5} = 3 (a + b \\sqrt{-5})\\) for some \\(a,b \\in \\Z\\), then \\((2-3a) + (1-3b)\\sqrt{-5} = 0\\). This means that \\(2 = 3a\\) and \\(1 = 3b\\). However there are no integers \\(a,b\\) satisfying these equations. A similar argument shows that \\(3 \\not\\mid (2 - \\sqrt{-5})\\).\n\n\n\n\nDefinition 3.6 (Irreducible element) Let \\(R\\) be a commutative ring. A nonzero element \\(x\\in R\\) is called irreducible if \\(x\\) is not a unit in \\(R\\) and, for \\(y,z\\in R\\), \\(x=yz\\Rightarrow\\) \\(y\\) is a unit or \\(z\\) is a unit.\n\n\nExample 3.7  \n\n\nIn \\(\\Z\\), the primes and their negatives are irreducible elements.\nIn \\(\\R[x]\\) a quadratic is irreducible if it has no roots in \\(\\R\\). Consider the quadratic \\(2x^2 +3\\) in the ring of polynomials over \\(\\Q\\). Suppose \\(2x^2 +3 = f(x)g(x)\\) for some \\(f(x), g(x) \\in \\Q[x]\\). Since \\(2x^2 +3\\) cannot be factorised in \\(\\Q[x]\\), one of \\(f(x)\\) or \\(g(x)\\) must be a constant polynomial and so a unit.\nWe saw that \\(3\\) is not prime in \\(\\Z[\\sqrt{-5}]\\). However it is irreducible. Suppose \\(3 = (a+b\\sqrt{-5})(c+d\\sqrt{-5})\\). Then, \\[ 9=|3|^2 = |(a+b\\sqrt{-5})(c+d\\sqrt{-5})| =  (a^2 + 5b^2)(c^2 + 5d^2).\\] Notice that there are no integers \\(a,b \\in \\Z\\) such that \\(a^2 + 5b^2 = 3\\). Therefore one of \\((a^2 +5b^2)\\) or \\((c^2 + 5d^2)\\) must be equal to \\(1\\). Without loss of generality we may assume that \\((a^2 + 5b^2) =1\\). This is only possible if \\(b=0\\) and \\(a = \\pm 1\\). Thus \\(a + b\\sqrt{5} = a\\) is a unit.\n\n\n\n\nDefinition 3.7 (Divisor of zero) Let \\(R\\) be a ring and let \\(x \\in R\\) be nonzero. Then \\(x\\) is a divisor of zero in \\(R\\) if there exists \\(y \\in R\\backslash\\{0_R\\}\\) such that \\(xy=0_R\\).\n\n\nExample 3.8  \n\nIn \\(M_2(\\R)\\), the element \\(\\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\\) is a zero divisor since \\[\\begin{pmatrix} 0  & 1 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} 0  & x \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0  & 0 \\\\ 0 & 0 \\end{pmatrix}.\\]\nTechnically, Definition 3.7 above is of a left zero divisor. There is an analogous definition for a right zero divisor. In the above, \\(\\begin{pmatrix} 0 & x \\\\ 0 & 0 \\end{pmatrix}\\) is a right zero divisor.\n\n\n\nDefinition 3.8 (Integral Domain) An integral domain is a nonzero commutative ring in which there are no divisors of zero. Hence in an integral domain \\(R\\), for all \\(x,y\\in R\\) we have \\[\\begin{equation*}\nxy=0_{R}\\imp x=0_{R}\\:\\:{\\rm{or}}\\:\\: y=0_{R}.\n\\end{equation*}\\]\n\n\nLemma 3.1 Let \\(R\\) be an integral domain and let \\(a,b,c\\in R\\) with \\(a\\neq 0_{R}\\). Then \\[\\begin{equation*}\nab=ac\\imp b=c.\n\\end{equation*}\\]\n\n\n\nProof. \nWe have \\[ ab -ac = a(b-c) = 0.\\] Since \\(R\\) is an integral domain, and \\(a \\ne 0\\), then \\(b-c = 0\\) and \\(b=c\\) as required.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf \\(R\\) is an integral domain, then \\(R[x]\\) is also an integral domain.\n\\(\\Z_n\\) is an integral domain if and only if \\(n\\) is prime.\nThe ring \\(M_{n}(\\R)\\) is not an integral domain.\n\n\n\n\n\nLemma 3.2 Let \\(R\\) be a ring. For all \\(x\\in R\\), \\[\\:\\:x0_{R}=0_{R}x=0_{R}.\\]\n\n\n\nProof. \n\\[x0_R = x(0_R + 0_R) = x0_R + x0_R.\\] Subtracting \\(x0_R\\) from both sides: \\[ 0_R = x0_R.\\]\nIn a similar way, it can be shown that \\(0_Rx = 0_R\\).\n\n\n\nLemma 3.3 Every field is an integral domain.\n\n\n\nProof. \nLet \\(\\F\\) be a field and \\(x\\) be a non-zero element of \\(\\F\\). Suppose there is a \\(y \\in \\F\\) such that \\(xy = 0_{\\F}\\). Then \\[y = 1y =  (x^{-1}x) y = x^{-1}0_{\\F} = 0_{\\F}.\\] Thus if \\(xy = 0_{\\F}\\) for \\(x,y \\in \\F\\) either \\(x = 0_{\\F}\\) or \\(y = 0_{\\F}\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nEvery finite integral domain is a field.\n\n\n\n\nThe content that follows is non-examinable.\n\n\nDefinition 3.9 (Characteristic) Let \\(R\\) be an integral domain. The characteristic of \\(R\\) (denoted \\(\\chr(R)\\)) is the period of \\(1_{R}\\) in the group \\((R,+)\\).\nIf \\(1_{R}\\) has infinite period in \\((R,+)\\) then we define \\(\\chr(R)=0\\).\n\n\nTheorem 3.1 Let \\(R\\) be an integral domain. Then \\(\\chr(R)\\) is either \\(0\\) or a prime.\n\n\nProof. Let \\(m \\in \\N\\) be period of \\(1_{R}\\) in \\((R,+)\\). Then, \\(m 1_{R} = 0_{R}\\). If \\(m\\) is not prime, then there are elements \\(k,l \\in \\N\\) such that \\(k,l <m\\) and \\(m1_{R} = (kl)1_{R} = k1_{R} l1_{R} = 0_{R}\\). As \\(R\\) is an integral domain either \\(k1_{R} = 0_{R}\\) or \\(l1_{R} = 0_{R}\\). In either case we get a contradiction as \\(m\\) is the smallest positive integer satisfying \\(m1_{R} = 0_{R}\\). Therefore, \\(m\\) must be prime.\n\n\n\n\n\n\n\nNote\n\n\n\n\nEvery field of characteristic zero has a copy of \\(\\Q\\) inside it.\nEvery field of characteristic \\(p\\) has a copy of \\(\\zp\\) inside it.\n\n\n\n\nEnd of non-examinable content.\n\n\n\nLemma 3.4 If \\(R\\) is an integral domain, then all prime elements are irreducible.\n\n\n\nProof. \nLet \\(p\\) be a prime element of \\(R\\) and suppose \\(p = xy\\) for some \\(x\\) and \\(y\\) in \\(R\\).\nSince \\(p|p\\) then \\(p |(xy)\\). As \\(p\\) is prime it follows that either \\(p|x\\) or \\(p|y\\). Without loss of generality we may assume that \\(p|x\\). Then there is some \\(z \\in R\\) such that \\(x = pz\\). It follows that \\(p = pzy\\). By Lemma 3.1 it follows that \\(1= zy\\) and \\(y\\) is a unit. Therefore \\(p\\) is irreducible.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll prime elements of an integral domain are irreducible but not all irreducible elements of an integral domain are prime. For example we have seen that \\(3\\) is not prime in \\(\\Z[\\sqrt{-5}]\\) but it is irreducible in \\(\\Z[\\sqrt{-5}]\\).\n\n\n\nExample 3.9  \n\n\nAs \\(\\Q\\) is an integral domain, then \\(\\Q[x]\\) is also an integral domain. We have seen that all linear polynomials are prime, it follows that all linear polynomials are irreducible in \\(\\Q[x]\\).\n19 is prime in \\(\\Z\\) therefore it is irreducible in \\(\\Z\\). We can verify this, for if \\(19 = xy\\) for integers \\(x,y\\), then either \\(x\\) or \\(y\\) is equal to 1."
  },
  {
    "objectID": "03-Week3.html#sec-Subrings",
    "href": "03-Week3.html#sec-Subrings",
    "title": "3  Ring Theory Fundamentals",
    "section": "3.2 Subrings",
    "text": "3.2 Subrings\nA subset \\(S\\) of a ring \\(R\\) is called a subring if \\(S\\) is itself a ring under the addition and multiplication given in \\(R\\).\n\n\n\n\n\n\nNote\n\n\n\nIn any ring \\(R\\), \\(\\{0_{R}\\}\\) and \\(R\\) are subrings of \\(R\\).\n\n\n\nLemma 3.5 Let \\(R\\) be a ring and let \\(S \\subseteq R\\). Then \\(S\\) is a subring of \\(R\\) if and only if the three conditions below are satisfied:\n\n\\(S\\) has an identity element;\n\\(a, b \\in S \\, \\imp \\, a-b \\in S\\);\n\\(a, b \\in S \\, \\imp \\, ab \\in S\\)\n\n\n\nProof. The proof that follows is non-examinable.\nLet \\(R\\) be a ring and let \\(S\\) be a subset of \\(R\\).\n\n\\((\\imp)\\)\n\nFirst we show that if \\(S\\) is a subring of \\(R\\) then the three conditions hold.\n\nAs \\(S\\) is a subring then \\(1_S \\in S\\). By the definition of a subring \\(1_S = 1_R\\), so \\(1_R \\in S\\).\nNow let \\(a, b \\in S\\). Since \\(S\\) is a ring it must be closed under addition. Further there is an additive inverse for \\(b\\) in \\(S\\) (R4) and since additive inverses are unique in \\(R\\) this inverse is the same as \\(-b\\) in \\(R\\). Hence \\(a-b = a+(-b) \\in S\\).\nSimilarly, \\(S\\) is closed under multiplication.\n\n\n\\((\\limp)\\)\n\nNext we show that if \\(S\\) satisfies the three enumerated conditions then it is a subring of \\(R\\), that is, \\(S\\) is a ring in itself with the operations on \\(S\\) being defined as the operations on \\(R\\).\n\nNon-empty:\n\nBy i. \\(S\\) is non-empty.\n\n(R3):\n\nSince \\(S\\) is non-empty, let \\(a \\in S\\). By ii., \\(a-a \\in S\\) and so \\(0_R \\in S\\). We claim that \\(0_R\\) is an additive identity for \\(S\\). For any \\(s \\in S\\), we have \\(0_R+s=s\\), since \\(s \\in R\\) and since \\(0_R\\) is the additive identity in \\(R\\). Thus \\(0_R\\) is an additive identity for \\(S\\).\n\n(R4):\n\nLet \\(a \\in S\\). Then, by ii. we have \\(0_S-a \\in S\\). But \\(0_S-a=0_S+(-a)\\), where \\(-a\\) is the additive inverse of \\(a\\) in \\(R\\). So \\(-a \\in S\\) and \\(-a\\) is an additive inverse for \\(a\\) in \\(S\\).\n\n(R1):\n\nLet \\(a, b \\in S\\). Then \\(-b \\in S\\) and \\(-(-b)=b\\), since if \\(-b\\) is an additive inverse for \\(b\\), then \\(b\\) is an additive inverse for \\(-b\\). Thus, \\(a+b=a-(-b) \\in S\\), by ii., so \\(S\\) is closed under addition.\n\n(R5):\n\nLet \\(a, b \\in S\\). Then \\(a+b \\in S\\) as \\(S\\) is closed under addition and, since \\(a, b \\in R\\), we have \\(a+b=b+a\\) by commutativity of addition in \\(R\\).\n\n(R2):\n\nLet \\(a, b, c \\in S\\). Then, since \\(a, b, c \\in R\\), we have that \\(a+(b+c)=(a+b)+c\\) and closure ensures that these sums are in \\(S\\).\n\nMultiplicative identity:\n\nThis is assumed by i.\n\n(R7):\n\nLet \\(a, b, c \\in S\\). Since \\(a, b, c \\in R\\) then \\(a(bc)=(ab)c\\) in \\(R\\) and since \\(S\\) is closed under multiplication then \\(a(bc)=(ab)c\\) in \\(S\\).\n\n(R6):\n\nThis follows from iii..\n\n(R8):\n\nSimilarly to (R2), this follows as (R8) holds in \\(R\\) and \\(S\\) is a subset of \\(R\\).\n\n\n\n\n\n\nExample 3.10  \n\nThe ring \\(\\Z\\) is a subring of \\(\\Q\\) which is a subring of \\(\\R\\) which is a subring of \\(\\C\\).\nConsider the ring \\(\\Z[i]= \\{a + ib : a,b \\in \\Z\\}\\), \\(i^2 = -1\\).\n\n\\(\\Z[i]\\) has the identity element \\(1= 1 + 0i\\).\nLet \\(x_1 = a_1 + b_1 i\\) and \\(x_2 = a_2 + b_2 i\\) be elements of \\(\\Z[i]\\). Then: \\[\\begin{eqnarray*}\nx_1 -x_2 &=& (a_1 + b_1i) - (a_2 + b_2i)\\\\\n        &=& (a_1 -a_2) + (b_1-b_2)i \\in \\Z[i].\n  \\end{eqnarray*}\\]\n\\[\\begin{eqnarray*}\n   x_1x_2 &=& (a_1 + b_1i)(a_2 + b_2i)\\\\\n         &=& (a_1a_2 - b_1b_2) + (a_1b_2 +a_2 b_1)i \\in \\Z[i]\n\\end{eqnarray*}\\]\n\nSo \\(\\Z[i]\\) is a subring of \\(\\C\\).\n\n\n\nExample 3.11 Let \\(R\\) be a ring and let \\(a \\in R\\). Show that the subset \\[C(a) = \\{x \\in R \\where xa=ax\\}\\] is a subring of \\(R\\).\n\nThe set \\(C(a)\\) is non-empty as it contains the identity element \\(1\\) of \\(R\\).\nLet \\(x,y \\in C(a)\\)\n\\[(x-y)a = xa - ya = ax - ay = a(x-y).\\] Therefore \\(x-y \\in C(R)\\). Furthermore \\[(xy)a = xay = a xy\\] and \\(xy \\in C(a)\\).\nIt follows that \\(C(a)\\) is a subring of \\(R\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen \\(R\\) has an identity element \\(1_{R}\\) it does not always follow that \\(1_{R}=1_{S}\\). An important example is the subring \\(S=\\{0_{R}\\}\\) which is a subring of every ring. Note that in this case \\(1_{S}=0_{R}\\).\n\n\n\nExample 3.12  \n\n\nLet \\(R = \\Z_{10}\\). The multiplicative identity if \\(1\\). Set \\(S = \\{0,2,4,6,8\\}\\) is a subring of \\(\\Z_{10}\\) with multiplicative identity \\(6\\) as: \\[\\begin{eqnarray*}\n  0 \\times 6 &=& 0 \\\\\n  2 \\times 6 &=& 2 \\\\\n  4 \\times 6 &=& 4 \\\\\n  6 \\times 6 &=& 6 \\\\\n  8 \\times 6 &=& 8.\n\\end{eqnarray*}\\]\nConsider \\(R = M_{2}(\\R)\\) and take \\[S = \\left\\{ \\begin{pmatrix} x & 0 \\\\ 0 & 0 \\end{pmatrix} \\where x \\in \\R  \\right\\}\\] Then \\(S\\) us a subring of \\(R\\). However, the identity \\(I_S\\) of \\(S\\) is not equal to the identity \\(I_{R}\\) of \\(R\\). \\[I_{S} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\ne \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_R.\\]"
  },
  {
    "objectID": "03-Week3.html#sec-RingHomomorphisms",
    "href": "03-Week3.html#sec-RingHomomorphisms",
    "title": "3  Ring Theory Fundamentals",
    "section": "3.3 Ring Homomorphisms",
    "text": "3.3 Ring Homomorphisms\n\nDefinition 3.10 Let \\(R\\) and \\(S\\) be rings and let \\(\\theta : R\\longrightarrow S\\) be a mapping. We say that \\(\\theta\\) is a ring homomorphism if, for all \\(a,b\\in R\\), \\[\\begin{equation*}\n\\theta(a+b)=\\theta(a)+\\theta(b)\n\\end{equation*}\\] and \\[\\begin{equation*}\n\\theta(ab)=\\theta(a)\\theta(b).\n\\end{equation*}\\]\n\n\nLemma 3.6 Let \\(R\\) and \\(S\\) be rings and let \\(\\theta : R\\longrightarrow S\\) be a ring homomorphism. Then\n\n\\(\\theta(0_{R})=0_{S}\\)\n\\(\\theta(-x)=-\\theta(x)\\) for all \\(x\\in R\\).\n\n\n\n\nProof. \n\nObserve that \\[\\theta(0_{R}) = \\theta(0_R + 0_R) = \\theta(0_R) + \\theta(0_R).\\] Subtracting \\(\\theta(0_R)\\) from both sides then gives the result: \\(\\theta(0_R) = 0_{S}\\).\nWe have \\[\\begin{eqnarray*}\n   0_S &=& \\theta(0_R) = \\theta(x + (-x)) \\\\\n   &=& \\theta(x) + \\theta(-x).\n  \\end{eqnarray*}\\] Subtracting \\(\\theta(x)\\) from both sides: \\[-\\theta(x) = \\theta(-x)\\] as required.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA bijective ring homomorphism is a ring isomorphism.\n\n\n\nExample 3.13 (Examples of ring homomorphisms)  \n\n\nThe map \\(\\theta: \\Z \\to \\Z_{2}\\) defined by \\(\\theta(a) = \\begin{cases} 0\\text{ if } a \\text{ is even,} \\\\ 1\\text{ if } a \\text{ is odd}\\end{cases}\\) is a ring homomorphism.\n\nThe map is additive since the sum of two odd or even integers is even and the sum of an odd and even integer is odd.\nThe map is multiplicative since the product of two odd integers if odd and the product of an even integer with an even or odd integer is even.\n\nLet \\(\\theta: \\Q[x] \\to \\Q\\) be defined by \\(\\theta(p(x)) = p(0)\\). This is a ring homomorphism since,\n\nthe constant term of the sum of two polynomials is the sum of their constant terms, and,\nthe constant term of the product of two polynomials is the product of their constant terms.\n\n\n\n\n\nDefinition 3.11 (Kernel) Let \\(R\\) and \\(S\\) be rings and let \\(\\theta : R\\longrightarrow S\\) be a ring homomorphism. The kernel of \\(\\theta\\) is the set \\[ \\krn(\\theta)=\\{x\\in R \\where \\theta(x)=0_{S}\\} \\subseteq R.\\]\n\n\nLemma 3.7 Let \\(\\theta : R\\longrightarrow S\\) be a ring homomorphism. Then \\(\\theta\\) is injective if and only if \\(\\krn(\\theta)=\\{0_{R}\\}\\).\n\n\n\nProof. \nIf \\(\\theta\\) is injective, then \\(\\krn(\\theta) = \\{0_{R}\\}\\). This follows as \\(\\theta(0_R) = 0_S\\) and injectivity means that \\(\\theta(x) = 0_S\\) if and only if \\(x = 0_R\\).\nNow suppose \\(\\krn(\\theta) = \\{0_{R}\\}\\). Let \\(x,y \\in \\R\\) and suppose that \\(\\theta(x) = \\theta(y)\\). This means that \\[\\theta(x-y) = \\theta(x)- \\theta(y) = 0_S.\\] Therefore \\((x-y) \\in \\krn(\\theta)\\) and so \\(x-y = 0_R\\). Adding \\(y\\) to both sides now gives \\(x = y\\).\n\n\n\nDefinition 3.12 (Image) Let \\(\\theta : R\\longrightarrow S\\) be a ring homomorphism. The image of \\(R\\) is the set \\[\\begin{equation*}\n\\im(\\theta)=\\{\\theta(x) \\where x\\in R\\} \\subseteq S.\n\\end{equation*}\\]\n\n\nLemma 3.8 Let \\(\\theta : R\\longrightarrow S\\) be a ring homomorphism. Then \\(\\im(\\theta)\\) is a subring of \\(S\\).\n\n\n\nProof. \nNotice that \\(\\im(\\theta)\\) is non-empty since it contains \\(0_S\\).\nFor any \\(x \\in R\\), \\[\\theta(1_R) \\theta(x) = \\theta(1_Rx) = \\theta(x) = \\theta(x1_R) = \\theta(x)\\theta(1_R).\\] It follows that \\(\\theta(1_R)\\) is the identity of \\(\\im(\\theta)\\).\nLet \\(x,y \\in R\\). Then \\[\\theta(x) - \\theta(y) = \\theta(x-y) \\in \\im(\\theta)\\] and \\[\\theta(x)\\theta(y) = \\theta(xy) \\in \\im(\\theta).\\] It follows that \\(\\im(\\theta)\\) is a subring of \\(S\\).\n\n\n\nDefinition 3.13 We define the following:\n\nAn injective ring homomorphism is called a monomorphism.\nA surjective ring homomorphism is called an epimorphism.\nA bijective ring homomorphism is called an isomorphism\n\n\nWhen there is an isomorphism \\(\\theta : R\\longrightarrow S\\) of rings we say that the rings \\(R\\) and \\(S\\) are isomorphic and we write \\(R\\cong S\\) to represent this.\nWhen two rings are isomorphic we are basically looking at two copies of the same ring. (structurally speaking)\n\nDefinition 3.14 (Automorphism) Let \\(R\\) be a ring. A ring automorphism is a ring isomorphism from \\(R\\) to \\(R\\). The set of all ring automorphisms of \\(R\\) is a group called the automorphism group of \\(R\\). This group is denoted \\(\\aut(R)\\)."
  },
  {
    "objectID": "03-Week3.html#sec-Ideals",
    "href": "03-Week3.html#sec-Ideals",
    "title": "3  Ring Theory Fundamentals",
    "section": "3.4 Ideals",
    "text": "3.4 Ideals\nWe now turn our attention to a special type of subset of a ring.\n\nDefinition 3.15 (Ideal) Let \\(R\\) be a ring and let \\(I\\subseteq R\\) be a subgroup of the abelian group \\((R,+)\\). We say that I is an ideal of \\(R\\) if, for all \\(x\\in I\\) and for all \\(r\\in R\\), the element \\(xr\\in I\\) and also the element \\(rx\\in I\\). When \\(I\\) is an ideal of \\(R\\) we use the notation \\(I\\ideal R\\) to denote this.\n\n\n\n\n\n\n\nNote\n\n\n\nIn fact what we have just defined is a two-sided ideal. A subgroup \\(I\\) of \\((R, +)\\) satisfying \\(xr \\in I \\; \\forall\\, x \\in I, r \\in R\\) is called a right ideal of \\(R\\). Similarly, a subgroup \\(I\\) of \\((R, +)\\) satisfying \\(rx \\in I \\; \\forall\\, x \\in I, r \\in R\\) is called a left ideal of \\(R\\). In commutative ring theory any one-sided ideal is automatically two-sided. Note also that as \\(I\\) is a subgroup of the additive group \\((R,+)\\) we have \\(0_{R}\\in I\\).\n\n\n\nExample 3.14  \n\n\nIn any ring \\(R\\), the subsets \\(R\\) and \\(\\{0_R\\}\\) are ideals of \\(R\\).\nIn the ring \\(\\Z\\), the subset \\(2\\Z = \\{ 2x : x \\in \\Z\\}\\) of even integers is an ideal of \\(\\Z\\).\n\n\n\n\nLemma 3.9 Let \\(R\\) be a ring and let \\(I\\subseteq R\\). Then \\(I\\) is an ideal of \\(R\\) if and only if the following conditions hold :\n\n\\(I\\neq\\phi\\);\n\\(a,b\\in I\\Rightarrow a-b\\in I\\);\n\\(x\\in I\\) and \\(r\\in R\\Rightarrow xr\\) and \\(rx\\) both belong to \\(I\\).\n\n\n\nProof. This is left as an exercise.\n\n\n\n\n\n\n\nNote\n\n\n\nThe first condition is different from the first condition in the test for a subring. Obviously \\((I, +)\\) is a subgroup of \\((R, +)\\) and so \\(0_R \\in I\\). We use this in examples to establish property i.\n\n\n\nDefinition 3.16 Let \\(R\\) be a ring and let \\(a\\in R\\). We define the subset \\[\\begin{equation*}\naR=\\{ar \\where r\\in R\\}.\n\\end{equation*}\\]\n\n\nLemma 3.10 Let \\(R\\) be a commutative ring. For each \\(a\\in R\\) the subset \\(aR\\) is an ideal of \\(R\\) and \\(aR\\) is the smallest ideal of \\(R\\) to which \\(a\\) belongs.\n\n\n\nProof. \nClearly \\(aR\\) is non-empty since it contains \\(0_{R}\\) and \\(a = a1_{R}\\).\nLet \\(x,y \\in aR\\). There are \\(u,v \\in R\\) such that \\(x = au\\) and \\(y=av\\). Then \\[x-y = au - av = a(u-v) \\in aR.\\]\nLet \\(au \\in aR\\) and \\(r \\in R\\), then \\(aur = rau \\in aR\\).\nLet \\(I\\) be any ideal of \\(R\\) containing \\(a\\). Then \\(ax \\in I\\) for any \\(x \\in R\\) and so \\(aR \\subseteq I\\). Thus \\(aR\\) is the smallest ideal of \\(R\\) containing \\(a\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is essential that \\(R\\) is commutative here to get a two-sided ideal. If \\(R\\) is not commutative, \\(aR\\) is just a right ideal of \\(R\\) in general.\n\n\n\nDefinition 3.17 (Principal Ideal) For a commutative ring, \\(R\\), the ideal \\(aR\\) is called the principal ideal of \\(R\\) generated by the element \\(a\\).\n\n\nExample 3.15  \n\n\nFor \\(n \\in \\Z\\), \\(n\\Z\\) is a principal ideal of \\(\\Z\\), it is the set of all multiples of \\(n\\). It can be shown that every ideal of \\(Z\\) if of the form \\(n\\Z\\) for some \\(n \\in \\Z\\).\nConsider the principal ideal \\(I=(x-1)\\R[x]\\) of \\(\\R[x]\\). If \\(f(x) \\in I\\), then \\(f(x)\\) is of the from \\((x-1)g(x)\\) for some \\(g(x) \\in \\R[x]\\). In particular \\(f(1) = 0\\).\nOn the other hand, if \\(f(x) \\in \\R[x]\\) satisfies \\(f(1) = 0\\), then \\(f(x) = (x-1)g(x)\\) for some \\(g(x) \\in \\R[x]\\) and \\(f(x) \\in I\\).\n\n\n\n\nDefinition 3.18 Let \\(R\\) be a ring and let \\(I\\ideal R\\).\n\nWe say that \\(I\\) is a non-trivial ideal if \\(I\\neq\\zring\\).\nWe say that \\(I\\) is a proper ideal if \\(I\\neq R\\).\n\n\n\nLemma 3.11 Let \\(R\\) be a ring and let \\(I\\ideal R\\). If \\(I\\) contains a unit then \\(I=R\\).\n\n\n\nProof. \nNotice that if \\(I\\) contains \\(1_R\\) then \\(I =R\\). Therefore it suffices to show that \\(1_{R} \\in I\\). Let \\(x \\in I\\) be a unit. Then \\(x^{-1}x = 1_{R} \\in I\\). This concludes the proof.\n\n\nIn particular, note that if an ideal contains \\(1_{R}\\) then \\(I=R\\).\n\nTheorem 3.2 Let \\(R\\) be a nonzero, commutative ring. Then \\(R\\) is a field if and only if the only ideals of \\(R\\) are \\(\\{0_{R}\\}\\) and \\(R\\).\n\n\n\nProof. \nSuppose \\(R\\) is a field. Let \\(I\\) be any ideal of \\(R\\). If \\(I\\) contains an element \\(x \\ne 0_{R}\\), then \\(I\\) contains \\(1_{R} xx^{-1}\\). Therefore \\(I = R\\).\nSuppose the only ideals of \\(R\\) are \\(\\{0_R\\}\\) and \\(R\\). Since \\(R\\) is a commutative ring, if we can show that every non-zero element of \\(R\\) has a multiplicative inverse, then we can conclude that \\(R\\) is a field.\nLet \\(x \\in R\\) be a non-zero element. The principal ideal \\(xR\\) must be equal to \\(R\\) by assumption (since \\(xR \\ne \\{0_{R}\\}\\) as it contains \\(x\\)). Therefore \\(xR\\) contains \\(1_{R}\\) and there is an element \\(x^{-1} \\in R\\) such that \\(xx^{-1} = x^{-1}x = R\\). Thus, \\(x\\) has a multiplicative inverse in \\(R\\).\n\n\n\nExample 3.16 Let \\[ R=\\Bigg\\{\\left(\\begin{array}{cc} x & y \\\\ 0 & z\\end{array}\\right) \\where x,y,z\\in\\mathbb{R}\\Bigg\\} \\] and \\[ I=\\Bigg\\{\\left(\\begin{array}{cc} x & y \\\\ 0 & 0\\end{array}\\right) \\where x,y\\in\\mathbb{R}\\Bigg\\}. \\] Show that \\(I\\) is an ideal of \\(R\\).\n\nWe use Lemma 3.9.\nWe note that \\(I \\ne \\emptyset\\) since \\(I\\) contains the \\(0\\) matrix.\nLet \\(A= \\begin{pmatrix} x_1 & y_1 \\\\ 0 & 0\\end{pmatrix}\\) and \\(B=\\begin{pmatrix} x_2 & y_2 \\\\ 0 & 0\\end{pmatrix}\\) be elements of \\(I\\) and \\(C = \\begin{pmatrix} x & y \\\\ 0 & z\\end{pmatrix}\\) be an element of \\(R\\). Then \\[A-B = \\begin{pmatrix} x_1 -x_2 & y_1-y_2 \\\\ 0 & 0\\end{pmatrix} \\in I,\\] \\[AC = \\begin{pmatrix} x_1x & x_1y + y_1 z \\\\  0 & 0 \\end{pmatrix} \\in I\\] and, \\[CA = \\begin{pmatrix} xx_1 & xy_1 \\\\ 0 & 0\\end{pmatrix} \\in I.\\]\nIt follows that \\(I\\) is an ideal of \\(R\\).\nLet \\(D = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0\\end{pmatrix} \\in R\\). Observe that\n\\[DR:= \\left\\{ \\begin{pmatrix} 1 & 0 \\\\ 0 & 0\\end{pmatrix}\\begin{pmatrix} x & y \\\\ 0 & z\\end{pmatrix} \\mid x,y,z \\in \\R\\right\\} = \\left\\{ \\begin{pmatrix} x & y \\\\ 0 & 0\\end{pmatrix} \\mid x,y \\in R \\right\\} = I.\\]\nThus one might be tempted to conclude based on this that \\(I\\) is an ideal. However, \\(R\\) is not a commutative ring, and so the fact that \\(I = DR\\) is enough only to show that \\(I\\) is a right ideal of \\(R\\).\n\n\n\nLemma 3.12 Let \\(I\\) and \\(J\\) be ideals of a ring \\(R\\). Then \\(I \\cap J\\) is also an ideal of \\(R\\) and is the largest ideal contained in both \\(I\\) and \\(J\\).\n\n\n\nProof. \nNotice that if \\(K\\) is any ideal contained in both \\(I\\) and \\(J\\), then \\(K\\) is contained in \\(I \\cap J\\). Therefore if \\(I \\cap J\\) is an ideal, it is the largest ideal contained in both \\(I\\) and \\(J\\). We show below that \\(I \\cap J\\) is indeed and ideal of \\(R\\).\n\\(I\\) and \\(J\\) both contain \\(0_R\\) and so \\(0_{R} \\in I \\cap J\\).\nLet \\(x,y \\in I \\cap J\\) and \\(r \\in R\\). Then \\[ x-y \\in I \\cap J\\] since \\(x-y \\in I\\) and \\(x-y \\in J\\), and, \\[rx,xr \\in I \\cap j\\] since \\(rx,xr \\in I\\) and \\(rx,xr \\in J\\). It follows that \\(I \\cap J\\) is an ideal of \\(R\\).\n\n\n\nLemma 3.13 Let \\(R\\) be a ring and let \\(I\\) and \\(J\\) be ideals of \\(R\\). Then \\[\\begin{equation*}\nI+J = \\{a+b \\where a\\in I,\\:\\:b\\in J\\}\n\\end{equation*}\\] is also an ideal of \\(R\\) which contains \\(I\\) and \\(J\\). Further, \\(I+J\\) is the smallest ideal of \\(R\\) to contain both \\(I\\) and \\(J\\).\n\n\n\nProof. \nNotice that \\(I = \\{ i + 0_{R} : i \\in I\\} \\subseteq I+J\\) since \\(0_{R} \\in J\\). Likewise \\(J \\subseteq I+J\\).\nWe note that if \\(I+J\\) is an ideal, then it has to be the smallest ideal of \\(R\\) containing both \\(I\\) and \\(J\\). For, if \\(K\\) is any ideal of \\(R\\) containing both \\(I\\) and \\(J\\), then \\(K\\) must contain \\(I+J=\\{i+j: i \\in I, j \\in j\\}\\).\nWe now show that \\(I+J\\) is an ideal of \\(R\\).\nLet \\(i_1 + j_1, i_2 + j_2 \\in I+J\\), where \\(i_1,i_2 \\in I\\) and \\(j_1, j_2 \\in J\\), and let \\(r \\in R\\). Then: \\[\\begin{eqnarray*}\n(i_1 + j_1) - (i_2 + j_2) &=& (i_1 - i_2)(j_2 -j_2) \\in I+J, \\\\\nr (i_1 + j_1) &=& ri_1 + rj_1 \\in I+j \\text{ and } \\\\\n(i_1 + j_1)r &=& i_1 r = j_1 r \\in I + J.\n\\end{eqnarray*}\\]\n\n\n\nLemma 3.14 Let \\(\\theta : R\\longrightarrow S\\) be a ring homomorphism. Then \\(\\krn(\\theta)\\) is an ideal of \\(R\\).\n\n\n\nProof. \nNote that \\(\\krn(\\theta)\\) is non-empty since it contains \\(0_R\\).\nLet \\(x,y \\in \\krn(\\theta)\\) and \\(r \\in R\\). Then: \\[\\begin{eqnarray*}\n\\theta(x-y) &=& \\theta(x) - \\theta(y) = 0_S - 0_S = 0_S,\\\\\n\\theta(rx)  &=& \\theta(r) \\theta(x) = \\theta(r) 0_S = 0_S \\text{ and } \\\\\n\\theta(xr)  &=& \\theta(x)\\theta(r)  = 0_S \\theta(r)  = 0_S\n\\end{eqnarray*}\\]\nIt follows that \\(x-y\\), \\(rx\\) and \\(rx\\) are elements of \\(\\krn(\\theta)\\). Therefore \\(\\krn(\\theta)\\) is an ideal."
  },
  {
    "objectID": "03-Week3.html#sec-FactorRings",
    "href": "03-Week3.html#sec-FactorRings",
    "title": "3  Ring Theory Fundamentals",
    "section": "3.5 Factor Rings",
    "text": "3.5 Factor Rings\n\nDefinition 3.19 (Coset) Let \\(R\\) be a ring and let \\(I\\) be an ideal of \\(R\\). Let \\(a\\in R\\). The coset of \\(I\\) in \\(R\\) determined by \\(a\\) is the set \\[\\begin{equation*}\na+I=\\{a+b \\where b\\in I\\}.\n\\end{equation*}\\]\n\nSo, \\(a + I\\) is a subset of \\(R\\); it consists of all those elements of \\(R\\) that differ from \\(a\\) by an element of \\(I\\). Note that \\(a + I\\) does not generally have algebraic structure in its own right, it is typically not closed under the addition or multiplication of \\(R\\).\n\nExample 3.17  \n\nConsider the ring \\(\\Z\\) and the ideal \\(I = 5\\Z\\). The distinct cosets of \\(5\\Z\\) are \\[5\\Z, 1+5\\Z, 2+ 5\\Z, 3 + 5\\Z, \\text{ and, } 4 + 5\\Z.\\] Note that \\(7,12 \\in 2 + 5\\Z\\), but \\(7+12 = 19\\) is an element of \\(4 + 5\\Z\\) and not \\(2+ 5\\Z\\). Cosets are not necessarily closed under addition. Similarly, \\(8,13 \\in 3 + 5\\Z\\), but \\(8x13 \\in 4 + 5\\Z\\) and not \\(3 + 5\\Z\\). Cosets are not necessarily closed under multiplication.\nConsider the ring \\(\\Q[x]\\). Let \\(I\\) be the ideal \\((x^2 -1)\\Q[x]\\). Then \\((x-1) \\not \\in I\\) and \\((x-1) + I\\) is a non-trivial coset of \\(I\\). Similarly \\((x+1) \\not\\in I\\) and so \\((x+1)+I\\) is another non-trivial coset of \\(I\\). Notice that \\((x-1)(x+1) = x^2 -1 \\in I\\).\n\n\n\nLemma 3.15 Let \\(a\\) and \\(b\\) be elements of a ring \\(R\\) in which \\(I\\) is a two-sided ideal.\n\nIf \\(a-b \\in I\\), then \\(a+I = b+I\\).\nIf \\(a-b \\notin I\\), the cosets \\(a + I\\) and \\(b + I\\) are disjoint subsets of \\(R\\).\n\n\n\n\nProof. \n\nSuppose \\(a-b \\in I\\). Then there is an element \\(i \\in I\\) such that \\(a = i+b\\) and \\(b = a-i\\). Let \\(j \\in I\\). Then \\[\\begin{eqnarray*}\na + j &=& i + b +j = b + (i+j) \\in b+I \\\\\nb + j &=& a - i + j = a + (j-i) \\in a+I.\n  \\end{eqnarray*}\\] Therefore \\(a+I = b+I\\).\nSuppose \\(a-b \\not\\in I\\). Suppose \\((a+I) \\cap (b+I) \\ne \\emptyset\\). Then there are \\(i_1,i_2 \\in I\\) such that \\(a+ i_1 = b+i_2\\). Rearranging: \\[a-b = (i_2 -i_1) \\in I\\] which is a contradiction. Therefore \\(a+I \\cap b+I = \\emptyset\\).`\n\n\n\nLemma 3.15 shows that the different cosets of \\(I\\) in \\(R\\) are disjoint subsets of \\(R\\). Note that their union is all of \\(R\\) since every element \\(a\\) of \\(R\\) belongs to some coset of \\(I\\) in \\(R\\) (\\(a \\in a+I\\)). The set of all cosets of \\(I\\) in \\(R\\) is denoted by \\(R/I\\). Hence \\[\\begin{equation*}\nR\\diagup I=\\{a+I \\where a\\in R\\}.\n\\end{equation*}\\] It transpires that we can define addition and multiplication in \\(R/I\\). However, we do have to ensure that the coset sum and the coset product do not depend on the choice of representative of the respective cosets. That is the subject of the following lemma.\n\nLemma 3.16 Let \\(R\\) be a ring and \\(I\\ideal R\\). Then \\[\\begin{equation*}\n(a+I)+(b+I)=(a+b)+I\n\\end{equation*}\\] and \\[\\begin{equation*}\n(a+I)(b+I)=ab+I\n\\end{equation*}\\] for all \\(a,b\\in R\\).\n\n\n\nProof. \nFirst we observe that \\((a+b) + I\\) is a subset of \\((a+I) + (b+I)\\). Similarly, \\(ab+I\\) is a subset of \\((a+I)(b+I)\\). We now show that \\((a+b)+I \\subseteq (a+I) + (b+I)\\) and \\(ab+I \\subseteq (a+I)(b+I)\\).\nLet \\(i_1, i_2 \\in I\\) and consider: \\[(a+ i_1) + (b+ i_2) = (a+b) + (i_2 + i_2) \\in (a+b)+ I\\] and \\[(a+i_1)(b+i_2) = ab + ai_2 + i_1b + i_1 i_1 =  ab + (ai_2 + i_1b + i_1 i_1) \\in ab + I\\] since \\(ai_1, i_1b \\in I\\) as \\(I\\) is an ideal.\nThus, \\((a+b)+I \\subseteq (a+I) + (b+I)\\) and \\(ab+I \\subseteq (a+I)(b+I)\\). We conclude that \\((a+b)+I = (a+I) + (b+I)\\) and \\(ab+I = (a+I)(b+I)\\)\nNow if \\(a',b' \\in R\\) are such that \\(a + I = a' + I\\), and \\(b + I = b' + I\\), then \\[(a' + I) + (b'+I)  = (a+I) + (b+I) = (a+b) +I\\] and \\[(a'+I)(b'+I)  = (a+I)(b+I) = (a+b) +I.\\] However, since \\(a' \\in a'+I\\) and \\(b' \\in b'+I\\), it follows that \\((a'+b') \\in (a+b) +I\\). Therefore by Lemma 3.15 \\((a'+b') + I = (a+b)+I\\). Similarly, \\(a'b' \\in (a'+I)(b'+I)\\) and so \\(a'b' \\in ab +I\\). It follows by Lemma 3.15 that \\(a'b' + I = ab +I\\).\nThus, coset sum and coset product are well-defined binary operations on \\(R/I\\), the result is independent of the choice of coset representative.\n\n\n\nTheorem 3.3 The set \\(R/I\\), when endowed with the addition and multiplication defined above, is a ring.\n\n\nProof. That coset sum and coset product are closed follows from the definition. Thus to show that \\(R/I\\) is a ring, we only need verify the other ring axioms. We leave this as an exercise noting that the remaining axioms all follow from the fact that \\(R\\) is a ring. For example to see that coset sum is associative:\nLet \\(a+I, b+I, c+I \\in R/I\\), we have: \\[\\begin{eqnarray*}\n((a+I)+(b+I))+(c+I) &=& ((a+b)+I)+ (c+I) \\\\\n                    &=& (a+b+c)+I \\\\\n                    &=& (a + (b+c)) + I \\\\\n                    &=& (a+I) + ((b+c)+I) \\\\\n                    &=& (a+I) + ((b+I) + (c+I))\n\\end{eqnarray*}\\]\n\n\nDefinition 3.20 (Factor ring) Under the addition and multiplication given above, we define the resulting ring \\(R/I\\) to be the factor ring of \\(R\\) modulo \\(I\\).\n\n\nTheorem 3.4 (First Isomorphism Theorem) Let \\(\\theta : R\\longrightarrow S\\) be a ring homomorphism. Then \\[\\begin{equation*}\nR/\\krn(\\theta)\\cong\\im(\\theta).\n\\end{equation*}\\]\n\n\n\nProof. \nSet \\(K= \\ker(\\theta)\\).\nLet \\(\\phi: R/K \\to \\im(\\theta)\\) be defined by \\(\\phi(r+K) = \\theta(r)\\).\nFirst we show that \\(\\phi\\) is well-defined.\nLet \\(r,r' \\in R\\) be such that \\(r+K = r' +K\\). Then, using Lemma 3.15, there is a \\(k \\in K\\) such that \\(r' = r+k\\). It follows that \\[\\phi(r') = \\theta(r+k) = \\theta(r) + \\theta(k) = \\theta(r) + 0_{S} = \\theta(r).\\] Thus \\(\\phi\\) is well-defined.\nNow we show that \\(\\phi\\) is an isomorphism.\nIt is clear that \\(\\phi\\) is surjective. To see that \\(\\phi\\) is injective, let \\(r+K, r'+K \\in R/K\\) and suppose that \\(\\phi(r+K) = \\phi(r'+K)\\). Then \\[\\theta(r) = \\theta(r')\\] and so \\[\\theta(r-r') = \\theta(r) - \\theta(r') = 0_{S}.\\] It follows that \\(r-r' \\in K\\). Thus, by Lemma 3.15 \\(r+k r' +k\\). Thus \\(\\phi\\) is injective.\nTo see that \\(\\phi\\) is a homomorphism, let \\(r+K, r'+K \\in R/K\\), then \\[\\begin{eqnarray*}\n\\phi((r+K) + (r'+k)) &=& \\phi((r+r')+K) \\\\\n                     &=& \\theta(r + r') \\\\\n                     &=& \\theta(r) + \\theta(r') \\\\\n                     &=& \\phi(r+K) + \\phi(r'+k)\n\\end{eqnarray*}\\] and \\[\\begin{eqnarray*}\n\\phi((r+K) (r'+k)) &=& \\phi((rr')+K) \\\\\n                     &=& \\theta(rr') \\\\\n                     &=& \\theta(r) \\theta(r') \\\\\n                     &=& \\phi(r+K)\\phi(r'+k).\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "03-Week3.html#sec-sheet3",
    "href": "03-Week3.html#sec-sheet3",
    "title": "3  Ring Theory Fundamentals",
    "section": "3.6 Problem Sheet 3",
    "text": "3.6 Problem Sheet 3\nQuestions 3.31 – 3.7 for Week 8; Questions 3.8 – 3.11 for Week 10. \n\n\n\n\nQuestion 3.1\n\n\nLet \\(R\\) be a ring. The centre of \\(R\\) is defined as \\[\\begin{equation*}\nZ(R)=\\{a\\in R : ab=ba\\:\\:\\:\\:{\\rm{for\\:\\: all}}\\:\\:\\:\\: b\\in R\\}.\n\\end{equation*}\\] Prove that \\(Z(R)\\) is a subring of \\(R\\).\n\n\n\n\n\nShow Solution 3.1\n\n\n\n\nSolution 3.1\n\n\nWe have \\[\\begin{equation*}\n1_{R}b=b1_{R}=b\n\\end{equation*}\\] for all \\(b\\in R\\) and so \\(1_{R}\\in Z(R)\\) and \\(Z(R)\\neq\\phi\\).\nNow let \\(x,y\\in Z(R)\\). Then \\(xa=ax\\) and \\(ya=ay\\) for all \\(a\\in R\\). Hence \\[\\begin{equation*}\n(x-y)a=xa-ya=ax-ay=a(x-y)\n\\end{equation*}\\] and so \\(x-y\\in Z(R)\\).\nFinally we have \\[\\begin{equation*}\n(xy)a=x(ya)=x(ay)=(xa)y=(ax)y=a(xy)\n\\end{equation*}\\] and so \\(xy\\in Z(R)\\) and \\(Z(R)\\) is a subring of \\(R\\).\n\n\n\n\n\n\n\nQuestion 3.2\n\n\nLet \\(R\\) be a ring. Prove that if \\(a\\in R\\) is a unit then its inverse is unique.\n\n\n\n\n\nShow Solution 3.2\n\n\n\n\nSolution 3.2\n\n\nLet \\(a\\in R\\) be a unit and assume that \\(b,c\\in R\\) are both inverses of \\(a\\). Then \\(ab=ba=1_{R}\\) and \\(ac=ca=1_{R}\\). Hence \\[\\begin{equation*}\nb=b1_{R}=b(ac)=(ba)c=1_{R}c=c.\n\\end{equation*}\\]\n\n\n\n\n\n\n\nQuestion 3.3\n\n\nA ring \\(R\\) is called Boolean if every element of \\(R\\) is idempotent (i.e. for all \\(x\\in R\\), \\(x^{2}=x\\))\n\nShow that if \\(R\\) is Boolean then any nonzero element \\(x\\in R\\) has period two in the group \\((R,+)\\).\nShow that any Boolean ring is necessarily commutative.\n\n\n\n\n\n\nShow Solution 3.3\n\n\n\n\nSolution 3.3\n\n\n\nWe have \\[\\begin{align*}\n2x = x+x & = (x+x)^{2} \\\\\n& = x^{2}+2x+x^{2} \\\\\n& = x + 2x + x \\\\\n& = 4x.\n\\end{align*}\\] Hence \\(2x=0\\) i.e. \\(x+x=0\\) and \\(x\\) has order 2 in \\((R,+)\\).\nLet \\(x,y\\in R\\). We have \\[\\begin{align*}\nx+y=(x+y)^{2} & = x^{2}+xy+yx+y^{2} \\\\\n& = x+xy+yx+y.\n\\end{align*}\\] Hence \\(xy+yx=0\\Rightarrow xy=-yx=yx\\) by part (a). It follows that \\(R\\) is commutative.\n\n\n\n\n\n\n\n\nQuestion 3.4\n\n\nLet \\[\\begin{equation*}\nS=\\Bigg\\{\\left(\\begin{array}{cc} x & 0 \\\\ 0 & 0\\end{array}\\right) : x\\in\\mathbb{R}\\Bigg\\}.\n\\end{equation*}\\] Prove that \\(S\\) is a subring of \\(M_{2}(\\mathbb{R})\\).\n[Note that the \\(S\\) and \\(M_{2}(\\mathbb{R})\\) each have identity elements but they are not the same]\n\n\n\n\n\nShow Solution 3.4\n\n\n\n\nSolution 3.4\n\n\nThe element \\(\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right)\\) acts as an identity element in \\(S\\) as, for all \\(\\left(\\begin{array}{cc} x & 0 \\\\ 0 & 0\\end{array}\\right)\\in S\\) we have \\[\\begin{equation*}\n\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right)\\left(\\begin{array}{cc} x & 0 \\\\ 0 & 0\\end{array}\\right)=\\left(\\begin{array}{cc} x & 0 \\\\ 0 & 0\\end{array}\\right)\\left(\\begin{array}{cc} 1 & 0 \\\\ 0 & 0\\end{array}\\right)=\\left(\\begin{array}{cc} x & 0 \\\\ 0 & 0\\end{array}\\right).\n\\end{equation*}\\] Now let \\(A=\\left(\\begin{array}{cc} x & 0 \\\\ 0 & 0\\end{array}\\right),B=\\left(\\begin{array}{cc} y & 0 \\\\ 0 & 0\\end{array}\\right)\\in S\\). Then \\[\\begin{equation*}\nA-B=\\left(\\begin{array}{cc} x-y & 0 \\\\ 0 & 0\\end{array}\\right)\\in S\n\\end{equation*}\\] and \\[\\begin{equation*}\nAB=\\left(\\begin{array}{cc} xy & 0 \\\\ 0 & 0\\end{array}\\right)\\in S.\n\\end{equation*}\\] Hence \\(S\\) is a subring of \\(M_{2}(\\mathbb{R})\\).\n\n\n\n\n\n\n\nQuestion 3.5\n\n\nLet \\(R\\) be a ring. Show that, for a positive integer \\(n\\), the ring \\(M_{n}(R)\\) of \\(n\\times n\\) matrices over \\(R\\) is commutative if and only if \\(R=\\{0\\}\\), or \\(n=1\\) and \\(R\\) is commutative.\n\n\n\n\n\nShow Solution 3.5\n\n\n\n\nSolution 3.5\n\n\n\\(M_{n}(\\{0\\})\\) is clearly commutative and \\(M_{1}(R)\\) is essentially the same as \\(R\\) so is commutative if \\(R\\) is commutative.\nNow assume that \\(M_{n}(R)\\) is commutative and that \\(R\\neq\\{0\\}\\). Let \\(E_{ik}\\) be the \\(n\\times n\\) matrix whose entries are all zero except for the \\((i,k)\\)th entry which is \\(1\\). Then \\(E_{nn}=E_{n1}E_{1n}=E_{1n}E_{n1}=E_{11}\\). So \\(n=1\\) as \\(M_{1}(R)\\) is essentially the same as \\(R\\), \\(R\\) must be commutative.\n\n\n\n\n\n\n\nQuestion 3.6\n\n\nLet \\(D\\) be an integral domain.\n\nShow that, for all \\(x\\in D\\), \\[\\begin{equation*}\nx^{2}=1\\Rightarrow x=1\\:\\:{\\rm{or}}\\:\\:x=-1.\n\\end{equation*}\\]\nDeduce that if \\(D\\) contains only finitely many units then the product of these units equals \\(-1\\).\nFinally show that, for every prime integer \\(p\\), \\[\\begin{equation*}\n(p-1)!\\equiv -1 \\:\\:({\\rm{mod}}\\: p).\n\\end{equation*}\\] This is Wilson’s Theorem.\n\n\n\n\n\n\nShow Solution 3.6\n\n\n\n\nSolution 3.6\n\n\n\n\\(x^{2}=1\\Rightarrow x^{2}-1=0\\Rightarrow (x-1)(x+1)=0\\). Now \\(D\\) is an integral domain and hence we must have \\(x-1=0\\) or \\(x+1=0\\) i.e. \\(x=1\\) or \\(x=-1\\).\nWe note that part a. gives us the result that, if an element in \\(D\\) is self-inverse then it is \\(\\pm{1}\\). Hence all other units in \\(D\\) must come in distinct pairs \\((a_{i},a_{i}^{-1})\\) (\\(i=1,2,...,t\\)) say with \\(a_{i}\\neq a_{i}^{-1}\\). (Recall that if \\(a_{i}\\) is a unit in \\(D\\) then \\(a_{i}^{-1}\\) is also a unit in \\(D\\) with \\((a_{i}^{-1})^{-1}=a_{i}\\)) Now the finite product of the units in \\(D\\) is \\[\\begin{equation*}\n1\\times(-1)\\times\\prod_{i=1,2,...,t} a_{i}a_{i}^{-1}=-1\\times 1=-1.\n\\end{equation*}\\]\nFor the final part, take \\(D=\\mathbb{Z}_{p}\\). (set of congruence classes mod \\(p\\)) Apply the the result of part (b)\nThen \\(\\mathbb{Z}_{p}\\) is a field and hence every nonzero element is a unit. These are the elements \\(\\{1,2,...,p-1\\}\\). Hence the product of these is \\(-1\\) in \\(\\mathbb{Z}_{p}\\). i.e. \\[\\begin{equation*}\n(p-1)\\times (p-2)\\times ...\\times 2\\times 1\\equiv -1 ({\\rm{mod}}\\:\\:p)\\Rightarrow (p-1)!\\equiv -1 ({\\rm{mod}}\\:\\: p)\n\\end{equation*}\\]\n\n\n\n\n\n\n\n\nQuestion 3.7\n\n\nLet \\(R\\) be a commutative ring. Show that \\[\\begin{equation*}\nZ(M_{n}(R))=\\{aI_{n} : a\\in R\\}.\n\\end{equation*}\\] [For the trickier \\(\\subseteq\\) containment, it might be useful to consider the matrix unit which is the matrix \\(E_{ik}\\in M_{n}(R)\\) where the \\((i,k)\\)th entry of \\(E_{ik}\\) is the integer \\(1\\) and all other entries are zero. Then any \\(A=(a_{ik})\\in M_{n}(R)\\) can be written as \\(A=\\sum_{i,k=1}^{n} a_{ik}E_{ik}\\).]\n\n\n\n\n\nShow Solution 3.7\n\n\n\n\nSolution 3.7\n\n\nLet \\(X\\in M_{n}(R)\\) and \\(a\\in R\\). Then \\[\\begin{equation*}\nX(aI_{n})=a(XI_{n})=aX=a(I_{n}X)=(aI_{n})X.\n\\end{equation*}\\] So \\(aI_{n}\\in Z(M_{n}(R))\\).\nNow let \\(A\\in Z(M_{n}(R))\\). Then \\(A=\\sum_{i,k=1}^{n} a_{ik}E_{ik}\\) where \\(a_{ik}\\in R\\) and \\(E_{ik}\\) is the matrix unit. For \\(p,q=1,...,n\\) we have \\[\\begin{equation*}\nE_{qp}A=AE_{qp}\n\\end{equation*}\\] i.e. \\[\\begin{equation*}\nE_{qp}\\sum_{i,k=1}^{n} a_{ik}E_{ik}=\\sum_{i,k=1}^{n} a_{ik}E_{ik}E_{qp}\n\\end{equation*}\\] i.e. \\[\\begin{equation*}\n\\sum_{k=1}^{n} a_{pk}E_{qk}=\\sum_{i=1}^{n} a_{iq}E_{ip}.\n\\end{equation*}\\] i.e. \\(a_{pp}=a_{qq}\\), \\(a_{pk}=0 (p\\neq k)\\), \\(a_{iq}\\neq 0 (i\\neq q)\\) \\((i,k=1,2,...,n)\\).\nHence \\(A=aI_{n}\\) for some \\(a\\in R\\). Hence \\(Z(M_{n}(R))=\\{aI_{n} : a\\in R\\}\\).\n\n\n\n\n\n\n\nQuestion 3.8\n\n\nLet \\(R\\) be a commutative ring and let \\(a\\in R\\). Show that the set \\[\\begin{equation*}\nI=\\{x\\in R : xa=0\\}\n\\end{equation*}\\] is an ideal of \\(R\\). (This ideal is sometimes called the annihilator of \\(a\\in R\\))\n\n\n\n\n\nShow Solution 3.8\n\n\n\n\nSolution 3.8\n\n\nWe have \\(0_{R}a=0_{R}\\) and so \\(0_{R}\\in I\\).\nNow let \\(x,y\\in I\\). Then \\(xa=ya=0\\). Hence \\((x-y)a=xa-ya=0-0=0\\). So \\(x-y\\in I\\).\nFinally let \\(x\\in I\\) and \\(r\\in R\\). Then we have \\[\\begin{equation*}\n(rx)a=r(xa)=r0=0.\n\\end{equation*}\\] and (using the fact that \\(R\\) is commutative) \\[\\begin{equation*}\n(xr)a=(rx)a=r(xa)=r0=0.\n\\end{equation*}\\] Hence \\(rx,xr\\in I\\) and \\(I\\) is an ideal of \\(R\\).\n\n\n\n\n\n\n\nQuestion 3.9\n\n\nLet \\(R\\) be the set of all matrices of the form \\[\\begin{equation*}\n\\left(\\begin{array}{cc} a & b \\\\ c & d\\end{array}\\right)\n\\end{equation*}\\] over \\(\\Q\\) such that \\(a=d\\) and \\(b=0\\). Let \\(I\\) be the subset of \\(R\\) such that \\(a=d=0\\). Show that \\[\\begin{equation*}\nR/I\\cong\\Q.\n\\end{equation*}\\] [Hint : think about defining a homomorphism…]\n\n\n\n\n\nShow Solution 3.9\n\n\n\n\nSolution 3.9\n\n\nDefine a mapping \\[\\begin{equation*}\n\\theta : R\\longrightarrow\\mathbb{Q}\n\\end{equation*}\\] given by \\[\\begin{equation*}\n\\theta\\left(\\left(\\begin{array}{cc} a & 0 \\\\ c & a\\end{array}\\right)\\right):=a.\n\\end{equation*}\\] Then if \\(A=\\left(\\begin{array}{cc} a & 0 \\\\ c & a\\end{array}\\right)\\), \\(B=\\left(\\begin{array}{cc} x & 0 \\\\ y & x\\end{array}\\right)\\in R\\), we have \\[\\begin{equation*}\n\\theta(A+B)=\\theta\\left(\\left(\\begin{array}{cc} a+x & 0 \\\\ c+y & a+x\\end{array}\\right)\\right)=a+x=\\theta(A)+\\theta(B)\n\\end{equation*}\\] and \\[\\begin{equation*}\n\\theta(AB)=\\theta\\left(\\left(\\begin{array}{cc} ax & 0 \\\\ cx+ay & ax\\end{array}\\right)\\right)=ax=\\theta(A)\\theta(B).\n\\end{equation*}\\] It follows that \\(\\theta\\) is a ring homomorphism and \\[\\begin{align*}\n{\\rm{ker}}(\\theta) & =\\Bigg\\{A=\\left(\\begin{array}{cc} a & 0 \\\\ c & a\\end{array}\\right)\\in R : \\theta(A)=0\\Bigg\\} \\\\\n& =\\Bigg\\{A\\in R : a=0\\Bigg\\} \\\\\n& =\\Bigg\\{\\left(\\begin{array}{cc} 0 & 0 \\\\ c & 0\\end{array}\\right) : c\\in\\mathbb{Q}\\Bigg\\} \\\\\n& =I.\n\\end{align*}\\] Additionally we have im\\((\\theta)=\\mathbb{Q}\\). By the First Isomorphism Theorem for rings, \\[\\begin{equation*}\nR\\diagup I\\cong\\mathbb{Q}.\n\\end{equation*}\\]\n\n\n\n\n\n\n\nQuestion 3.10\n\n\nLet \\(R\\) be a ring and let \\(J\\unlhd R\\). Show that \\(R\\diagup J\\) is commutative if and only if \\(xy-yx\\in J\\) for all \\(x,y\\in R\\).\nDeduce that, if \\(K_{1},K_{2}\\unlhd R\\) and both \\(R\\diagup K_{1}\\) and \\(R\\diagup K_{2}\\) are commutative, then \\(R\\diagup (K_{1}\\cap K_{2})\\) is also commutative.\n\n\n\n\n\nShow Solution 3.10\n\n\n\n\nSolution 3.10\n\n\nLet \\(x+J,y+J\\in R\\diagup J\\). Then \\(R\\diagup J\\) is commutative iff \\((x+J)(y+J)=(y+J)(x+J)\\) i.e. iff \\(xy+J=yx+J\\) i.e iff \\(xy-yx\\in J\\).\nFor the second part, let \\(x,y\\in R\\). Since \\(R\\diagup K_{1}\\) is commutative, \\(xy-yx\\in K_{1}\\). Since \\(R\\diagup K_{2}\\) is commutative, we also have \\(xy-yx\\in K_{2}\\) and hence \\(xy-yx\\in K_{1}\\cap K_{2}\\). Hence, since \\(x,y\\in R\\) were arbitrary, \\(R\\diagup (K_{1}\\cap K_{2})\\) is commutative.\n\n\n\n\n\n\n\nQuestion 3.11\n\n\nLet \\(D\\) be an integral domain and let \\(0\\neq I,J\\unlhd D\\). Show that \\(I\\cap J\\neq 0\\).\n\n\n\n\n\nShow Solution 3.11\n\n\n\n\nSolution 3.11\n\n\nLet \\(0\\neq a\\in I\\) and \\(0\\neq b\\in J\\). (This is possible since each of the ideals are nonzero) Then \\(ab\\in I\\) (since \\(I\\unlhd R\\)) but also \\(ab\\in J\\). (since \\(J\\unlhd R\\)) Hence \\(ab\\in I\\cap J\\). But also, since \\(R\\) is an integral domain, we have \\(ab\\neq 0\\). Hence \\(0\\neq ab\\in I\\cap J\\) and so \\(I\\cap J\\neq 0\\)."
  },
  {
    "objectID": "04-Week4.html#sec-PrincipalIdealDomains",
    "href": "04-Week4.html#sec-PrincipalIdealDomains",
    "title": "4  Special Types of Rings and Ideals",
    "section": "4.1 Principal Ideal Domains",
    "text": "4.1 Principal Ideal Domains\n\nDefinition 4.1 (Principal Ideal Domain) An integral domain in which every ideal is principal is called a principal ideal domain, or PID for short.\n\n\nLemma 4.1 The ring \\(\\Z\\) is a PID\n\n\n\nProof. \nLet \\(I\\) be any ideal of \\(\\Z\\). Then \\((I,+)\\) is a subgroup of \\((\\Z,+)\\). Since \\((\\Z,+)\\) is a cyclic group generated by \\(1\\), and every subgroup of a cyclic group is cyclic, it follows that \\((I,+)\\) is cyclic generated by some \\(m \\in \\Z\\). Thus, \\(I= ,\\Z = \\{mk: k \\in \\Z\\}\\).\n\n\n\nExample 4.1 What about other PID’s? The following are examples of PID’s:\n\nPolynomials in one variable over a field\n\\(\\Z[\\sqrt{2}]\\) is a PID.\n\\(\\Z[i]\\) is a PID (as we will see later).\n\n\nWe now reconsider an old idea in terms of principal ideals.\n\nLemma 4.2 Let \\(R\\) be a commutative ring and let \\(a,b\\in R\\). Then \\(a \\divides b\\) if and only if \\(bR \\subseteq aR\\).\n\n\n\nProof. \n\n(\\(\\Rightarrow\\))\n\nSuppose that \\(a \\divides b\\). Then \\(b = as\\) for some \\(s \\in R\\). It follows that \\[bR = \\{ b r: r \\in R\\} = \\{asr: r\\in R\\} \\subseteq aR.\\]\n\n(\\(\\Leftarrow\\))\n\nSuppose that \\(bR\\subseteq aR\\). Now \\(b = b1_R \\in bR\\) and so \\(b \\in aR\\). This means we can find an \\(s \\in R\\) such that \\(b = as\\) as required.\n\n\n\n\n\nExample 4.2  \n\nLet \\(R=\\Z\\) and consider the principal ideal \\(3\\Z= \\{\\ldots, -9,-6,-3,0,3,6,9,\\ldots\\}\\). Now \\(6\\Z = \\{ \\ldots,-12, -6,0,6,12, \\ldots\\}\\) is clearly a subset of \\(3\\Z\\) and \\(3|6\\)."
  },
  {
    "objectID": "04-Week4.html#sec-MaximalIdeals",
    "href": "04-Week4.html#sec-MaximalIdeals",
    "title": "4  Special Types of Rings and Ideals",
    "section": "4.2 Maximal Ideals",
    "text": "4.2 Maximal Ideals\n\nDefinition 4.2 (Maximal ideal) Let \\(R\\) be a ring and let \\(I\\) be an ideal of \\(R\\). We say that \\(I\\) is a maximal ideal of \\(R\\) if \\(I\\neq R\\) and, whenever \\(J\\) is an ideal of \\(R\\) then \n\n\nExample 4.3  \n\n\nIn the ring \\(R= \\Z\\), for a prime \\(p\\), the ideal \\(p\\Z\\) is maximal. Suppose \\(p\\Z\\subseteq J\\) for an ideal \\(J\\) of \\(\\Z\\). Now as \\(\\Z\\) is a PID, \\(J = m \\Z\\) for some \\(m \\in \\Z\\). By Lemma 4.2, since \\(p\\Z \\subseteq m\\Z\\), it follows that \\(m|p\\). Thus \\(m=1\\) or \\(m=p\\). In which case \\(m\\Z = p\\Z\\) or \\(m\\Z = \\Z\\).\nConsider \\(R=\\Z_{12}\\). We claim that \\(2R\\) is a maximal ideal. Let \\(J\\) be any ideal of \\(R\\) such that \\(2R \\subseteq J\\). There are two ways to proceed, since \\((J,+)\\) is a subgroup of \\((\\Z_{12},+)\\) and \\((\\Z_{12},+)\\) is cyclic generated by \\(1\\), it follows that \\(J = mR\\) for some \\(m \\in \\Z_{12}\\). By Lemma 4.2 again, it follows that \\(m|2\\) and so \\(m=1\\) or \\(m=2\\). Thus \\(mR\\) is either \\(2R\\) or \\(R\\).\nAn alternative argument is as follows. Suppose that \\(J \\ne R\\). We will show that \\(J = 2R\\). Since \\(J \\ne R\\), then \\(J\\)t cannot contain any of the units of \\(R\\). The units of \\(R\\) are precisely, \\(1, 5,7\\) and \\(11\\) (all elements of \\(\\Z_{12}\\) co-prime to \\(12\\).) Now, if \\(J \\ne 2R\\), then \\(J\\) must contain both \\(3\\) or \\(9\\) (since \\(9\\) is the additive inverse of \\(3\\) in \\(Z_{12}\\) and so if \\(J\\) contains \\(3\\) is must also contain \\(9\\) and vice versa). However if \\(3 \\in J\\), since \\(2R \\subseteq J\\), it follows that \\(2+3 =5 \\in J\\), in which case \\(J = R\\) yielding a contradiction. Thus, it must be the case that \\(J = 2R\\) as required.\n\n\n\n\nLemma 4.3 Let \\(R\\) be a commutative ring and let \\(I\\unlhd R\\). Then \\(R\\diagup I\\) is a field if and only if \\(I\\) is a maximal ideal.\n\n\n\nProof. \n\n(\\(\\Rightarrow\\))\n\nSuppose \\(R\\diagup I\\) is a field. Let \\(J\\) be any ideal of \\(R\\) such that \\(I \\subseteq J\\). We observe that \\(J/I\\) is an ideal of \\(R/I\\). Clearly \\(J/I\\) is a subring of \\(R/I\\). Let \\(r+I \\in R/I\\) and \\(j+I \\in J/I\\). Then \\[(r+I)(j+I) = (rj)+I = (jr) +I = (j+I)(r+I) \\in J/I.\\] Therefore \\(J/I\\) is an ideal of \\(R/I\\). Since \\(R/I\\) is a field, then by Theorem 3.2, \\(J/I\\) is either equal to \\(\\{0_{R/I}\\}\\) or \\(R/I\\). If \\(J/I = \\{0_{R/I}\\}\\), then \\(J = I\\), while if \\(J/I = R/I\\) then \\(J=R\\). Therefore, as \\(J\\) was an arbitrarily chosen ideal containing \\(I\\), it follows that \\(I\\) is a maximal ideal of \\(R\\).\n\n(\\(\\Leftarrow\\))\n\nSuppose \\(I\\) is a maximal ideal of \\(R\\). We may assume that \\(R/I\\) is non-zero since otherwise we are done. Let \\(r+I\\) be any non-zero element of \\(R/I\\). It follows that \\(r \\ne I\\). Set \\(J = (rR + I)\\). Notice that \\(J\\) is an ideal of \\(R\\) (since the sum of two ideals is again an ideal) and \\(I \\subsetneq J\\) since \\(r = r1_{R} + 0_{R} \\in J\\backslash I\\). Since \\(I\\) is maximal then \\(J\\) is either \\(I\\) or \\(R\\). Thus \\(J\\) must be equal to \\(R\\) as \\(J/I \\ne \\{0_{R}+I\\}\\). This means there is an element \\(s \\in R\\) and an element \\(l \\in I\\) such that \\(rs +l = 1_{R}\\). Therefore \\(rs - 1_{R} \\in I\\). It follow then that \\[(r+I)(s+I) = (rs) + I = (sr) + I = (s+I)(r+I) = I\\] by Lemma 3.9. Therefore \\((r+I)\\) is an invertible element of \\((R/I)\\). We conclude, since \\((r+I)\\) was an arbitrarily chosen non-zero element of \\(R/I\\), that every non-zero element of \\(R/I\\) is invertible and \\(R/I\\) is a field.\n\n\n\n\n\nExample 4.4  \n\nWe have seen that all ideals of \\(\\Z\\) have the form \\(n\\Z\\) for some \\(n \\in \\Z\\). The question now arises: for which \\(n\\) is the ideal \\(n\\Z\\) maximal?\nDefine the homomorphism \\(\\theta: \\Z \\to n\\Z\\) given by \\(\\theta(a) = [a]_{n}\\). It is an exercise to verify that \\(\\theta\\) is a ring homomorphism.\nNotice that \\[\\ker(\\theta) = \\{ a \\in \\Z :  [a]_{n}= [0]_{n} \\} = \\{ a \\in \\Z: n|a\\} = \\{kn : k \\in \\Z \\} = n\\Z.\\]\nBy the First Isomorphism Theorem (Theorem 3.4) \\(R/\\ker(\\theta) \\cong \\Z_{n}\\). By Lemma 4.3 \\(\\Z_{n}\\) is a field if and only if \\(n\\Z\\) is a maximal ideal. We know that \\(\\Z_{n}\\) is a field if and only if \\(n\\) is prime. It follows that \\(n\\Z\\) is a maximal ideal if and only if \\(n\\) is prime.\n\n\n\nTheorem 4.1 If \\(R\\) is a PID, then for a non-zero, non-unit element \\(x \\in R\\), \\(xR\\) is maximal if and only if \\(x\\) is irreducible.\n\n\n\nProof. \n\n(\\(\\Rightarrow\\))\n\nSuppose \\(xR\\) is maximal and \\(x = yz\\) for some \\(y,z \\in R\\). Then by Lemma 4.2, \\(xR \\subseteq yR\\). Since \\(xR\\) is maximal, then \\(yR = R\\) or \\(yR = xR\\). If \\(yR =R\\) then \\(y\\) is a unit. If \\(yR = xR\\), then there is an \\(r \\in R\\) such that \\(y = xr\\). It follows that \\(x=yz = xrz\\). Since \\(R\\) is an integral domain, and \\(x \\ne 0_{R}\\), we conclude that \\(rz = zr =1_{R}\\) and \\(z\\) is a unit.\n\n(\\(\\Leftarrow\\))\n\nSuppose \\(x\\) is irreducible. Let \\(J\\) be any ideal such that \\(xR \\subseteq J\\). Since \\(R\\) is a PID, there is a \\(y \\in R\\) such that \\(J = yR\\). Since \\(xR \\subseteq yR\\), then \\(x = yz\\) for some \\(z \\in R\\). Since \\(x\\) is irreducible, then \\(y\\) or \\(z\\) is a unit. If \\(y\\) is a unit, we are done. If \\(z\\) is a unit. Then \\(xz^{-1} = y\\), in which case \\(yR \\subseteq xR\\) and so \\(J=yR = xR\\)."
  },
  {
    "objectID": "04-Week4.html#sec-Primeideals",
    "href": "04-Week4.html#sec-Primeideals",
    "title": "4  Special Types of Rings and Ideals",
    "section": "4.3 Prime ideals",
    "text": "4.3 Prime ideals\n\nDefinition 4.3 (Prime ideal) Let \\(R\\) be a commutative ring. An ideal \\(P\\) of \\(R\\) is a prime ideal if \\(P\\neq R\\) and \\[\nxy\\in P\\Rightarrow x\\in P\\:\\:\\text{or}\\:\\: y\\in P.\n\\]\n\n\nExample 4.5  \n\n\nConsider the \\(R= \\Z\\) and \\(p\\) a prime. Then \\(p\\Z\\) is a prime ideal. For if \\(xy \\in p\\Z\\), then \\(p|xy\\). Since \\(p\\) is prime, then \\(p|x\\) or \\(p|y\\).\nIn \\(\\Z[x]\\), then the ideal \\(I = 2\\Z[x] + x\\Z[x]\\) is a prime ideal. This is the ideal of all polynomials over \\(\\Z\\) with an even constant term.\n\n\n\n\nLemma 4.4 Let \\(R\\) be a commutative ring and let \\(I\\unlhd R\\). Then \\(R\\diagup I\\) is an integral domain if and only if \\(I\\) is a prime ideal.\n\n\n\nProof. \n\n(\\(\\Rightarrow\\))\n\nSuppose \\(R/I\\) is an integral domain. Let \\(x,y \\in R\\) be such that \\(xy \\in I\\). Then it follows that \\((x+I)(y+I) = (xy)+I = 0_{R}+I\\). Since \\(R/I\\) is an integral domain, then \\(x+I = 0_{R} +I\\) or \\(y+I = 0_{R}+I\\). Thus, \\(x \\in I\\) or \\(y \\in I\\).\n\n(\\(\\Leftarrow\\))\n\nSuppose that \\(I\\) is a prime ideal. Let \\(x+I, y+I \\in R \\diagup I\\) be such that \\((x+I)(y+I) = xy +I = 0_{R} + I\\). It follows that \\(xy \\in I\\). Since \\(I\\) is a prime ideal, \\(x \\in I\\) or \\(y \\in I\\). In which case, \\(x+I = 0_R +I\\) or \\(y+I = 0_R +I\\) and \\(R \\diagup I\\) is an integral domain.\n\n\n\n\n\nTheorem 4.2 Let \\(R\\) be a PID. Then:\n\nevery nonzero prime ideal is maximal;\nevery irreducible element is prime.\n\n\n\n\nProof. \n\nLet \\(I\\) be a nonzero prime ideal of \\(R\\). Let \\(J\\) be any ideal of \\(I\\) such that \\(I \\subseteq J\\). There are \\(x,y \\in R\\) such that \\(I= xR\\) and \\(J = yR\\). Since \\(I \\subseteq J\\) it follows that \\(x = y z\\) for some \\(z \\in R\\). Thus \\(yz \\in I\\). Since \\(I\\) is a prime ideal, then \\(y \\in I\\) or \\(z \\in I\\). If \\(y \\in I\\), then \\(yR \\subseteq xR\\) and \\(I = J\\). If \\(z \\in I\\), then \\(z = xt\\) for some \\(t \\in R\\). In which case \\(x = yxt = xyt\\). Since \\(x\\) is nonzero, (as \\(xI\\) is nonzero), and \\(R\\) is a PID, then \\(yt = 1_{R}\\) and \\(yR = R\\). It follows that any ideal which contains \\(I\\) is either equal to \\(I\\) or \\(R\\).\nLet \\(x\\) be an irreducible element of \\(R\\). Then \\(xR\\) is a maximal ideal and so \\(R/xR\\) is a field. A field is an integral domain and so \\(xR\\) is a prime ideal. Thus, if \\(x|yz\\) for some \\(y,z \\in R\\), then \\(yz \\in xR\\) and so \\(y \\in xR\\) or \\(z \\in xR\\), in other words, \\(x|y\\) or \\(x|z\\). Therefore \\(x\\) is prime.\n\n\n\n\nExample 4.6 The ring of Gaussian Integers \\(\\mathbb{Z}[i]=\\{a+ib : a,b\\in\\mathbb{Z}\\}\\) is a PID.\n\nSince \\(\\Z[i]\\) is a subring of \\(\\C\\), then \\(\\Z[i]\\) is an integral domain. Set \\(R = \\Z[i]\\) and let \\(I\\) be any nonzero ideal of \\(R\\). Let \\(z \\in I\\) be a non-zero element such that \\(|z|\\) is minimal amongst the elements of \\(I \\backslash \\{0\\}\\). We note that \\(z\\) exists since \\(I\\) is non-zero.\nWe show that \\(I = zR\\). Clearly, as \\(z \\in I\\), then \\(zI \\subseteq I\\). It remain show then that \\(I \\subseteq zI\\).\nLet \\(w \\in I\\) be any non-zero element. Consider the element \\(w/z \\in \\C\\). We can find a \\(q \\in R\\) such that \\(|w/z -q|^2 \\le 1/2\\).\n\n\n\n\n\n\n\n\n\nSet \\(r = w - qz\\), and observe that \\(r \\in I\\) since both \\(w\\) and \\(qz\\) are elements of \\(I\\). We compute \\[\\begin{eqnarray*}\n  |r|^2 = |w-qz|^2 = |z|^2|w/z -q|^2 \\le |z|^2/2.\n\\end{eqnarray*}\\]\nThus, \\(|r| < |z|\\) and so \\(r\\) must be equal to \\(0_{R}\\) (since \\(|z|\\) is smallest amongst all non-zero elements of \\(R\\)). We conclude that \\(w = qz\\) and so \\(I = zR\\)."
  },
  {
    "objectID": "04-Week4.html#sec-UniqueFactorisationDomains",
    "href": "04-Week4.html#sec-UniqueFactorisationDomains",
    "title": "4  Special Types of Rings and Ideals",
    "section": "4.4 Unique Factorisation Domains",
    "text": "4.4 Unique Factorisation Domains\nConsider in \\(\\mathbb{Z}\\) the factorisation of the integer \\(308\\) : \\[2^2 \\times 7 \\times 11\\] and \\[ 11 \\times 7 \\times 2^2.\\] Clearly these two factorisations should be regarded as the same since \\(\\mathbb{Z}\\) is commutative. We also observe that these factorisations for \\(308\\) are complete and so are superior to, say, \\(308=14\\cdot22\\).\nCausing us slightly more trouble is the notion that we can change the sign of some of the integers without altering the fact that we have a factorisation e.g. \\[(-2)^2 \\times 7 \\times 11.\\]\nWe now set ourselves the task of assessing when two factorisations of an element in an arbitrary commutative ring are ‘essentially the same’.\n\nDefinition 4.4 Let \\(R\\) be a ring and let \\(x,y\\in R\\). We say that \\(x\\) is associated to \\(y\\) if there exists a unit \\(u\\in R\\) such that \\(ux=y\\).\n\nNow let \\(r\\in R\\) be such that Then we say that these factorisations for \\(r\\in R\\) are essentially the same if\n\n\\(s=t\\), and\nthere exists a permutation \\(\\sigma\\in S_{s}\\) such that, for each \\(i\\), \\(x_{\\sigma(i)}\\) is associated to \\(y_{i}\\).\n\n\nDefinition 4.5 (Unique Factorisation Domain) Let \\(R\\) be an integral domain. We say that \\(R\\) is a unique factorisation domain (or UFD) if every nonzero non-unit of \\(R\\) can be expressed as a product of irreducibles in an essentially unique way.\n\n\nExample 4.7  \n\n\nThe ring \\(\\Z\\) is a unique factorisation domain.\nThe ring \\(\\Z[i]\\) is a unique factorisation domain.\n\n\n\n\nDefinition 4.6 (Greatest common divisor) Let \\(R\\) be a commutative ring and let \\(a,b\\in R\\). We say that an element \\(d\\in R\\) is a common divisor of \\(a\\) and \\(b\\) if \\(d\\divides a\\) and \\(d\\divides b\\).\nWe say that a common divisor \\(d\\in R\\) of \\(a\\) and \\(b\\) is a greatest common divisor of \\(a\\) and \\(b\\) if, for all \\(c\\in R\\), \\(c\\divides a\\) and \\(c\\divides b\\imp c\\divides d\\). In this case we write \\(d=\\greatcd(a, b)\\).\n\n\nGreatest common divisors in a PID\n\nLet \\(R\\) be a PID and let \\(m\\) and \\(n\\) be elements of \\(R\\). Consider the ideal \\(I = mR + nR\\). Since \\(R\\) is a PID, there is an \\(l \\in R\\) such that \\(I = lR\\). It follows that \\(l|m\\) and \\(l|n\\) since \\(m,n \\in I = lR\\). Therefore \\(l\\) is a common divisor of \\(m\\) and \\(n\\).\nSuppose \\(k\\) is an element of \\(R\\) such that \\(k|m\\) and \\(k|n\\). Then \\(mR \\subseteq kR\\) and \\(nr \\subseteq kR\\). Therefore \\(I = (mR + kR) \\subseteq kR\\). It follows that \\(k|t\\). Therefore, \\(t\\) is a greatest common divisor of \\(m\\) and \\(n\\).\n\n\nExample 4.8 Let \\(\\alpha=6+i\\), \\(\\beta=1+2i\\in\\mathbb{Z}[i]\\). Find a greatest common divisor of \\(\\alpha\\) and \\(\\beta\\) and hence find \\(\\gamma\\in\\mathbb{Z}[i]\\) such that \\(\\gamma\\mathbb{Z}[i]=\\alpha\\mathbb{Z}[i]+\\beta\\mathbb{Z}[i]\\).\n\nLet \\(\\gamma = x+iy\\), \\(x,y \\in \\Z\\) be an element of \\(\\Z[i]\\) such that \\(\\gamma \\divides\\alpha\\) and \\(\\gamma \\divides \\beta\\). It follows that \\(|\\gamma|^2\\divides |\\alpha|^2 = 37\\) and \\(|\\gamma|^2\\divides |\\beta|^2 = 5\\). Therefore \\(|\\gamma|^2 = 1\\). Since \\(x\\) and \\(y\\) are integers, either \\(x^2 = 1\\) and \\(y=0\\) or \\(y^2 = 1\\) and \\(x^2 =0\\). Since \\(\\pm 1\\) and \\(\\pm i\\) divide any element of \\(\\Z[i]\\), it follows that \\(\\{ \\pm 1, \\pm i\\}\\) is the full list of (greatest) common divisors of \\(\\alpha\\) and \\(\\beta\\). Therefore, \\(\\gamma \\in \\{ \\pm 1, \\pm i\\}\\). Notice that \\(\\gamma \\Z[i] = \\Z[i]\\) for any such choice of \\(\\gamma\\), since \\(\\pm 1\\) and \\(\\pm i\\) are units. Therefore, if \\(\\gamma\\Z[i] = \\alpha \\Z[i] + \\beta \\Z[i]\\), then \\(\\gamma\\) is a common divisor of \\(\\alpha\\) and \\(\\beta\\) and \\(\\gamma \\Z[i] = \\Z[i]\\).\nNow suppose \\(\\alpha = 4+3i\\) and \\(\\beta = 2-i\\).\nLet \\(\\gamma = x+iy in \\Z[i]\\) be such that \\(\\gamma\\) divides both \\(\\alpha\\) and \\(\\beta\\). We find that \\(|\\gamma|^2 \\divides 25\\) and \\(|\\gamma|^2 \\divides 5\\).\nif \\(|\\gamma|^2 \\divides 5\\), and \\(|\\gamma|^2 \\ne 1\\) then \\(|\\gamma|^2 = 5\\). Therefore \\(x^2 +y^2 = 5\\). The only integer solutions to this are \\(x = \\pm 2\\) and \\(y =\\pm 1\\) or \\(x = \\pm 1\\) and \\(y^2 = \\pm 2\\).\nFirst observe that since \\(i(x+iy) = -y + ix\\), then any possible common divisors \\(\\gamma\\) with \\(|\\gamma|^2 =5\\) is a product of a unit times an element of \\(\\{2 +i, 2-i\\}\\). Therefore if \\(\\alpha\\) and \\(\\beta\\) have a common divisor \\(\\gamma\\) with \\(|\\gamma|^2 =5\\), then either \\(2+i\\) or \\(2-i\\) is a common divisor of \\(\\alpha\\) and \\(\\beta\\).\nFirst we try \\(\\gamma = 2+i\\). Let \\(a,b \\in \\Z\\) and suppose \\((2+i)(a+ib) = 4 + 3i\\). Then \\((2a-b) + i(a+2b) = 4 + 3i\\). Solving these simultaneous equations, we get that \\(5a = 11\\), but there is no integer \\(a\\) satisfying this equation. We conclude that \\((2+i)\\) does not divide \\(a\\).\nNext we try \\(\\gamma = 2-i\\). Let \\(a,b \\in \\Z\\) and suppose \\((2-i)(a+ib) = 4 + 3i\\). Then \\((2a+b) + i(-a+2b) = 4 + 3i\\). Solving this system of equations, we get \\(a = 1\\) and \\(b = 2\\). Therefore \\(\\gamma \\divides a\\). Notice that \\(b = \\gamma\\) and so \\(\\gamma \\divides b\\).\nIt follows that \\(\\gamma = 2-i\\) is a common divisor of \\(a\\) and \\(b\\). Notice that \\(\\gamma = 2-i\\) must be a greatest common divisor of \\(a\\) and \\(b\\) since any other divisor is either a unit or the product of \\(\\gamma\\) and a unit.\nTherefore,an element \\(\\gamma \\in \\Z[i]\\) satisfying \\(\\gamma \\Z[i] = \\alpha \\Z[i] + \\beta\\Z[i]\\) is \\((2-i)\\)."
  },
  {
    "objectID": "04-Week4.html#sec-sheet4",
    "href": "04-Week4.html#sec-sheet4",
    "title": "4  Special Types of Rings and Ideals",
    "section": "4.5 Problem Sheet 4",
    "text": "4.5 Problem Sheet 4\nFor Week 10.\n\n\n\n\nQuestion 4.1\n\n\nLet \\(R\\) be a ring. Prove that the ideals of the ring \\(M_{n}(R)\\) are the subsets of the form \\(M_{n}(Q)\\) where \\(Q\\unlhd R\\).\n\n\n\n\n\nShow Solution 4.1\n\n\n\n\nSolution 4.1\n\n\nClearly for an ideal \\(Q\\) of \\(R\\), \\(M_{n}(Q)\\) is an ideal of \\(R\\). This follows since for \\(B=(b_{ik}) \\in M_{n}(Q)\\) and \\(A = (a_{ik})\\) in \\(R\\), \\(AB\\) and \\(BA\\) are both elements of \\(M_{n}(Q)\\). For instance for \\(1 \\le i,j \\le n\\), the \\(ik\\)th entry of \\(AB\\) is \\[\\sum_{l =1}^{n} a_{il}b_{lk}\\] now \\(A_{il}B_{lj} \\in Q\\) for all \\(1 \\le l \\le n\\), and as \\(Q\\) is a subring of \\(R\\) we have that \\((AB)_{ij} \\in Q\\). A similar argument holds for \\(BA\\).\nLet \\(I\\) be an ideal of \\(M_{n}(R)\\). Set \\[Q:= \\{ a_{11}: A \\in I \\}.\\] We first demonstrate that \\(M_{n}(Q) = I\\). Then we show it is an ideal of \\(R\\).\nFor \\(1 \\le a,b \\le n\\) let \\(E_{ab}\\) be the matrix with entry \\(1_{R}\\) in position \\(ab\\) and zero every where else. For \\(1 \\le i,j \\le n\\) we write \\((E_{ab})_{ij}\\) for the \\(ij\\)th entry of \\(E_{ab}\\).\nLet \\(A = (a_{ij}) \\in I\\) be arbitrary. Fix \\(1 \\le i \\le k\\). We show that the matrix \\(a_{ik}E_{11}\\) is an element of \\(I\\).\nConsider the product the pqth entry of the product \\(E_{ki}AE_{kk}\\) \\[ \\sum_{l=1}^{n} \\left(\\sum_{m=1}^n (E_{ki})_{pm}a_{ml}\\right) (E_{kk})_{lq}.\\]\nSince \\((E_{ki})_{ab} = 0_{R}\\) unless \\(a=k\\) and \\(b=i\\) and \\((E_{kk})_{ab} = 0_{R}\\) unless \\(a = k\\) and \\(b=k\\), the \\(pq\\)th entry of the product \\(E_{ki}AE_{ki}\\) is zero whenever \\((p,q) \\ne (k,k)\\). If \\(p=k\\) and \\(q=k\\), then the kkth entry of \\(E_{ki}AE_{kk}\\) is \\[ \\sum_{l=1}^{n} \\left(\\sum_{m=1}^n (E_{ki})_{km}a_{ml}\\right) (E_{kk})_{lk} = \\sum_{l=1}^{n}a_{il}(E_{kk})_{lk} = a_{ik}.\\]\nTherefore \\[E_{ik}AE_{kk} = a_{ik}E_{kk} \\in I.\\] Now observe that \\[E_{11} = E_{1k}E_{kk}E_{k1}.\\] Therefore \\[a_{ik}E_{11} =  E_{1k}a_{ik}E_{kk}E_{k1}\\] is an element of \\(I\\). It follows that \\(a_{ik} \\in Q\\).\nSince \\(A \\in I\\) was arbitrarily chosen, we deduce \\(I = M_{n}(Q)\\). That \\(Q\\) is an ideal of \\(R\\). Follows from the fact that \\(I\\) is an ideal of \\(R\\) and for \\(a,b \\in R\\), \\(aE_{11} + bE_{11} = (a+b)E_{11}\\) and \\(aE_{11}bE_{11} = (ab)E_{11}\\).\n\n\n\n\n\n\n\nQuestion 4.2\n\n\nLet \\(R\\) be a PID and \\(a,b \\in R\\) be both nonzero. Show that there is a \\(c \\in R\\) such that\n\n\\(a|c\\) and \\(b|c\\);\nIf \\(a\\divides d\\) and \\(b \\divides d\\) then \\(c\\divides d\\).\n\n\n\n\n\n\nShow Solution 4.2\n\n\n\n\nSolution 4.2\n\n\nLet \\(c \\in R\\) be such that \\(aR \\cap bR\\). Such a \\(c\\) exists because \\(aR \\cap bR\\) is an ideal of \\(R\\) and \\(R\\) is a PID.\n\nNote that \\(cR \\subseteq aR\\) and \\(cR \\subseteq bR\\). It follows that \\(a|c\\) and \\(b|c\\).\nBy Lemma 3.12 \\(cR = aR\\cap bR\\) is the largest ideal of \\(R\\) that is contained in both \\(aR\\) and \\(bR\\). Now since \\(a|d\\), and \\(b|d\\), then, then \\(dR \\subseteq aR\\) and \\(dR \\subseteq bR\\). It follows that \\(dR\\) is contained in \\(cR\\) and so \\(c|d\\).\n\n\n\n\n\n\n\n\nQuestion 4.3\n\n\nLet \\(R\\) be a commutative ring and let \\(I_1 \\subseteq I_2 \\subseteq I_3 \\subseteq \\ldots\\) be a chain of proper ideals of \\(R\\). Prove that \\[I = \\bigcup_{i=1}^{\\infty} I_{i}\\] is a proper ideal of \\(R\\).\n\n\n\n\n\nShow Solution 4.3\n\n\n\n\nSolution 4.3\n\n\nWe need to demonstrate two things. Firstly, that \\(I \\subsetneq R\\) and secondly that \\(I\\) is an ideal of \\(R\\).\nIf \\(I = R\\), then \\(1_{R} \\in I\\). This means that \\(1_{R} \\in I_{i}\\) for some \\(i\\). However, this forces \\(I_i = R\\) and so \\(I_{i}\\) would not be a proper ideal of \\(R\\) — a contradiction. Therefore \\(I\\) is a proper subset of \\(R\\).\nWe now demonstrate that \\(I\\) is an ideal.\nClearly \\(I\\) is non-empty.\nLet \\(a,b \\in I\\) and \\(r \\in R\\). There is an \\(i \\in \\N\\) such that \\(a,b \\in I_{i}\\) it follows that \\(a+b \\in I_i\\) and \\(rb,br \\in I_{i}\\). We conclude that \\(I\\) is closed under addition is an ideal of \\(R\\).\n\n\n\n\n\n\n\nQuestion 4.4\n\n\nLet \\(R\\) be a PID show that there is not an infinite sequence of ideals \\(J_1, J_2, J_3, \\ldots\\) such that \\(J_{n} \\subsetneq J_{n+1}\\) for all \\(n \\in N\\).\n\n\n\n\n\nShow Solution 4.4\n\n\n\n\nSolution 4.4\n\n\nUsing Question 4.3 we know that \\(J = \\cup_{i=1}^{\\infty} J_i\\) is a proper ideal of \\(R\\). It follows, as \\(R\\) is a PID, that \\(J = aR\\) for some \\(a \\in R\\). Now, as \\(a \\in J\\), there is some \\(i\\), such that \\(a \\in J_i\\) and so \\(aI = J \\subseteq J_i \\subseteq J\\). We then we conclude that \\(J_{i} = J_{i+1} \\ldots\\) which contradicts the fact that \\(J_{i} \\subsetneq J_{i+1}\\).\n\n\n\n\n\n\n\nQuestion 4.5\n\n\nLet \\(R\\) be a PID, prove that \\(R\\) is a UFD.\n\n\n\n\n\nShow Solution 4.5\n\n\n\n\nSolution 4.5\n\n\nWe need to demonstrate two things. Firstly that \\(R\\) is a factorisation domain and secondly that it is a unique factorisation domain.\nLet \\(r \\in R\\) be an arbitrary nonzero non-unit element. Suppose for a contradiction that \\(r\\) cannot be written as a finite product of irreducible elements of \\(R\\). Note that \\(r\\) cannot be irreducible as a consequence of this. Therefore there are nonzero non-unit elements \\(r_1s_1 \\in R\\) such that \\(r = r_1s_1\\). Now if both \\(r_1\\) and \\(s_1\\) can be written as a finite product of irreducible elements of \\(R\\), then so can \\(r\\). Therefore one of \\(r_1\\) or \\(s_1\\) is not a product of irreducible elements of \\(R\\). Without loss of generality (since \\(R\\) is a commutative ring) we may assume that \\(r_1\\) is not a product of irreducibles. Notice that \\(rR \\subsetneq r_1R\\). Since if \\(rR = r_1R\\), then there is an \\(a \\in R\\) such that \\(r_1 = ar = r_1 as_1\\). This now means, since \\(R\\) is a PID, that \\(as_1 = 1_R\\) and \\(s_1\\) is a unit — a contradiction.\nWe may thus repeat the argument above with \\(r_1\\) in place of \\(r\\). Inductively, we see that for any \\(n \\in N\\), there is a sequence \\(r=r_{0},r_1, r_2, \\ldots, r_n\\) of non-irreducible, nonzero, non-unit, elements of \\(R\\) such that \\(r_{i}\\divides r_{i-1}\\) and \\(r_{i-1}R \\subsetneq r_{i}R\\).. This gives rise to a sequence of ideals \\(r_0 R \\subsetneq r_1 R \\subsetneq r_2R \\subsetneq \\ldots\\).\nHowever, using Question 4.4, it must be the case, as \\(R\\) is a PID, that there is an \\(i\\) such that \\(r_i R = r_{i+1}R\\). This yields the desired contradiction. We conclude that \\(r\\) can be written as a finite product of irreducible elements of \\(R\\).\nWe now argue that \\(R\\) is a unique factorisation domain. We do so by induction on the length of the smallest factorisation into irreducibles.\nWe establish the base case. This occurs for elements which can be written as a product of a single irreducible elements — i.e for irreducible elements of \\(R\\).\nLet \\(r\\) be an irreducible element of \\(R\\). Suppose there are irreducible elements \\(x_1, x_2, \\ldots, x_{k} \\in R\\) such that \\(r = x_1 x_2 \\ldots x_k\\). Since \\(R\\) is a PID then every irreducible element of \\(R\\) is prime. Now, as \\(r \\divides (x_1 x_2 \\ldots x_k)\\) and \\(r\\) is prime, then \\(r |x_i\\) for some \\(i\\). Without loss of generality (since \\(R\\) is commutative) we may assume that \\(i =1\\). Thus, \\(x_1 = ru\\) for some unit \\(u \\in R\\) (\\(u\\) must be a unit since both \\(x_1\\) and \\(r\\) are irreducible). It follows that \\[r - x_1 x_2 \\ldots x_k = r(1_{R} -  ux_2 \\ldots x_{k}).\\] Thus, \\(1_{R} = ux_2 \\ldots x_{k}\\) since \\(r\\) is irreducible. If \\(k>1\\), then \\(x_i\\) is a unit for all \\(2 \\le i \\le k\\) since \\(x_iux_2 \\ldots x_{i-1}x_{i+1}\\ldots x_{k} = 1_{R}\\). However, this contradicts the fact that \\(x_i\\), for \\(2 \\le i \\le k\\) is irreducible. Therefore, we conclude that \\(k=1\\) and \\(r\\) can be expressed in an essentially unique way as a product of irreducibles (since \\(x_1\\) is associated to \\(r\\)).\nAssume by induction that any nonzero non-unit element of \\(R\\) which can be expressed as a product of \\(n\\) irreducible elements of \\(R\\) has an essentially unique factorisation as a product of irreducible elements of \\(R\\).\nLet \\(r \\in R\\) be a nonzero non-unit element. Suppose there are irreducibles \\(x_1,x_2, \\ldots x_{n+1}, x_{n+1}, y_{1}, y_{2}, \\ldots, y_{s} \\in R\\) such that \\(r = x_1,x_2, \\ldots x_{n+1}, x_{n+1} = y_{1}, y_{2}, \\ldots, y_{s}\\). Now since \\(x_1\\) is irreducible, it is prime, so as in the base case, we may assume, without loss of generality that \\(y_1 = x_1 u\\) for a unit \\(u \\in R\\). Thus \\(r = x_1 x_2 \\ldots x_{n+1} = x_1uy_2 \\ldots y_s\\). Using the fact that \\(R\\) is an integral domain and \\(x_1\\) is irreducible we conclude that \\(r' = x_2 \\ldots x_{n+1} = y'_2 y_3 \\ldots y_s\\), where \\(y'_2 = uy_2\\). We may now apply the induction hypothesis to conclude that \\(s = n+1\\) and \\(x_{i}\\) is associated to \\(y_i\\) for all \\(3\\le i \\le n+1\\) and \\(x_2\\) is associated to \\(y_2\\) (since \\(x_2\\) is associated to \\(y'_2\\) which is associated to \\(y_2\\)) . Now, since \\(x_1\\) is associated to \\(y_1\\), it follows that \\(x_i\\) is associated to \\(y_i\\) for all \\(1 \\le i \\le n+1\\). We conclude that \\(r\\) has an essentially unique factorisation into irreducibles.\nThe result now follows by induction.\n\n\n\n\n\n\n\nQuestion 4.6\n\n\nLet \\(\\F\\) be a field. Show that \\(\\F[x]\\) is a principal ideal domain.\n[You may use the fact that given \\(f,g \\in \\F[x]\\), there exist unique \\(q,r \\in \\F[x]\\) such that \\(f = qg +r\\) and either \\(r=0\\) or the degree of \\(r\\) is strictly less than the degree of \\(G\\).]\n\n\n\n\n\nShow Solution 4.6\n\n\n\n\nSolution 4.6\n\n\nLet \\(I\\) be any ideal of \\(\\F[x]\\). Let \\(a \\in I\\) be an element with the smallest degree in \\(I\\). We note that \\(a\\) exists. Let \\(f\\) be any other element of \\(I\\). Then using the hint, there are \\(q,r \\in \\F[x]\\) such that \\(f = qa + r\\) and either \\(r =0\\) or the degree of \\(r\\) is strictly less than the degree of \\(a\\). Since \\(a\\) has the smallest degree in \\(I\\), and \\(r = f - qa \\in I\\) (since \\(a,f \\in I\\)), it follows that \\(r =0\\) and \\(f = qa\\). We conclude, as \\(f \\in I\\) was chosen arbitrarily, that \\(I = a\\F[x]\\).\n\n\n\n\n\n\n\nQuestion 4.7\n\n\nConsider the ring \\(R = \\Q[x]\\).\n\nShow that \\(x^5-7\\) is a prime.\nLet \\(I= (x^5 -7)R\\). Show that \\(R/I\\) is a field.\n\n\n\n\n\n\nShow Solution 4.7\n\n\n\n\nSolution 4.7\n\n\n\nSince \\(R\\) is a PID (by Question 4.6) it suffices to show that \\(x^5 -7\\) is irreducible. First observe that by De Moivre’s Theorem, we may write \\[x^5-7 = \\prod_{i=0}^{4}x-|7|^{\\frac{1}{5}}\\left(\\exp^{i\\frac{2i\\pi}{5}}\\right).\\] Set \\(p_i(x) = x-|7|^{\\frac{1}{5}}\\left(\\exp^{i\\frac{2i\\pi}{5}}\\right)\\) for \\(0 \\le i \\le 4.\\) Suppose \\(f(x), g(x) \\in \\Q[x]\\) are such that \\(x^5 -7 = f(x)g(x)\\). Then the sum of the degrees of \\(f(x)\\) and \\(g(x)\\) must be equal to \\(5\\) and \\(f(x)\\) and \\(g(x)\\) can both be expressed as products of the \\(p_i(x)\\)’s.\nIf one of \\(f(x)\\) and \\(g(x)\\) has degree \\(4\\), then the other must be equal to \\(p_i(x)\\) for some \\(i\\). However, \\(p_i(x)\\) is not an element of \\(\\Q[x]\\) for any \\(i\\). So this is not possible.\nIf one of \\(f(x)\\) and \\(g(x)\\) has degree \\(3\\), then the other has degree \\(2\\) and is a product \\(p_i(x)p_j(x)\\) for \\(0 \\le i,j \\le 4\\) and \\(i \\ne j\\). However, notice that \\(|p_i(0)p_j(0)| = |7|^{2/5} \\notin \\Q\\). Therefore \\(p_i(x)p_{j}(x) \\notin \\Q[x]\\) for any distinct \\(i\\) and \\(j\\). Therefore, this is not a possibility either.\nIt follows that if \\(x^5-7 = f(x)g(x)\\), then one of \\(f(x)\\) and \\(g(x)\\) has degree \\(5\\) and the other is therefore a unit. We conclude that \\(x^5-7\\) is an irreducible, and so prime, element of \\(\\Q[x]\\).\nSince \\(\\Q[x]\\) is a PID and \\((x^5-7)\\) is irreducible then \\(I=(x^5-7)\\Q[x]\\) is a maximal ideal of \\(\\Q[x]\\). It then follows that \\(\\Q[x]/I\\) is a field.\n\n\n\n\n\n\n\n\nQuestion 4.8\n\n\nConsider the guassian integers \\(\\Z[i]\\). Let \\(\\alpha = 10 + 11i\\) and \\(\\beta = 8+i\\). Find \\(\\gamma \\in \\Z[i]\\) such that \\(\\gamma \\Z[i] = \\alpha \\Z[i] + \\beta\\Z[i]\\).\n\n\n\n\n\nShow Solution 4.8\n\n\n\n\nSolution 4.8\n\n\nNotice that since \\(\\Z[i]\\) is a PID, then there is a \\(\\gamma \\in \\Z[i]\\) such that \\(\\alpha \\Z[i] + \\beta \\Z[i] = \\gamma \\Z[i]\\).\nLet \\(\\gamma \\in \\Z[i]\\) be such that \\(\\alpha \\Z[i] + \\beta \\Z[i] = \\gamma \\Z[i]\\). Since, \\(\\alpha, \\beta \\in \\alpha \\Z[i] + \\beta \\Z[i]\\), it follows that \\(\\gamma\\) divides both \\(\\alpha\\) and \\(\\beta\\).\nIf \\(\\gamma \\in \\Z[i]\\) divides both \\(\\alpha\\) and \\(\\beta\\), then \\(|\\gamma|^2\\) divides \\(|\\alpha|^2\\) and \\(|\\beta|^2\\). Thus, \\(|\\gamma|^2\\) divides both \\(221\\) and \\(65\\). It follows that \\(|\\gamma|^2\\) is either \\(1\\) or \\(13\\).\nWe first try \\(|\\gamma|^2 = 13\\). Set \\(\\gamma = a+ib\\), then \\(a^2 +b^2 = 13\\). Solving for \\(a,b \\in \\Z\\) then gives \\((a,b) \\in \\{(\\pm 2, \\pm3),(\\pm 3, \\pm 2)\\}\\). Notice that \\(-i(a+ib) = (b - ia)\\) and \\(i(a-ib) = (b + ia)\\). Since the units of \\(\\Z[i]\\) are precisely \\(\\{\\pm 1, \\pm i\\}\\), and \\(\\gamma \\Z[i] = (\\gamma t) \\Z[i]\\) for any unit \\(t\\), we only need consider the possibilities \\(\\gamma = z\\) and \\(\\gamma = \\bar{z}\\) for \\(z = 3+2i\\).\nNow observe that if \\(z=3+2i\\) divides both \\(\\alpha\\) and \\(\\beta\\), then \\(\\bar{z}\\) does not divide both \\(\\alpha\\) and \\(\\beta\\). This follows since if there are \\(u,v \\in \\Z[i]\\) such that \\(\\alpha = zu = \\overline{z}v\\), then \\(|u| = |v|\\) (since \\(|z| = |\\bar{z}|\\)). Thus either \\(\\bar{u} = v\\) (in which case \\(\\bar{\\alpha} = \\overline{zu} = \\bar{z}v = \\alpha\\) which is a contradiction) or \\(u = v t\\) for a unit \\(t\\) of \\(\\Z[i]\\) (in this case, since \\(\\Z[i]\\) is a PID, we conclude that \\(z = \\overline{z}t\\), but there is a no unit \\(t \\in \\Z[i]\\) such that \\(z = \\overline{z}t\\).) Thus either exactly one of \\(z\\) and \\(\\bar{z}\\) divides both \\(\\alpha\\) and \\(\\beta\\) or neither \\(z\\) nor \\(\\bar{z}\\) divides \\(\\alpha\\) and \\(\\beta\\).\nWe check first if \\(z\\) divides both \\(\\alpha\\) and \\(\\beta\\). Let \\(c+di, e+fi \\in \\Z[i]\\) be such that \\((3+2i)(c+di) = 10 + 11i\\) and \\((3+2i)(e+fi) = 8+i\\). We have\n\n\n\n\\[\\begin{eqnarray*}\n   3c -2d &=& 10\\\\\n   2c + 3d &=& 11\n\\end{eqnarray*}\\]\n\n\n\\[\\begin{eqnarray*}\n   3e -2f &=& 8\\\\\n   2e + 3f &=& 1\n\\end{eqnarray*}\\]\n\n\n\nSolving both of these simultaneous equations gives \\(c=4, d=1\\) and \\(e=2,f=-1\\). Therefore \\(z\\) divides both \\(\\alpha\\) and \\(\\beta\\). It follows that \\(\\bar{z}\\) does not divide both \\(\\alpha\\) and \\(\\beta\\).\nWe conclude that \\((\\alpha \\Z[i] + \\beta \\Z[i]) = (3+2i)\\Z[i]\\) since any non-unit element of \\(\\Z[i]\\) which divides both \\(\\alpha\\) and \\(\\beta\\) must be equal to \\((3+2i)t\\) for a unit \\(t\\) of \\(\\Z[i]\\)."
  }
]